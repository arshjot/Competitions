{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/deepLearn/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from rankboost import BipartiteRankBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (595212, 59)\n",
      "Test shape: (892816, 58)\n"
     ]
    }
   ],
   "source": [
    "# Read in our input data\n",
    "train = pd.read_csv('../Dataset/train/train.csv')\n",
    "test = pd.read_csv('../Dataset/test/test.csv')\n",
    "\n",
    "\n",
    "# This prints out (rows, columns) in each dataframe\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "\n",
    "\n",
    "id_train = train['id'].values\n",
    "y = train.target.values\n",
    "id_test = test['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "# Create an XGBoost-compatible metric from Gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', gini_score\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', 1-gini_score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We drop these variables as we don't want to train on them\n",
    "# The other 57 columns are all numerical and can be trained on without preprocessing\n",
    "\n",
    "train['countNAs'] = train.isin(['-1']).sum(axis=1)\n",
    "test['countNAs'] = test.isin(['-1']).sum(axis=1)\n",
    "\n",
    "train_cont = train.drop(['id','ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat','ps_car_01_cat',\n",
    "                    'ps_car_02_cat','ps_car_03_cat','ps_car_04_cat','ps_car_05_cat',\n",
    "                    'ps_car_06_cat','ps_car_07_cat','ps_car_08_cat','ps_car_09_cat',\n",
    "                    'ps_car_10_cat','ps_car_11_cat','target'], axis=1)\n",
    "test_cont = test.drop(['id','ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat','ps_car_01_cat',\n",
    "                    'ps_car_02_cat','ps_car_03_cat','ps_car_04_cat','ps_car_05_cat',\n",
    "                    'ps_car_06_cat','ps_car_07_cat','ps_car_08_cat','ps_car_09_cat',\n",
    "                    'ps_car_10_cat','ps_car_11_cat'], axis=1)\n",
    "\n",
    "# One-hot encoding\n",
    "one_hot = OneHotEncoder()\n",
    "train_cat = train[['ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat','ps_car_01_cat',\n",
    "                    'ps_car_02_cat','ps_car_03_cat','ps_car_04_cat','ps_car_05_cat',\n",
    "                    'ps_car_06_cat','ps_car_07_cat','ps_car_08_cat','ps_car_09_cat',\n",
    "                    'ps_car_10_cat','ps_car_11_cat']]\n",
    "test_cat = test[['ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat','ps_car_01_cat',\n",
    "                    'ps_car_02_cat','ps_car_03_cat','ps_car_04_cat','ps_car_05_cat',\n",
    "                    'ps_car_06_cat','ps_car_07_cat','ps_car_08_cat','ps_car_09_cat',\n",
    "                    'ps_car_10_cat','ps_car_11_cat']]\n",
    "\n",
    "train_cat = train_cat.replace(-1, 999)\n",
    "test_cat = test_cat.replace(-1, 999)\n",
    "\n",
    "train_cat = one_hot.fit_transform(train_cat).toarray()\n",
    "test_cat = one_hot.fit_transform(test_cat).toarray()\n",
    "\n",
    "X = np.concatenate((train_cat, train_cont.values), axis=1)\n",
    "test = np.concatenate((test_cat, test_cont.values), axis=1)\n",
    "standardise = StandardScaler()\n",
    "standardise = standardise.fit(X)\n",
    "X = standardise.transform(X)\n",
    "test = standardise.transform(test)\n",
    "# print('Train shape:', train.shape)\n",
    "# print('Test shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RankBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = BipartiteRankBoost(n_estimators=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building stump 1 out of 100\n",
      "building stump 2 out of 100\n",
      "building stump 3 out of 100\n",
      "building stump 4 out of 100\n",
      "building stump 5 out of 100\n",
      "building stump 6 out of 100\n",
      "building stump 7 out of 100\n",
      "building stump 8 out of 100\n",
      "building stump 9 out of 100\n",
      "building stump 10 out of 100\n",
      "building stump 11 out of 100\n",
      "building stump 12 out of 100\n",
      "building stump 13 out of 100\n",
      "building stump 14 out of 100\n",
      "building stump 15 out of 100\n",
      "building stump 16 out of 100\n",
      "building stump 17 out of 100\n",
      "building stump 18 out of 100\n",
      "building stump 19 out of 100\n",
      "building stump 20 out of 100\n",
      "building stump 21 out of 100\n",
      "building stump 22 out of 100\n",
      "building stump 23 out of 100\n",
      "building stump 24 out of 100\n",
      "building stump 25 out of 100\n"
     ]
    }
   ],
   "source": [
    "fitted = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = classifier.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26904366210649211"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini_normalized(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a submission file\n",
    "sub.to_csv('./submit/xgb_v7_missing_vals_specified.csv.gz', \n",
    "           index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28306339999999997"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_scores = [0.279973, 0.283588, 0.282138, 0.291781, 0.277837]\n",
    "np.mean(val_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepLearn]",
   "language": "python",
   "name": "conda-env-deepLearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
