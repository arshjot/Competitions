{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/deepLearn/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "import os\n",
    "from os.path import basename\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import tempfile\n",
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(data_folder_path, audio_length = 45, n_mels = 128, val_split = 0.2):\n",
    "    \"\"\"\n",
    "    Return consolidated and preprocessed wav file melspectograms and labels \n",
    "    imported from the provided locations \n",
    "    \n",
    "    Arguments:\n",
    "    data_folder_path  -- Path of folder containing training audio folders\n",
    "    audio_length      -- Length of output audio (milliseconds)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get list of speakers for train-val split\n",
    "    speakers = []\n",
    "    for file_ in glob.glob(data_folder_path+'/*/*.wav'):\n",
    "        if file_[file_[:file_.rfind('/')].rfind('/')+1:file_.rfind('/')]=='_background_noise_':\n",
    "            continue            \n",
    "        speakers.append(file_[file_.rfind('/')+1:file_.find('_')])\n",
    "    \n",
    "    # Split into training and validation\n",
    "    train_speakers, val_speakers = train_test_split(list(set(speakers)), test_size=val_split, random_state=42)\n",
    "    train_size = len([i for i in speakers if i in train_speakers])\n",
    "    val_size = len([i for i in speakers if i in val_speakers])\n",
    "    # Add more space for 'silence' labels\n",
    "    train_size+=1950\n",
    "    val_size+=450\n",
    "    \n",
    "    label_list = 'yes no up down left right on off stop go'.split()+['unknown', 'silence']\n",
    "    \n",
    "    # Initialize arrays for storing data\n",
    "    train_features = np.zeros((train_size, audio_length, n_mels, 2), np.float32)\n",
    "    val_features = np.zeros((val_size, audio_length, n_mels, 2), np.float32)\n",
    "    train_labels = np.zeros((train_size, len(label_list)), np.int64)\n",
    "    val_labels = np.zeros((val_size, len(label_list)), np.int64)\n",
    "    \n",
    "    # For shuffling\n",
    "    p = np.random.permutation(train_size)\n",
    "    \n",
    "    # Iterate over audio files to extract features and labels\n",
    "    tr_num, val_num = 0, 0\n",
    "    length = []\n",
    "    for file_ in glob.glob(data_folder_path+'/*/*.wav'):\n",
    "        label = file_[file_[:file_.rfind('/')].rfind('/')+1:file_.rfind('/')]\n",
    "        if label=='_background_noise_':\n",
    "            continue\n",
    "        if label not in label_list[:-2]:\n",
    "            unknown_label = label\n",
    "            label = 'unknown'\n",
    "#             if np.random.choice([True, False], p=[15/16, 1/16]):\n",
    "#                 continue\n",
    "            \n",
    "        \n",
    "        speaker = file_[file_.rfind('/')+1:file_.find('_')]\n",
    "        \n",
    "        X, sample_rate = sf.read(file_)\n",
    "        X, _ = librosa.effects.trim(X) # Remove leading and trailing silence\n",
    "        file_feature = librosa.feature.melspectrogram(X, sample_rate, n_mels=n_mels, hop_length=370)\n",
    "        file_feature2 = librosa.feature.mfcc(X, sample_rate, n_mfcc=n_mels, hop_length=370)\n",
    "#         length.append([file_feature.shape[1], label])\n",
    "        file_feature = cv2.resize(librosa.power_to_db(file_feature, ref=np.max), (audio_length, n_mels))\n",
    "        file_feature2 = cv2.resize(file_feature2, (audio_length, n_mels))\n",
    "        file_feature = np.stack((file_feature, file_feature2), axis=2)\n",
    "        \n",
    "        if (speaker in val_speakers and label != 'unknown') or (label == 'unknown' and \n",
    "                                                                speaker in val_speakers and\n",
    "                                                                unknown_label in ['house', 'dog', 'two', 'five']):\n",
    "            val_features[val_num] = file_feature.reshape((audio_length, n_mels, 2))\n",
    "            val_labels[val_num, label_list.index(label)] = 1.0\n",
    "            val_num+=1\n",
    "        elif (speaker in train_speakers and label != 'unknown') or (label == 'unknown' and\n",
    "                                                                    speaker in train_speakers and\n",
    "                                                                    unknown_label not in ['house', 'dog', 'two', 'five']):\n",
    "            train_features[p[tr_num]] = file_feature.reshape((audio_length, n_mels, 2))\n",
    "            train_labels[p[tr_num], label_list.index(label)] = 1.0\n",
    "            tr_num+=1\n",
    "        \n",
    "        if (tr_num+val_num)%10000==0:\n",
    "            print(tr_num+val_num)\n",
    "    \n",
    "    # Add 'silence' labels\n",
    "    for file_ in glob.glob(data_folder_path+'/_background_noise_/*.wav'):\n",
    "        X, sample_rate = librosa.load(file_, res_type='kaiser_fast')\n",
    "        file_feature = librosa.power_to_db(librosa.feature.melspectrogram(X, sample_rate, n_mels=n_mels),\n",
    "                                           ref=np.max)\n",
    "        file_feature2 = librosa.feature.mfcc(X, sample_rate, n_mfcc=n_mels)\n",
    "        file_feature = np.stack((file_feature, file_feature2), axis=2)\n",
    "        \n",
    "        random_samples = [file_feature[:,i:i+audio_length,:].reshape((audio_length, n_mels, 2))\n",
    "                          for i in np.random.randint(file_feature.shape[1]-audio_length, size=(400))]\n",
    "        \n",
    "        train_features[p[tr_num:tr_num+325]] = random_samples[:325]\n",
    "        train_labels[p[tr_num:tr_num+325], label_list.index('silence')] = 1.0\n",
    "        val_features[val_num:val_num+75] = random_samples[325:]\n",
    "        val_labels[val_num:val_num+75, label_list.index('silence')] = 1.0\n",
    "        tr_num+=325\n",
    "        val_num+=75\n",
    "        \n",
    "    # Remove blank rows\n",
    "    idx_tr = np.where(np.sum(train_labels, 1)==0)[0]\n",
    "    idx_val = np.where(np.sum(val_labels, 1)==0)[0]\n",
    "    train_labels = np.delete(train_labels, idx_tr, 0)\n",
    "    train_features = np.delete(train_features, idx_tr, 0)\n",
    "    val_labels = np.delete(val_labels, idx_val, 0)\n",
    "    val_features = np.delete(val_features, idx_val, 0)\n",
    "    \n",
    "    return {'train_X' : train_features, 'train_y' : train_labels,\n",
    "           'val_X' : val_features, 'val_y' : val_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "data = readData('./Data/train/audio/', n_mels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = {'train_X' : data['train_X'], 'train_y' : data['train_y']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(data, open('./Data/train_val.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('./Data/train_val_balanced.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(data, open('./Data/train_val.pickle', 'wb'))\n",
    "# pickle.dump(data, open('./Data/train_val_balanced.pickle', 'wb'))\n",
    "# pickle.dump(data, open('./Data/train_val_binary.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    im_height = 128\n",
    "    im_width = 45\n",
    "    im_depth = 1\n",
    "    num_epochs = 200\n",
    "    batch_size = 24\n",
    "    \n",
    "    network_width = 15\n",
    "    \n",
    "    # CNN\n",
    "    conv1_patch_size = 3\n",
    "    conv2_patch_size = 2\n",
    "    conv3_patch_size = 2\n",
    "    conv1_depth = network_width*1\n",
    "    conv2_depth = network_width*3\n",
    "    conv3_depth = network_width*5\n",
    "    \n",
    "    # RNN\n",
    "    num_layers = 1\n",
    "    rnn_num_hidden = 128    \n",
    "    \n",
    "    # Dense\n",
    "    num_hidden = 256\n",
    "    \n",
    "    # Number of classes\n",
    "    num_classes = 12\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class speechModel(object):\n",
    "    \n",
    "    def __init__(self, config, savefile):\n",
    "        \n",
    "        self.config = config\n",
    "        self.savefile = savefile\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def read(self):\n",
    "        \n",
    "        self.eval = tf.placeholder(tf.bool)\n",
    "        \n",
    "    def net(self, images, labels):\n",
    "\n",
    "        # Define CNN variables\n",
    "        intitalizer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        self.layer1_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv1_patch_size, self.config.conv1_patch_size, \n",
    "             self.config.im_depth, self.config.conv1_depth]),\n",
    "                                          name='conv1_W')\n",
    "        self.layer1_biases = tf.Variable(tf.zeros([self.config.conv1_depth]), name='conv1_b')\n",
    "        self.layer2_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv2_patch_size, self.config.conv2_patch_size, \n",
    "             self.config.conv1_depth, self.config.conv2_depth]), name='conv2_W')\n",
    "        self.layer2_biases = tf.Variable(tf.zeros([self.config.conv2_depth]), name='conv2_b')\n",
    "        self.layer3_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv3_patch_size, self.config.conv3_patch_size, \n",
    "             self.config.conv2_depth, self.config.conv3_depth]), name='conv3_W')\n",
    "        self.layer3_biases = tf.Variable(tf.zeros([self.config.conv3_depth]), name='conv3_b')\n",
    "\n",
    "        self.layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [(16*6*self.config.conv3_depth),\n",
    "             self.config.num_hidden], stddev=0.1), name='dense_W')\n",
    "        self.layer4_biases = tf.Variable(tf.constant(1.0, shape=[self.config.num_hidden]), name='dense_W')\n",
    "        self.layer5_weights = tf.Variable(tf.truncated_normal([self.config.num_hidden, self.config.num_classes], \n",
    "                                                              stddev=0.1), name='out_W')\n",
    "        self.layer5_biases = tf.Variable(tf.constant(1.0, shape=[self.config.num_classes]), name='out_W')\n",
    "\n",
    "        \n",
    "        # ==================1==================\n",
    "        # CNN\n",
    "        with tf.name_scope('CNN_1'):\n",
    "            conv = tf.nn.conv2d(images, self.layer1_weights, [1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + self.layer1_biases)\n",
    "            conv1_out = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "            conv1_out = tf.where(self.eval, conv1_out, tf.nn.dropout(conv1_out, 0.9))\n",
    "        \n",
    "        # ==================2==================\n",
    "        # CNN\n",
    "        with tf.name_scope('CNN_2'):\n",
    "            conv = tf.nn.conv2d(conv1_out, self.layer2_weights, [1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + self.layer2_biases)\n",
    "            conv2_out = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "            conv2_out = tf.where(self.eval, conv2_out, tf.nn.dropout(conv2_out, 0.9))\n",
    "\n",
    "        # ==================3==================\n",
    "        # CNN\n",
    "        with tf.name_scope('CNN_3'):\n",
    "            conv = tf.nn.conv2d(conv2_out, self.layer3_weights, [1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + self.layer3_biases)\n",
    "            conv3_out = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "            conv3_out = tf.where(self.eval, conv3_out, tf.nn.dropout(conv3_out, 0.9))\n",
    "        \n",
    "\n",
    "        with tf.name_scope('Dense'):\n",
    "            reshape = tf.reshape(conv3_out, [-1,  \n",
    "                                                  tf.shape(conv3_out)[1]*tf.shape(conv3_out)[2]*tf.shape(conv3_out)[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, self.layer4_weights) + self.layer4_biases)\n",
    "            hidden = tf.where(self.eval, hidden, tf.nn.dropout(hidden, 0.6))\n",
    "            \n",
    "            # Doing the affine projection\n",
    "            logits = tf.matmul(hidden, self.layer5_weights) + self.layer5_biases\n",
    "\n",
    "        # Reshaping back to the original shape\n",
    "        logits = tf.reshape(logits, [-1, self.config.num_classes])\n",
    "\n",
    "        # For saving model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Training computation\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "        # Optimizer\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        self.prediction = tf.nn.sigmoid(logits)\n",
    "        self.labels = labels\n",
    "        \n",
    "    def train(self, tr_X, tr_y, val_X, val_y):\n",
    "        num_steps = int((self.config.num_epochs*len(data['train_X']))/self.config.batch_size)\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Prepare input pipeline\n",
    "        images = tf.placeholder(tf.float32, shape=(None, self.config.im_width,\n",
    "                                                   self.config.im_height, self.config.im_depth))\n",
    "        labels = tf.placeholder(tf.float32, shape=(None, self.config.num_classes))\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.prefetch(1)\n",
    "        dataset = dataset.batch(self.config.batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        val_dataset = val_dataset.prefetch(1)\n",
    "        val_dataset = val_dataset.batch(self.config.batch_size)\n",
    "        \n",
    "        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n",
    "        next_example, next_label = iterator.get_next()\n",
    "        training_init_op = iterator.make_initializer(dataset)\n",
    "        validation_init_op = iterator.make_initializer(val_dataset)\n",
    "        \n",
    "        self.read()\n",
    "        self.net(next_example, next_label)\n",
    "        # The op for initializing the variables.\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        # TensorBoard Summaries\n",
    "        train_cost_summary = tf.summary.scalar(\"training_cost\", self.loss)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            summaries = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(\"./TensorBoard/lines\", sess.graph)            \n",
    "            sess.run(init_op)\n",
    "            print('Initialized')\n",
    "            \n",
    "            start = time.time()\n",
    "            steps_time = start\n",
    "            epoch_time = start\n",
    "            \n",
    "            epoch_train_cost = []\n",
    "            train_predictions = []\n",
    "            train_labels = []\n",
    "            best_val = 0\n",
    "            step = 0\n",
    "            \n",
    "            # Train\n",
    "            for ep in range(1, self.config.num_epochs+1):\n",
    "                feed_dict = {images : tr_X, labels : tr_y}\n",
    "                sess.run(training_init_op, feed_dict=feed_dict)\n",
    "                feed_dict = {self.eval : False}\n",
    "                while True:\n",
    "                    try:\n",
    "                        _, l, predictions, summ = sess.run([self.optimizer, self.loss,\n",
    "                                                          self.prediction, summaries],\n",
    "                                                               feed_dict=feed_dict)\n",
    "                        writer.add_summary(summ, step)\n",
    "                        step+=1\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                \n",
    "                # At epoch completion\n",
    "                print('Epoch', ep, 'Completed')\n",
    "                print('Time =', round(time.time() - epoch_time), 'seconds')\n",
    "                epoch_time = time.time()\n",
    "                \n",
    "                # Evaluation - Training set\n",
    "                feed_dict = {images : tr_X, labels : tr_y}\n",
    "                sess.run(validation_init_op, feed_dict=feed_dict)\n",
    "                feed_dict = {self.eval : True}\n",
    "                ep_predictions, ep_labels = [], []\n",
    "                while True:\n",
    "                    try:\n",
    "                        predictions, labels_ = sess.run([self.prediction, self.labels], feed_dict=feed_dict)\n",
    "                        ep_predictions.append(predictions)\n",
    "                        ep_labels.append(labels_)\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                ep_predictions, ep_labels = np.concatenate(ep_predictions), np.concatenate(ep_labels)\n",
    "                tr_acc = 100*np.sum(np.argmax(ep_predictions, 1)==np.argmax(ep_labels, 1)) / ep_predictions.shape[0]\n",
    "                tr_loss= log_loss(ep_labels, ep_predictions)\n",
    "                \n",
    "                # Evaluation - Validation set\n",
    "                feed_dict = {images : val_X, labels : val_y}\n",
    "                sess.run(validation_init_op, feed_dict=feed_dict)\n",
    "                feed_dict = {self.eval : True}\n",
    "                ep_predictions, ep_labels = [], []\n",
    "                while True:\n",
    "                    try:\n",
    "                        predictions, labels_ = sess.run([self.prediction, self.labels], feed_dict=feed_dict)\n",
    "                        ep_predictions.append(predictions)\n",
    "                        ep_labels.append(labels_)\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                ep_predictions, ep_labels = np.concatenate(ep_predictions), np.concatenate(ep_labels)\n",
    "                val_acc = 100*np.sum(np.argmax(ep_predictions, 1)==np.argmax(ep_labels, 1)) / ep_predictions.shape[0]\n",
    "                val_loss= log_loss(ep_labels, ep_predictions)\n",
    "                writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"val_cost\", simple_value=val_loss)]), ep)\n",
    "                \n",
    "                print('Cost: Training =', tr_loss, 'Validation =', val_loss)\n",
    "                print('Accuracy: Training =', \"{0:.2f}\".format(tr_acc), 'Validation =', \"{0:.2f}\".format(val_acc))\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                \n",
    "                if val_acc>best_val:\n",
    "                    best_val = val_acc\n",
    "                    self.saver.save(sess, self.savefile)\n",
    "                \n",
    "            writer.close()\n",
    "            print('Total runtime =', round(time.time() - start), 'seconds')\n",
    "            \n",
    "    def evaluate(self, X, y):\n",
    "        with tf.Session() as sess:\n",
    "            # restore the model\n",
    "            self.saver.restore(sess, self.savefile)\n",
    "            # Prepare input pipeline\n",
    "            images = tf.placeholder(tf.float32, shape=(None, self.config.im_width, self.config.im_height,\n",
    "                                                       self.config.im_depth))\n",
    "            labels = tf.placeholder(tf.float32, shape=(None, self.config.num_classes))\n",
    "\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "            dataset = dataset.prefetch(1)\n",
    "            dataset = dataset.batch(self.config.batch_size)\n",
    "            iterator = dataset.make_initializable_iterator()\n",
    "            next_example, next_label = iterator.get_next()\n",
    "\n",
    "            self.read()\n",
    "            self.net(next_example, next_label)\n",
    "            \n",
    "            feed_dict = {images : X, labels : y}\n",
    "            sess.run(training_init_op, feed_dict=feed_dict)\n",
    "            feed_dict = {self.eval : True}\n",
    "            eval_predictions, eval_labels = [], []\n",
    "            while True:  \n",
    "                try:\n",
    "                    predictions, labels_ = sess.run([self.prediction, self.labels], feed_dict=feed_dict)\n",
    "                    eval_predictions.append(predictions)\n",
    "                    eval_labels.append(labels_)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            eval_predictions, eval_labels = np.concatenate(eval_predictions), np.concatenate(eval_labels)\n",
    "            eval_acc = 100*np.sum(np.argmax(eval_predictions, 1)==np.argmax(eval_labels, 1)) / eval_predictions.shape[0]\n",
    "            eval_loss= log_loss(eval_labels, eval_predictions)\n",
    "            \n",
    "            return eval_loss, eval_acc\n",
    "            \n",
    "    def predict(self, X):\n",
    "        with tf.Session() as sess:\n",
    "            # restore the model\n",
    "            self.saver.restore(sess, self.savefile)\n",
    "            # Prepare input pipeline\n",
    "            images = tf.placeholder(tf.float32, shape=(None, self.config.im_width, self.config.im_height, 1))\n",
    "\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((images))\n",
    "            dataset = dataset.prefetch(1)\n",
    "            dataset = dataset.batch(self.config.batch_size)\n",
    "            iterator = dataset.make_initializable_iterator()\n",
    "            next_example = iterator.get_next()\n",
    "\n",
    "            self.read()\n",
    "            self.net(next_example, next_label)\n",
    "            \n",
    "            val_predictions = []\n",
    "            feed_dict = {images : X}\n",
    "            sess.run(iterator.initializer, feed_dict=feed_dict)\n",
    "            feed_dict = {self.eval : True}\n",
    "            while True:                \n",
    "                try:\n",
    "                    predictions = sess.run([self.prediction], feed_dict=feed_dict)\n",
    "                    val_predictions.append(predictions)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "                        \n",
    "            val_predictions = np.concatenate(val_predictions)\n",
    "            return val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch 1 Completed\n",
      "Time = 10 seconds\n",
      "Epoch 2 Completed\n",
      "Time = 10 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-aa785eab8111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeechModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./saved_model/model_binary_v28Dec17_v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-145-29e26023bec4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tr_X, tr_y, val_X, val_y)\u001b[0m\n\u001b[1;32m    153\u001b[0m                         _, l, predictions, summ = sess.run([self.optimizer, self.loss,\n\u001b[1;32m    154\u001b[0m                                                           self.prediction, summaries],\n\u001b[0;32m--> 155\u001b[0;31m                                                                feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    156\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                         \u001b[0mstep\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = speechModel(config=config, savefile='./saved_model/model_binary_v28Dec17_v0')\n",
    "model.train(data['train_X'], data['train_y'], data['val_X'], data['val_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'input' to a tensor and failed. Error: None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    523\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[0;32m--> 524\u001b[0;31m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-f2be88c5e143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeechModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./saved_model/model_v28Dec17_v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-134-3e8dd49f6e1a>\u001b[0m in \u001b[0;36mnet\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CNN_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             conv = tf.nn.conv2d(images, self.layer1_weights, [1, 1, 1, 1],\n\u001b[0;32m---> 44\u001b[0;31m                                 padding='SAME')\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             conv1_out = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    632\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    526\u001b[0m               raise ValueError(\n\u001b[1;32m    527\u001b[0m                   \u001b[0;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                   (input_name, err))\n\u001b[0m\u001b[1;32m    529\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    530\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to convert 'input' to a tensor and failed. Error: None values not supported."
     ]
    }
   ],
   "source": [
    "pred = speechModel(config=config, savefile='./saved_model/model_v28Dec17_v0')\n",
    "pred.read()\n",
    "pred.net()\n",
    "print(pred.evaluate(data['train_X'], data['train_y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/model_v28Dec17_v0\n"
     ]
    }
   ],
   "source": [
    "pred = speechModel(config=config, savefile='./saved_model/model_v28Dec17_v0')\n",
    "pred.read()\n",
    "pred.net()\n",
    "\n",
    "val_predictions = pred.predict(data['val_X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88800303720577067"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(val_predictions, 1) == np.argmax(data['val_y'], 1))/val_predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEyCAYAAADJI8VDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5R9JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f6ed516d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.3611585238\n"
     ]
    }
   ],
   "source": [
    "class_acc = []\n",
    "for i in range(12):\n",
    "    idx = np.where(np.argmax(data['val_y'], 1)==i)[0]\n",
    "    preds = np.argmax(val_predictions[idx], 1)\n",
    "#     preds[np.where(val_predictions[idx,10]>0.5)] = 10\n",
    "    class_acc.append((100*np.sum(np.argmax(data['val_y'][idx], 1) == preds))/idx.shape[0])\n",
    "\n",
    "label_list = 'yes no up down left right on off stop go'.split()+['unknown', 'silence']\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.arange(12), class_acc)\n",
    "plt.xticks(np.arange(12), label_list)\n",
    "for i, v in enumerate(class_acc):\n",
    "    plt.text(i, v+1, str('{0:.2f}'.format(v)), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n",
    "print(np.mean(class_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, sr = librosa.load('./Data/train/audio/four/0a9f9af7_nohash_0.wav')\n",
    "X, _ = librosa.effects.trim(y)\n",
    "y2, sr = librosa.load('./Data/train/audio/left/1a6eca98_nohash_0.wav')\n",
    "X2, _ = librosa.effects.trim(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "librosa.output.write_wav('temp.wav', np.concatenate((X[:X.shape[0]//2], X2[X2.shape[0]//2:]), axis=0), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Known/Unknown Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readUnknownData(data_folder_path, audio_length = 45, n_mels = 128, val_split = 0.2):\n",
    "    \"\"\"\n",
    "    Return consolidated and preprocessed wav file melspectograms and labels \n",
    "    imported from the provided locations \n",
    "    \n",
    "    Arguments:\n",
    "    data_folder_path  -- Path of folder containing training audio folders\n",
    "    audio_length      -- Length of output audio (milliseconds)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get list of speakers for train-val split\n",
    "    speakers = []\n",
    "    for file_ in glob.glob(data_folder_path+'/*/*.wav'):\n",
    "        if file_[file_[:file_.rfind('/')].rfind('/')+1:file_.rfind('/')]=='_background_noise_':\n",
    "            continue            \n",
    "        speakers.append(file_[file_.rfind('/')+1:file_.find('_')])\n",
    "    \n",
    "    # Split into training and validation\n",
    "    train_speakers, val_speakers = train_test_split(list(set(speakers)), test_size=val_split, random_state=42)\n",
    "    train_size = len([i for i in speakers if i in train_speakers])\n",
    "    val_size = len([i for i in speakers if i in val_speakers])\n",
    "    # Add more space for 'silence' labels\n",
    "    train_size+=1950\n",
    "    val_size+=450\n",
    "    \n",
    "    label_list = 'yes no up down left right on off stop go'.split()+['unknown', 'silence']\n",
    "    new_label_list = ['known', 'unknown']\n",
    "    \n",
    "    # Initialize arrays for storing data\n",
    "    train_features = np.zeros((train_size, audio_length, n_mels, 1), np.float32)\n",
    "    val_features = np.zeros((val_size, audio_length, n_mels, 1), np.float32)\n",
    "    train_labels = np.zeros((train_size, 1), np.int64)\n",
    "    val_labels = np.zeros((val_size, 1), np.int64)\n",
    "    \n",
    "    # For shuffling\n",
    "    p = np.random.permutation(train_size)\n",
    "    \n",
    "    # Iterate over audio files to extract features and labels\n",
    "    tr_num, val_num = 0, 0\n",
    "    length = []\n",
    "    for file_ in glob.glob(data_folder_path+'/*/*.wav'):\n",
    "        label = file_[file_[:file_.rfind('/')].rfind('/')+1:file_.rfind('/')]\n",
    "        if label=='_background_noise_':\n",
    "            continue\n",
    "        if label not in label_list[:-2]:\n",
    "            unknown_label = label\n",
    "            label = 'unknown'\n",
    "            if np.random.choice([True, False], p=[0.3125, 0.6875]):\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        speaker = file_[file_.rfind('/')+1:file_.find('_')]\n",
    "        \n",
    "        X, sample_rate = librosa.load(file_, res_type='kaiser_fast')\n",
    "        X, _ = librosa.effects.trim(X) # Remove leading and trailing silence\n",
    "        file_feature = librosa.feature.melspectrogram(X, sample_rate, n_mels=n_mels)\n",
    "#         length.append([file_feature.shape[1], label])\n",
    "        file_feature = cv2.resize(librosa.power_to_db(file_feature, ref=np.max), (audio_length, n_mels))\n",
    "        \n",
    "        if (speaker in val_speakers and label != 'unknown') or (label == 'unknown' and \n",
    "                                                                speaker in val_speakers and\n",
    "                                                                unknown_label in ['house', 'dog', 'two', 'five']):\n",
    "            val_features[val_num] = file_feature.reshape((audio_length, n_mels, 1))\n",
    "            if label == 'unknown':\n",
    "                val_labels[val_num] = 1.0\n",
    "            val_num+=1\n",
    "        elif (speaker in train_speakers and label != 'unknown') or (label == 'unknown' and\n",
    "                                                                    speaker in train_speakers and\n",
    "                                                                    unknown_label not in ['house', 'dog', 'two', 'five']):\n",
    "            train_features[p[tr_num]] = file_feature.reshape((audio_length, n_mels, 1))\n",
    "            if label == 'unknown':\n",
    "                train_labels[p[tr_num]] = 1.0\n",
    "            tr_num+=1\n",
    "        \n",
    "        if (tr_num+val_num)%10000==0:\n",
    "            print(tr_num+val_num)\n",
    "    \n",
    "    # Add 'silence' labels\n",
    "    for file_ in glob.glob(data_folder_path+'/_background_noise_/*.wav'):\n",
    "        X, sample_rate = librosa.load(file_, res_type='kaiser_fast')\n",
    "        file_feature = librosa.power_to_db(librosa.feature.melspectrogram(X, sample_rate, n_mels=n_mels),\n",
    "                                           ref=np.max)\n",
    "        \n",
    "        random_samples = [file_feature[:,i:i+audio_length].reshape((audio_length, n_mels, 1))\n",
    "                          for i in np.random.randint(file_feature.shape[1]-audio_length, size=(400))]\n",
    "        \n",
    "        train_features[p[tr_num:tr_num+325]] = random_samples[:325]\n",
    "        train_labels[p[tr_num:tr_num+325]] = 0.0\n",
    "        val_features[val_num:val_num+75] = random_samples[325:]\n",
    "        val_labels[val_num:val_num+75] = 0.0\n",
    "        tr_num+=325\n",
    "        val_num+=75\n",
    "        \n",
    "    # Remove blank rows\n",
    "    idx_tr = np.where(np.sum(train_features, (1,2,3))==0)[0]\n",
    "    idx_val = np.where(np.sum(val_features, (1,2,3))==0)[0]\n",
    "    train_labels = np.delete(train_labels, idx_tr, 0)\n",
    "    train_features = np.delete(train_features, idx_tr, 0)\n",
    "    val_labels = np.delete(val_labels, idx_val, 0)\n",
    "    val_features = np.delete(val_features, idx_val, 0)\n",
    "    \n",
    "    return {'train_X' : train_features, 'train_y' : train_labels,\n",
    "           'val_X' : val_features, 'val_y' : val_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    im_height = 128\n",
    "    im_width = 45\n",
    "    num_epochs = 200\n",
    "    batch_size = 24\n",
    "    \n",
    "    network_width = 15\n",
    "    \n",
    "    # CNN\n",
    "    conv1_patch_size = 3\n",
    "    conv2_patch_size = 2\n",
    "    conv3_patch_size = 2\n",
    "    conv1_depth = network_width*1\n",
    "    conv2_depth = network_width*3\n",
    "    conv3_depth = network_width*5\n",
    "    \n",
    "    # RNN\n",
    "    num_layers = 1\n",
    "    rnn_num_hidden = 128    \n",
    "    \n",
    "    # Dense\n",
    "    num_hidden = 256\n",
    "    \n",
    "    # Number of classes\n",
    "    num_classes = 1\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class speechModel(object):\n",
    "    \n",
    "    def __init__(self, config, savefile):\n",
    "        \n",
    "        self.config = config\n",
    "        self.savefile = savefile\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def read(self):\n",
    "        \n",
    "        self.images = tf.placeholder(tf.float32,\n",
    "                                          shape=(None, self.config.im_width,\n",
    "                                                 self.config.im_height, 1))\n",
    "        self.labels = tf.placeholder(tf.float32, shape=(None, self.config.num_classes))\n",
    "        self.val = tf.placeholder(tf.bool)\n",
    "        \n",
    "    def net(self):\n",
    "\n",
    "        # Define CNN variables\n",
    "        intitalizer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        self.layer1_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv1_patch_size, self.config.conv1_patch_size, 1, self.config.conv1_depth]),\n",
    "                                          name='conv1_W')\n",
    "        self.layer1_biases = tf.Variable(tf.zeros([self.config.conv1_depth]), name='conv1_b')\n",
    "        self.layer2_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv2_patch_size, self.config.conv2_patch_size, \n",
    "             self.config.conv1_depth, self.config.conv2_depth]), name='conv2_W')\n",
    "        self.layer2_biases = tf.Variable(tf.zeros([self.config.conv2_depth]), name='conv2_b')\n",
    "        self.layer3_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv3_patch_size, self.config.conv3_patch_size, \n",
    "             self.config.conv2_depth, self.config.conv3_depth]), name='conv3_W')\n",
    "        self.layer3_biases = tf.Variable(tf.zeros([self.config.conv3_depth]), name='conv3_b')\n",
    "\n",
    "        self.layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "            [(16*6*self.config.conv3_depth),\n",
    "             self.config.num_hidden], stddev=0.1), name='dense_W')\n",
    "        self.layer4_biases = tf.Variable(tf.constant(1.0, shape=[self.config.num_hidden]), name='dense_W')\n",
    "        self.layer5_weights = tf.Variable(tf.truncated_normal([self.config.num_hidden, self.config.num_classes], \n",
    "                                                              stddev=0.1), name='out_W')\n",
    "        self.layer5_biases = tf.Variable(tf.constant(1.0, shape=[self.config.num_classes]), name='out_W')\n",
    "\n",
    "        \n",
    "        # ==================1==================\n",
    "        # CNN\n",
    "        with tf.name_scope('CNN_1'):\n",
    "#             conv = tf.layers.batch_normalization(\n",
    "#                 inputs=self.images, axis=-1, momentum=0.999, epsilon=0.001, center=True, scale=True, training = not self.val)\n",
    "            conv = tf.nn.conv2d(self.images, self.layer1_weights, [1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "#             conv = tf.layers.batch_normalization(\n",
    "#                 inputs=conv, axis=-1, momentum=0.999, epsilon=0.001, center=True, scale=True, training = not self.val)\n",
    "            hidden = tf.nn.relu(conv + self.layer1_biases)\n",
    "            conv1_out = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "            if not self.val:\n",
    "                conv1_out = tf.nn.dropout(conv1_out, 0.75)\n",
    "        \n",
    "        # ==================2==================\n",
    "        # CNN\n",
    "        with tf.name_scope('CNN_2'):\n",
    "            conv = tf.nn.conv2d(conv1_out, self.layer2_weights, [1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "#             conv = tf.layers.batch_normalization(\n",
    "#                 inputs=conv, axis=-1, momentum=0.999, epsilon=0.001, center=True, scale=True, training = not self.val)\n",
    "            hidden = tf.nn.relu(conv + self.layer2_biases)\n",
    "            conv2_out = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "            if not self.val:\n",
    "                conv2_out = tf.nn.dropout(conv2_out, 0.7)\n",
    "\n",
    "        # ==================3==================\n",
    "        # CNN\n",
    "        with tf.name_scope('CNN_3'):\n",
    "            conv = tf.nn.conv2d(conv2_out, self.layer3_weights, [1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "#             conv = tf.layers.batch_normalization(\n",
    "#                 inputs=conv, axis=-1, momentum=0.999, epsilon=0.001, center=True, scale=True, training = not self.val)\n",
    "            hidden = tf.nn.relu(conv + self.layer3_biases)\n",
    "            conv3_out = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "            if not self.val:\n",
    "                conv3_out = tf.nn.dropout(conv3_out, 0.7)\n",
    "        \n",
    "\n",
    "        with tf.name_scope('Dense'):\n",
    "#             self.output = tf.concat(self.output, 2)[-1]\n",
    "            \n",
    "            # Reshaping to apply the same weights over the timesteps\n",
    "#             reshape_rnn = tf.reshape(self.output, [-1, 2*self.config.rnn_num_hidden])\n",
    "            \n",
    "            reshape = tf.reshape(conv3_out, [-1,  \n",
    "                                                  tf.shape(conv3_out)[1]*tf.shape(conv3_out)[2]*tf.shape(conv3_out)[3]])\n",
    "#             reshape = tf.concat((reshape_rnn, reshape_conv), 1)\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, self.layer4_weights) + self.layer4_biases)\n",
    "            if not self.val:\n",
    "                hidden = tf.nn.dropout(hidden, 0.5)\n",
    "            # Doing the affine projection\n",
    "            logits = tf.matmul(hidden, self.layer5_weights) + self.layer5_biases\n",
    "\n",
    "        # Reshaping back to the original shape\n",
    "        logits = tf.reshape(logits, [self.config.batch_size, self.config.num_classes])\n",
    "\n",
    "        # For saving model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Training computation\n",
    "#         class_indices = tf.stack(list(np.arange(10))+[11])\n",
    "#         class_labels = tf.gather(self.labels, class_indices, axis=1)\n",
    "#         class_logits = tf.gather(logits, class_indices, axis=1)\n",
    "#         unknown_index = tf.stack([10])\n",
    "#         unknown_labels = tf.gather(self.labels, unknown_index, axis=1)\n",
    "#         unknown_logits = tf.gather(logits, unknown_index, axis=1)\n",
    "#         self.loss = tf.reduce_mean(\n",
    "#             tf.nn.softmax_cross_entropy_with_logits(labels=class_labels, logits=class_logits)+\n",
    "#             tf.nn.sigmoid_cross_entropy_with_logits(labels=unknown_labels, logits=unknown_logits))\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=logits))\n",
    "\n",
    "        # Optimizer\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            self.optimizer = tf.train.AdamOptimizer(0.00005).minimize(self.loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        self.prediction = tf.nn.sigmoid(logits)\n",
    "        \n",
    "    def accuracy(self, predictions, labels):\n",
    "        preds = predictions>0.5\n",
    "        preds = preds.astype(np.int32)\n",
    "        return (100.0 * np.sum(preds == labels)\n",
    "                / predictions.shape[0])\n",
    "\n",
    "    def train(self):\n",
    "        num_steps = int((self.config.num_epochs*len(data['train_X']))/self.config.batch_size)\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.read()\n",
    "        self.val = False\n",
    "        self.net()\n",
    "\n",
    "        # The op for initializing the variables.\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        # TensorBoard Summaries\n",
    "        train_cost_summary = tf.summary.scalar(\"training_cost\", self.loss)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            summaries = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(\"./TensorBoard/lines\", sess.graph)            \n",
    "            sess.run(init_op)\n",
    "            print('Initialized')\n",
    "            \n",
    "            start = time.time()\n",
    "            steps_time = start\n",
    "            epoch_time = start\n",
    "            \n",
    "            epoch = 1\n",
    "            epoch_train_cost = []\n",
    "            train_predictions = []\n",
    "            train_labels = []\n",
    "            best_val = 0\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                offset = (step * self.config.batch_size) % (len(data['train_X']) - self.config.batch_size)\n",
    "                batch_images = data['train_X'][offset:(offset + self.config.batch_size), :, :]\n",
    "                batch_labels = data['train_y'][offset:(offset + self.config.batch_size)]\n",
    "\n",
    "                feed_dict = {self.images : batch_images, self.labels : batch_labels}\n",
    "                \n",
    "                self.val = False\n",
    "                _, l, predictions, summ = sess.run([self.optimizer, self.loss,\n",
    "                                                          self.prediction, summaries],\n",
    "                                                               feed_dict=feed_dict)\n",
    "                writer.add_summary(summ, step)\n",
    "                epoch_train_cost.append(l)\n",
    "                train_predictions.append(predictions)\n",
    "                train_labels.append(batch_labels)\n",
    "\n",
    "                if (step!=0 and (step+1) % int(len(data['train_X'])/self.config.batch_size) == 0):\n",
    "                    print('Epoch', epoch, 'Completed')\n",
    "                    print('Time =', round(time.time() - epoch_time), 'seconds')\n",
    "                    \n",
    "                    # Get val loss and accuracy\n",
    "                    epoch_val_cost = []\n",
    "                    val_predictions = []\n",
    "                    val_labels = []\n",
    "                    num_val_steps = int((len(data['val_X']))/self.config.batch_size)\n",
    "                    for val_step in range(num_val_steps):\n",
    "                        val_offset = (val_step * self.config.batch_size) % (len(data['val_X']) - self.config.batch_size+1)\n",
    "                        batch_val_images = data['val_X'][val_offset:(val_offset + self.config.batch_size), :, :]\n",
    "                        batch_val_labels = data['val_y'][val_offset:(val_offset + self.config.batch_size)]\n",
    "                        \n",
    "                        val_feed_dict = {self.images : batch_val_images, self.labels : batch_val_labels}\n",
    "                        \n",
    "                        self.val = True\n",
    "                        val_cost, predictions = sess.run([self.loss, self.prediction], feed_dict=val_feed_dict)\n",
    "                        \n",
    "                        epoch_val_cost.append(val_cost)\n",
    "                        val_predictions.append(predictions)\n",
    "                        val_labels.append(batch_val_labels)\n",
    "                    \n",
    "                    # Writing values to TensoLogs and printing epoch results\n",
    "                    epoch_val_cost = np.mean(epoch_val_cost)\n",
    "                    writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"val_cost\", \n",
    "                                                                          simple_value=epoch_val_cost)]), epoch)\n",
    "                    \n",
    "                    train_predictions = np.concatenate(train_predictions)\n",
    "                    val_predictions = np.concatenate(val_predictions)\n",
    "                    train_labels = np.concatenate(train_labels)\n",
    "                    val_labels = np.concatenate(val_labels)\n",
    "                    \n",
    "                    val_acc = self.accuracy(val_predictions.reshape((val_predictions.shape[0],\n",
    "                                                                                           self.config.num_classes)),\n",
    "                                                        val_labels)\n",
    "                    \n",
    "                    print('Cost: Training =', np.mean(epoch_train_cost), \n",
    "                          'Validation =', epoch_val_cost)\n",
    "                    print('Accuracy: Training =', self.accuracy(train_predictions.reshape((train_predictions.shape[0],\n",
    "                                                                                           self.config.num_classes)),\n",
    "                                                                train_labels), \n",
    "                          'Validation =', val_acc)\n",
    "                    print('-------------------------------------------------------------------------------')\n",
    "                    if val_acc > best_val:\n",
    "                        best_val = val_acc\n",
    "                        self.saver.save(sess, self.savefile)\n",
    "                    epoch_train_cost = []\n",
    "                    train_predictions = []\n",
    "                    train_labels = []\n",
    "                    epoch_time = time.time()\n",
    "                    epoch+=1\n",
    "            writer.close()\n",
    "            print('Total runtime =', round(time.time() - start), 'seconds')\n",
    "            \n",
    "    def predict(self, X):\n",
    "        with tf.Session() as sess:\n",
    "            # restore the model\n",
    "            self.saver.restore(sess, self.savefile)\n",
    "            val_predictions = []\n",
    "            num_val_steps = int((len(X))/self.config.batch_size)\n",
    "            for val_step in range(num_val_steps):\n",
    "                val_offset = (val_step * self.config.batch_size) % (len(X) - self.config.batch_size+1)\n",
    "                batch_val_images = X[val_offset:(val_offset + self.config.batch_size), :, :]\n",
    "                val_predictions.append(sess.run(self.prediction, feed_dict={self.images : batch_val_images}))\n",
    "            val_predictions = np.concatenate(val_predictions)\n",
    "            return val_predictions.reshape((val_predictions.shape[0], self.config.num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Epoch 1 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 9.41087 Validation = 0.956079\n",
      "Accuracy: Training = 50.4547312487 Validation = 46.8385214008\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 2 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.800889 Validation = 0.694182\n",
      "Accuracy: Training = 50.1967095851 Validation = 56.14461738\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 3 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.724882 Validation = 0.646661\n",
      "Accuracy: Training = 50.6335581443 Validation = 62.5648508431\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 4 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.705882 Validation = 0.634357\n",
      "Accuracy: Training = 50.7689556509 Validation = 63.813229572\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 5 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.699604 Validation = 0.656963\n",
      "Accuracy: Training = 52.0079705702 Validation = 59.2898832685\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 6 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.690378 Validation = 0.625735\n",
      "Accuracy: Training = 53.1141426528 Validation = 64.364461738\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 7 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.684676 Validation = 0.647728\n",
      "Accuracy: Training = 53.9290823626 Validation = 58.8845654994\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 8 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.664992 Validation = 0.623112\n",
      "Accuracy: Training = 58.0446556305 Validation = 63.1160830091\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 9 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.639719 Validation = 0.618505\n",
      "Accuracy: Training = 62.4463519313 Validation = 61.9649805447\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 10 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.621356 Validation = 0.61161\n",
      "Accuracy: Training = 65.0240138974 Validation = 63.9105058366\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 11 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.605636 Validation = 0.610921\n",
      "Accuracy: Training = 66.4367463724 Validation = 64.2671854734\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 12 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.594527 Validation = 0.621919\n",
      "Accuracy: Training = 67.5301451053 Validation = 63.9105058366\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 13 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.581457 Validation = 0.625216\n",
      "Accuracy: Training = 68.6976292663 Validation = 65.2399481193\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 14 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.566513 Validation = 0.628038\n",
      "Accuracy: Training = 69.9545268751 Validation = 63.2133592737\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 15 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.552647 Validation = 0.622593\n",
      "Accuracy: Training = 71.4285714286 Validation = 67.2016861219\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 16 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.539722 Validation = 0.616186\n",
      "Accuracy: Training = 72.3942366646 Validation = 68.4662775616\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 17 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.526556 Validation = 0.624762\n",
      "Accuracy: Training = 73.4646433681 Validation = 66.8125810636\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 18 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.513689 Validation = 0.631931\n",
      "Accuracy: Training = 74.1441855712 Validation = 67.3476005188\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 19 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.505671 Validation = 0.606345\n",
      "Accuracy: Training = 74.8416104639 Validation = 70.9630350195\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 20 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.495555 Validation = 0.62326\n",
      "Accuracy: Training = 75.7561823012 Validation = 70.4280155642\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 21 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.491884 Validation = 0.637969\n",
      "Accuracy: Training = 75.8634784386 Validation = 69.0012970169\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 22 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.482487 Validation = 0.617898\n",
      "Accuracy: Training = 76.4408338443 Validation = 71.546692607\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 23 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.475801 Validation = 0.609563\n",
      "Accuracy: Training = 77.0488452892 Validation = 71.5953307393\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 24 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.471351 Validation = 0.623012\n",
      "Accuracy: Training = 77.3196403025 Validation = 71.1575875486\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 25 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.463238 Validation = 0.598136\n",
      "Accuracy: Training = 77.7232781525 Validation = 72.7950713359\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 26 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.459955 Validation = 0.600016\n",
      "Accuracy: Training = 78.1933374208 Validation = 72.924773022\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 27 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.457249 Validation = 0.619123\n",
      "Accuracy: Training = 78.0758226037 Validation = 72.6653696498\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 28 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.4503 Validation = 0.603249\n",
      "Accuracy: Training = 78.5152258328 Validation = 73.4922178988\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 29 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.446138 Validation = 0.630044\n",
      "Accuracy: Training = 78.8090128755 Validation = 71.271076524\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 30 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.440072 Validation = 0.614716\n",
      "Accuracy: Training = 78.8090128755 Validation = 72.9085603113\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 31 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.437928 Validation = 0.618689\n",
      "Accuracy: Training = 79.2790721439 Validation = 73.7354085603\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 32 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.432195 Validation = 0.633664\n",
      "Accuracy: Training = 79.4553443695 Validation = 73.6057068742\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 33 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.428003 Validation = 0.609176\n",
      "Accuracy: Training = 79.5243204578 Validation = 73.0869001297\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 34 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.422694 Validation = 0.634491\n",
      "Accuracy: Training = 80.2932761087 Validation = 73.2814526589\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 35 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.42018 Validation = 0.613401\n",
      "Accuracy: Training = 80.1706519518 Validation = 74.2542153048\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch 36 Completed\n",
      "Time = 14 seconds\n",
      "Cost: Training = 0.416522 Validation = 0.629568\n",
      "Accuracy: Training = 80.2191906806 Validation = 73.3625162127\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-defa3fe85e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeechModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./saved_model/model_binary_v28Dec17_v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-1e707862e2a9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 _, l, predictions, summ = sess.run([self.optimizer, self.loss,\n\u001b[1;32m    227\u001b[0m                                                           self.prediction, summaries],\n\u001b[0;32m--> 228\u001b[0;31m                                                                feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mepoch_train_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = speechModel(config=config, savefile='./saved_model/model_binary_v28Dec17_v0')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/model_v28Dec17_v0\n"
     ]
    }
   ],
   "source": [
    "config_val = Config()\n",
    "config_val.batch_size = 1\n",
    "pred = speechModel(config=config_val, savefile='./saved_model/model_binary_v28Dec17_v0')\n",
    "pred.val_images = data['val_X']\n",
    "pred.val_labels = data['val_y']\n",
    "pred.read()\n",
    "pred.val=True\n",
    "pred.net()\n",
    "\n",
    "val_predictions = pred.predict(pred.val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEyCAYAAADJI8VDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni0TP+6gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6043439630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.5326101643\n"
     ]
    }
   ],
   "source": [
    "class_acc = []\n",
    "for i in range(12):\n",
    "    idx = np.where(np.argmax(data['val_y'], 1)==i)[0]\n",
    "    preds = np.argmax(val_predictions[idx], 1)\n",
    "    preds[np.where(val_predictions[idx,10]>0.5)] = 10\n",
    "    class_acc.append((100*np.sum(np.argmax(data['val_y'][idx], 1) == preds))/idx.shape[0])\n",
    "\n",
    "label_list = 'yes no up down left right on off stop go'.split()+['unknown', 'silence']\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.arange(12), class_acc)\n",
    "plt.xticks(np.arange(12), label_list)\n",
    "for i, v in enumerate(class_acc):\n",
    "    plt.text(i, v+1, str('{0:.2f}'.format(v)), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n",
    "print(np.mean(class_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepLearn]",
   "language": "python",
   "name": "conda-env-deepLearn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
