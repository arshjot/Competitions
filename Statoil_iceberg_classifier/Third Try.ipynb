{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.set_start_method('spawn') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "seed = 0\n",
    "# random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# from utils_3 import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('./Data/train.json')\n",
    "test = pd.read_json('./Data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "test['band_1'] = test['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "test['band_2'] = test['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce').fillna(0.0)\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "test['not_machine_generated'] = test['inc_angle'].apply(lambda x: len(str(x))) <= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_range (input_, min_, max_):\n",
    "    input_ += -(np.min(input_))\n",
    "    input_ /= np.max(input_) / (max_ - min_)\n",
    "    input_ += min_\n",
    "    return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1_tr = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_tr = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "X = np.stack((band_1_tr, band_2_tr), axis=1)\n",
    "# X_Real_std = np.stack((band_1_tr, band_2_tr), axis=1)\n",
    "X_ang = np.array(data['inc_angle']).reshape((len(data), 1))\n",
    "y = data['is_iceberg'].values.astype(np.float32)\n",
    "\n",
    "X_train, X_val, X_ang_train, X_ang_val, y_train, y_val = train_test_split(X, X_ang, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = [X_train, X_ang_train]\n",
    "X_val = [X_val, X_ang_val]\n",
    "\n",
    "band_1_test = np.concatenate([im for im in test['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_test = np.concatenate([im for im in test['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3_test = scale_range(band_1_test/band_2_test, -1, 1)\n",
    "X_test = np.stack((band_1_test, band_2_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "width = 75\n",
    "height = 75\n",
    "channels = 2\n",
    "padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "class icebergDataset(data_utils.Dataset):\n",
    "    \"\"\"Iceberg-Ship dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y=None, transform=None, u_ed=0.2, u_zo=0.5, u_noisy=0.4, u_shift=0.3):\n",
    "        self.X_images = X[0]\n",
    "        self.X_angles = torch.from_numpy(X[1]).float()\n",
    "        if y!=None:\n",
    "            self.y = torch.from_numpy(y).long()\n",
    "        else:\n",
    "            self.y=None\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.u_ed = u_ed\n",
    "        self.u_zo = u_zo\n",
    "        self.u_noisy = u_noisy\n",
    "        self.u_shift = u_shift\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.X_images[idx]\n",
    "        if self.transform:\n",
    "#             if np.random.random() < 0.2:\n",
    "#                 im = cv2.blur(im, (2,2))\n",
    "            im = randomErodeDilate(im, u=self.u_ed)\n",
    "            im = randomZoomOut(im, u=self.u_zo)\n",
    "            im = randomNoisy(im, u=self.u_noisy)\n",
    "            im = randomShift(im, u=self.u_shift)\n",
    "        \n",
    "        try:\n",
    "            if self.y==None:\n",
    "                return [torch.from_numpy(im).float(), self.X_angles[idx]]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return [torch.from_numpy(im).float(), self.X_angles[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='./Models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, './Models/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = icebergDataset(X_train, y_train, transform=True)\n",
    "val_dataset = icebergDataset(X_val, y_val)\n",
    "test_dataset = icebergDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = nn.LeakyReLU()\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.batch = nn.BatchNorm2d(channels)\n",
    "        self.batch1D = nn.BatchNorm1d(1)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, 9, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(9),\n",
    "            act,\n",
    "            nn.Conv2d(9, 18, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(18, 24, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(24),\n",
    "            act,\n",
    "            nn.Conv2d(24, 36, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(36),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(36, 72, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(72),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(72, 144, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(144),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer_shallow = nn.Sequential(\n",
    "            nn.Conv2d(channels, 128, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1+(3*3*144), 512),\n",
    "            act,\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 196),\n",
    "            act,\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc3 = nn.Linear(196, 2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x_im, x_angle):\n",
    "        x_im = self.batch(x_im)\n",
    "        x_angle = self.batch1D(x_angle)\n",
    "        out = self.layer1(x_im)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         out_shallow = self.layer_shallow(x_im)\n",
    "#         out_shallow  = F.max_pool2d(out_shallow, kernel_size=out_shallow.size()[2:])\n",
    "#         out_shallow = out_shallow.view(out_shallow.size(0), -1)\n",
    "        out = torch.cat([out, x_angle], dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (\n",
       "  (batch): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (batch1D): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (layer1): Sequential (\n",
       "    (0): Conv2d(2, 9, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): Conv2d(9, 18, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): LeakyReLU (0.01)\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer_skip): Sequential (\n",
       "    (0): Conv2d(2, 5, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): Dropout (p = 0.2)\n",
       "    (4): Conv2d(5, 10, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (5): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU (0.01)\n",
       "    (7): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): Conv2d(18, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): Conv2d(24, 36, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): LeakyReLU (0.01)\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer3): Sequential (\n",
       "    (0): Conv2d(36, 72, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): Conv2d(72, 144, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): LeakyReLU (0.01)\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer4): Sequential (\n",
       "    (0): Conv2d(72, 144, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer_shallow): Sequential (\n",
       "    (0): Conv2d(2, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0.2)\n",
       "  )\n",
       "  (fc1): Sequential (\n",
       "    (0): Linear (3601 -> 32)\n",
       "    (1): LeakyReLU (0.01)\n",
       "    (2): Dropout (p = 0.5)\n",
       "  )\n",
       "  (fc3): Linear (32 -> 2)\n",
       "  (sig): Sigmoid ()\n",
       ")"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "iceNet = net()\n",
    "iceNet.apply(weight_init).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    is_iceberg_tot = 0\n",
    "    is_iceberg_correct = 0\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features_angle = features_angle.cuda()\n",
    "        features = Variable(features, volatile=True)\n",
    "        features_angle = Variable(features_angle, volatile=True)\n",
    "        labels = Variable(labels, volatile=True)\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] \n",
      "Training Loss: 0.6475\n",
      "Validation Loss: 0.6377, Accuracy: 68.54%, Ice Berg Accuracy: 99.39%\n",
      "Epoch [2/200] \n",
      "Training Loss: 0.5652\n",
      "Validation Loss: 0.7135, Accuracy: 63.55%, Ice Berg Accuracy: 42.07%\n",
      "Epoch [3/200] \n",
      "Training Loss: 0.5129\n",
      "Validation Loss: 0.4428, Accuracy: 80.06%, Ice Berg Accuracy: 95.73%\n",
      "Epoch [4/200] \n",
      "Training Loss: 0.5060\n",
      "Validation Loss: 0.4430, Accuracy: 80.69%, Ice Berg Accuracy: 87.80%\n",
      "Epoch [5/200] \n",
      "Training Loss: 0.4727\n",
      "Validation Loss: 0.4242, Accuracy: 80.69%, Ice Berg Accuracy: 98.78%\n",
      "Epoch [6/200] \n",
      "Training Loss: 0.4639\n",
      "Validation Loss: 0.3626, Accuracy: 82.87%, Ice Berg Accuracy: 84.76%\n",
      "Epoch [7/200] \n",
      "Training Loss: 0.4385\n",
      "Validation Loss: 0.3642, Accuracy: 83.80%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [8/200] \n",
      "Training Loss: 0.3975\n",
      "Validation Loss: 0.3255, Accuracy: 86.60%, Ice Berg Accuracy: 87.20%\n",
      "Epoch [9/200] \n",
      "Training Loss: 0.3916\n",
      "Validation Loss: 0.3080, Accuracy: 85.98%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [10/200] \n",
      "Training Loss: 0.4030\n",
      "Validation Loss: 0.3309, Accuracy: 83.49%, Ice Berg Accuracy: 87.80%\n",
      "Epoch [11/200] \n",
      "Training Loss: 0.3745\n",
      "Validation Loss: 0.2916, Accuracy: 87.54%, Ice Berg Accuracy: 90.24%\n",
      "Epoch [12/200] \n",
      "Training Loss: 0.3834\n",
      "Validation Loss: 0.2715, Accuracy: 88.47%, Ice Berg Accuracy: 95.12%\n",
      "Epoch [13/200] \n",
      "Training Loss: 0.3670\n",
      "Validation Loss: 0.2664, Accuracy: 86.29%, Ice Berg Accuracy: 95.12%\n",
      "Epoch [14/200] \n",
      "Training Loss: 0.4021\n",
      "Validation Loss: 0.2877, Accuracy: 85.98%, Ice Berg Accuracy: 97.56%\n",
      "Epoch [15/200] \n",
      "Training Loss: 0.3843\n",
      "Validation Loss: 0.2608, Accuracy: 90.65%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [16/200] \n",
      "Training Loss: 0.3691\n",
      "Validation Loss: 0.2684, Accuracy: 90.65%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [17/200] \n",
      "Training Loss: 0.3368\n",
      "Validation Loss: 0.3188, Accuracy: 83.18%, Ice Berg Accuracy: 73.78%\n",
      "Epoch [18/200] \n",
      "Training Loss: 0.3994\n",
      "Validation Loss: 0.2677, Accuracy: 88.79%, Ice Berg Accuracy: 84.15%\n",
      "Epoch [19/200] \n",
      "Training Loss: 0.3168\n",
      "Validation Loss: 0.2456, Accuracy: 92.52%, Ice Berg Accuracy: 91.46%\n",
      "Epoch [20/200] \n",
      "Training Loss: 0.3305\n",
      "Validation Loss: 0.2359, Accuracy: 91.28%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [21/200] \n",
      "Training Loss: 0.2836\n",
      "Validation Loss: 0.2977, Accuracy: 87.23%, Ice Berg Accuracy: 79.27%\n",
      "Epoch [22/200] \n",
      "Training Loss: 0.3458\n",
      "Validation Loss: 0.2450, Accuracy: 90.65%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [23/200] \n",
      "Training Loss: 0.3543\n",
      "Validation Loss: 0.2634, Accuracy: 91.90%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [24/200] \n",
      "Training Loss: 0.3346\n",
      "Validation Loss: 0.2237, Accuracy: 93.77%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [25/200] \n",
      "Training Loss: 0.3241\n",
      "Validation Loss: 0.2414, Accuracy: 89.72%, Ice Berg Accuracy: 95.12%\n",
      "Epoch [26/200] \n",
      "Training Loss: 0.3162\n",
      "Validation Loss: 0.2227, Accuracy: 91.28%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [27/200] \n",
      "Training Loss: 0.3380\n",
      "Validation Loss: 0.2359, Accuracy: 90.34%, Ice Berg Accuracy: 87.80%\n",
      "Epoch [28/200] \n",
      "Training Loss: 0.3385\n",
      "Validation Loss: 0.2230, Accuracy: 90.97%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [29/200] \n",
      "Training Loss: 0.3267\n",
      "Validation Loss: 0.2253, Accuracy: 89.72%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [30/200] \n",
      "Training Loss: 0.3133\n",
      "Validation Loss: 0.2426, Accuracy: 89.72%, Ice Berg Accuracy: 84.15%\n",
      "Epoch [31/200] \n",
      "Training Loss: 0.2791\n",
      "Validation Loss: 0.2276, Accuracy: 90.65%, Ice Berg Accuracy: 87.20%\n",
      "Epoch [32/200] \n",
      "Training Loss: 0.2782\n",
      "Validation Loss: 0.3059, Accuracy: 86.92%, Ice Berg Accuracy: 77.44%\n",
      "Epoch [33/200] \n",
      "Training Loss: 0.2519\n",
      "Validation Loss: 0.2329, Accuracy: 89.72%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [34/200] \n",
      "Training Loss: 0.2658\n",
      "Validation Loss: 0.2355, Accuracy: 91.90%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [35/200] \n",
      "Training Loss: 0.2607\n",
      "Validation Loss: 0.2350, Accuracy: 91.59%, Ice Berg Accuracy: 87.20%\n",
      "Epoch [36/200] \n",
      "Training Loss: 0.2579\n",
      "Validation Loss: 0.2162, Accuracy: 91.28%, Ice Berg Accuracy: 88.41%\n",
      "Epoch [37/200] \n",
      "Training Loss: 0.2505\n",
      "Validation Loss: 0.2186, Accuracy: 90.65%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [38/200] \n",
      "Training Loss: 0.2533\n",
      "Validation Loss: 0.2527, Accuracy: 88.16%, Ice Berg Accuracy: 80.49%\n",
      "Epoch [39/200] \n",
      "Training Loss: 0.2753\n",
      "Validation Loss: 0.2690, Accuracy: 89.72%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [40/200] \n",
      "Training Loss: 0.3008\n",
      "Validation Loss: 0.2094, Accuracy: 91.59%, Ice Berg Accuracy: 90.24%\n",
      "Epoch [41/200] \n",
      "Training Loss: 0.3161\n",
      "Validation Loss: 0.2150, Accuracy: 91.28%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [42/200] \n",
      "Training Loss: 0.2841\n",
      "Validation Loss: 0.2226, Accuracy: 89.72%, Ice Berg Accuracy: 87.80%\n",
      "Epoch [43/200] \n",
      "Training Loss: 0.2306\n",
      "Validation Loss: 0.2290, Accuracy: 90.65%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [44/200] \n",
      "Training Loss: 0.2581\n",
      "Validation Loss: 0.2031, Accuracy: 90.34%, Ice Berg Accuracy: 88.41%\n",
      "Epoch [45/200] \n",
      "Training Loss: 0.2523\n",
      "Validation Loss: 0.2101, Accuracy: 90.65%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [46/200] \n",
      "Training Loss: 0.2746\n",
      "Validation Loss: 0.2355, Accuracy: 90.65%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [47/200] \n",
      "Training Loss: 0.2456\n",
      "Validation Loss: 0.2036, Accuracy: 91.90%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [48/200] \n",
      "Training Loss: 0.2421\n",
      "Validation Loss: 0.2427, Accuracy: 89.72%, Ice Berg Accuracy: 96.34%\n",
      "Epoch [49/200] \n",
      "Training Loss: 0.2638\n",
      "Validation Loss: 0.2162, Accuracy: 90.34%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [50/200] \n",
      "Training Loss: 0.2668\n",
      "Validation Loss: 0.2175, Accuracy: 90.03%, Ice Berg Accuracy: 95.12%\n",
      "Epoch [51/200] \n",
      "Training Loss: 0.3198\n",
      "Validation Loss: 0.1951, Accuracy: 91.90%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [52/200] \n",
      "Training Loss: 0.2708\n",
      "Validation Loss: 0.2149, Accuracy: 90.97%, Ice Berg Accuracy: 95.12%\n",
      "Epoch [53/200] \n",
      "Training Loss: 0.3120\n",
      "Validation Loss: 0.2086, Accuracy: 90.65%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [54/200] \n",
      "Training Loss: 0.2703\n",
      "Validation Loss: 0.2318, Accuracy: 90.34%, Ice Berg Accuracy: 89.63%\n",
      "Epoch [55/200] \n",
      "Training Loss: 0.2429\n",
      "Validation Loss: 0.2179, Accuracy: 90.65%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [56/200] \n",
      "Training Loss: 0.2657\n",
      "Validation Loss: 0.2182, Accuracy: 91.59%, Ice Berg Accuracy: 91.46%\n",
      "Epoch [57/200] \n",
      "Training Loss: 0.2336\n",
      "Validation Loss: 0.2222, Accuracy: 90.65%, Ice Berg Accuracy: 88.41%\n",
      "Epoch [58/200] \n",
      "Training Loss: 0.2217\n",
      "Validation Loss: 0.2033, Accuracy: 92.52%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [59/200] \n",
      "Training Loss: 0.2548\n",
      "Validation Loss: 0.2202, Accuracy: 92.83%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [60/200] \n",
      "Training Loss: 0.2667\n",
      "Validation Loss: 0.2224, Accuracy: 89.41%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [61/200] \n",
      "Training Loss: 0.2282\n",
      "Validation Loss: 0.2035, Accuracy: 92.21%, Ice Berg Accuracy: 95.73%\n",
      "Epoch [62/200] \n",
      "Training Loss: 0.2393\n",
      "Validation Loss: 0.2245, Accuracy: 90.97%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [63/200] \n",
      "Training Loss: 0.2671\n",
      "Validation Loss: 0.1958, Accuracy: 91.90%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [64/200] \n",
      "Training Loss: 0.2469\n",
      "Validation Loss: 0.2162, Accuracy: 90.34%, Ice Berg Accuracy: 87.20%\n",
      "Epoch [65/200] \n",
      "Training Loss: 0.2093\n",
      "Validation Loss: 0.1886, Accuracy: 92.21%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [66/200] \n",
      "Training Loss: 0.2288\n",
      "Validation Loss: 0.2129, Accuracy: 92.83%, Ice Berg Accuracy: 95.12%\n",
      "Epoch [67/200] \n",
      "Training Loss: 0.2328\n",
      "Validation Loss: 0.1909, Accuracy: 91.59%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [68/200] \n",
      "Training Loss: 0.2606\n",
      "Validation Loss: 0.1966, Accuracy: 92.21%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [69/200] \n",
      "Training Loss: 0.2437\n",
      "Validation Loss: 0.1886, Accuracy: 91.59%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [70/200] \n",
      "Training Loss: 0.2215\n",
      "Validation Loss: 0.2197, Accuracy: 91.90%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [71/200] \n",
      "Training Loss: 0.2086\n",
      "Validation Loss: 0.2124, Accuracy: 90.97%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [72/200] \n",
      "Training Loss: 0.2267\n",
      "Validation Loss: 0.2450, Accuracy: 89.41%, Ice Berg Accuracy: 87.20%\n",
      "Epoch [73/200] \n",
      "Training Loss: 0.2104\n",
      "Validation Loss: 0.2320, Accuracy: 90.65%, Ice Berg Accuracy: 89.63%\n",
      "Epoch [74/200] \n",
      "Training Loss: 0.2170\n",
      "Validation Loss: 0.2103, Accuracy: 92.52%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [75/200] \n",
      "Training Loss: 0.2034\n",
      "Validation Loss: 0.2698, Accuracy: 89.72%, Ice Berg Accuracy: 84.76%\n",
      "Epoch [76/200] \n",
      "Training Loss: 0.2233\n",
      "Validation Loss: 0.2077, Accuracy: 92.83%, Ice Berg Accuracy: 95.73%\n",
      "Epoch [77/200] \n",
      "Training Loss: 0.1961\n",
      "Validation Loss: 0.1890, Accuracy: 92.83%, Ice Berg Accuracy: 93.29%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/200] \n",
      "Training Loss: 0.2271\n",
      "Validation Loss: 0.2177, Accuracy: 92.52%, Ice Berg Accuracy: 91.46%\n",
      "Epoch [79/200] \n",
      "Training Loss: 0.2264\n",
      "Validation Loss: 0.2344, Accuracy: 89.72%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [80/200] \n",
      "Training Loss: 0.2170\n",
      "Validation Loss: 0.2080, Accuracy: 91.28%, Ice Berg Accuracy: 91.46%\n",
      "Epoch [81/200] \n",
      "Training Loss: 0.2198\n",
      "Validation Loss: 0.2776, Accuracy: 88.47%, Ice Berg Accuracy: 95.73%\n",
      "Epoch [82/200] \n",
      "Training Loss: 0.2876\n",
      "Validation Loss: 0.2659, Accuracy: 90.65%, Ice Berg Accuracy: 88.41%\n",
      "Epoch [83/200] \n",
      "Training Loss: 0.2880\n",
      "Validation Loss: 0.2415, Accuracy: 90.97%, Ice Berg Accuracy: 88.41%\n",
      "Epoch [84/200] \n",
      "Training Loss: 0.2114\n",
      "Validation Loss: 0.2404, Accuracy: 90.97%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [85/200] \n",
      "Training Loss: 0.2387\n",
      "Validation Loss: 0.2322, Accuracy: 91.90%, Ice Berg Accuracy: 90.24%\n",
      "Epoch [86/200] \n",
      "Training Loss: 0.2588\n",
      "Validation Loss: 0.2345, Accuracy: 90.97%, Ice Berg Accuracy: 87.80%\n",
      "Epoch [87/200] \n",
      "Training Loss: 0.2658\n",
      "Validation Loss: 0.2092, Accuracy: 90.03%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [88/200] \n",
      "Training Loss: 0.3058\n",
      "Validation Loss: 0.1933, Accuracy: 93.15%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [89/200] \n",
      "Training Loss: 0.2549\n",
      "Validation Loss: 0.2227, Accuracy: 91.90%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [90/200] \n",
      "Training Loss: 0.2431\n",
      "Validation Loss: 0.2162, Accuracy: 92.21%, Ice Berg Accuracy: 95.73%\n",
      "Epoch [91/200] \n",
      "Training Loss: 0.3208\n",
      "Validation Loss: 0.2302, Accuracy: 92.52%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [92/200] \n",
      "Training Loss: 0.2764\n",
      "Validation Loss: 0.2319, Accuracy: 92.21%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [93/200] \n",
      "Training Loss: 0.2399\n",
      "Validation Loss: 0.2174, Accuracy: 91.59%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [94/200] \n",
      "Training Loss: 0.2175\n",
      "Validation Loss: 0.2014, Accuracy: 92.21%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [95/200] \n",
      "Training Loss: 0.2331\n",
      "Validation Loss: 0.2166, Accuracy: 91.59%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [96/200] \n",
      "Training Loss: 0.2061\n",
      "Validation Loss: 0.2342, Accuracy: 90.97%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [97/200] \n",
      "Training Loss: 0.2331\n",
      "Validation Loss: 0.2384, Accuracy: 90.97%, Ice Berg Accuracy: 87.80%\n",
      "Epoch [98/200] \n",
      "Training Loss: 0.2465\n",
      "Validation Loss: 0.2181, Accuracy: 91.59%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [99/200] \n",
      "Training Loss: 0.2224\n",
      "Validation Loss: 0.2189, Accuracy: 90.97%, Ice Berg Accuracy: 89.63%\n",
      "Epoch [100/200] \n",
      "Training Loss: 0.2267\n",
      "Validation Loss: 0.2181, Accuracy: 92.21%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [101/200] \n",
      "Training Loss: 0.1949\n",
      "Validation Loss: 0.2088, Accuracy: 92.21%, Ice Berg Accuracy: 91.46%\n",
      "Epoch [102/200] \n",
      "Training Loss: 0.2046\n",
      "Validation Loss: 0.2433, Accuracy: 91.59%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [103/200] \n",
      "Training Loss: 0.1881\n",
      "Validation Loss: 0.2298, Accuracy: 91.90%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [104/200] \n",
      "Training Loss: 0.2386\n",
      "Validation Loss: 0.2156, Accuracy: 90.97%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [105/200] \n",
      "Training Loss: 0.2347\n",
      "Validation Loss: 0.1914, Accuracy: 90.65%, Ice Berg Accuracy: 89.02%\n",
      "Epoch [106/200] \n",
      "Training Loss: 0.2038\n",
      "Validation Loss: 0.1952, Accuracy: 92.83%, Ice Berg Accuracy: 94.51%\n",
      "Epoch [107/200] \n",
      "Training Loss: 0.2531\n",
      "Validation Loss: 0.1963, Accuracy: 91.28%, Ice Berg Accuracy: 89.63%\n",
      "Epoch [108/200] \n",
      "Training Loss: 0.2019\n",
      "Validation Loss: 0.2041, Accuracy: 90.03%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [109/200] \n",
      "Training Loss: 0.2201\n",
      "Validation Loss: 0.1970, Accuracy: 92.52%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [110/200] \n",
      "Training Loss: 0.2024\n",
      "Validation Loss: 0.2077, Accuracy: 92.21%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [111/200] \n",
      "Training Loss: 0.2161\n",
      "Validation Loss: 0.2061, Accuracy: 91.59%, Ice Berg Accuracy: 89.63%\n",
      "Epoch [112/200] \n",
      "Training Loss: 0.2993\n",
      "Validation Loss: 0.2023, Accuracy: 91.90%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [113/200] \n",
      "Training Loss: 0.2022\n",
      "Validation Loss: 0.2041, Accuracy: 92.52%, Ice Berg Accuracy: 91.46%\n",
      "Epoch [114/200] \n",
      "Training Loss: 0.2118\n",
      "Validation Loss: 0.2099, Accuracy: 92.21%, Ice Berg Accuracy: 92.68%\n",
      "Epoch [115/200] \n",
      "Training Loss: 0.2055\n",
      "Validation Loss: 0.2320, Accuracy: 92.21%, Ice Berg Accuracy: 90.85%\n",
      "Epoch [116/200] \n",
      "Training Loss: 0.2117\n",
      "Validation Loss: 0.2026, Accuracy: 92.52%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [117/200] \n",
      "Training Loss: 0.2344\n",
      "Validation Loss: 0.2669, Accuracy: 89.41%, Ice Berg Accuracy: 85.37%\n",
      "Epoch [118/200] \n",
      "Training Loss: 0.1930\n",
      "Validation Loss: 0.1942, Accuracy: 92.21%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [119/200] \n",
      "Training Loss: 0.2233\n",
      "Validation Loss: 0.2006, Accuracy: 92.83%, Ice Berg Accuracy: 92.07%\n",
      "Epoch [120/200] \n",
      "Training Loss: 0.1898\n",
      "Validation Loss: 0.2147, Accuracy: 92.21%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [121/200] \n",
      "Training Loss: 0.1810\n",
      "Validation Loss: 0.2170, Accuracy: 92.83%, Ice Berg Accuracy: 93.29%\n",
      "Epoch [122/200] \n",
      "Training Loss: 0.1851\n",
      "Validation Loss: 0.2084, Accuracy: 91.90%, Ice Berg Accuracy: 93.90%\n",
      "Epoch [123/200] \n",
      "Training Loss: 0.2194\n",
      "Validation Loss: 0.2193, Accuracy: 91.28%, Ice Berg Accuracy: 88.41%\n",
      "Epoch [124/200] \n",
      "Training Loss: 0.2807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-425-dafee74b7698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch [%d/%d] \\nTraining Loss: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation Loss: %.4f, Accuracy: %.2f%%, Ice Berg Accuracy: %.2f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mis_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprec1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_prec1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-424-45bfc8f88e75>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mis_iceberg_tot\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "best_prec1 = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = []\n",
    "    for i, (features, features_angle, labels) in enumerate(train_loader):\n",
    "        iceNet.train()\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features = Variable(features).float()\n",
    "        features_angle = Variable(features_angle).cuda()\n",
    "        labels = Variable(labels).long()\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        writer.add_graph(iceNet, outputs)\n",
    "#         writer.add_histogram('hist_l1_1', iceNet.layer1[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_l1_2', iceNet.layer1[3].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_l2_1', iceNet.layer2[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_l2_2', iceNet.layer2[3].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_l3_1', iceNet.layer3[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_l3_2', iceNet.layer3[3].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc1', iceNet.fc1[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc2', iceNet.fc2[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc3', iceNet.fc3.weight.data.cpu().numpy(), i)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    prec1 = accuracy(val_loader)[0]\n",
    "    print ('Epoch [%d/%d] \\nTraining Loss: %.4f' % (epoch+1, num_epochs, np.mean(epoch_train_loss).data[0]))\n",
    "    print('Validation Loss: %.4f, Accuracy: %.2f%%, Ice Berg Accuracy: %.2f%%' % accuracy(val_loader))\n",
    "    \n",
    "    is_best = prec1 < best_prec1\n",
    "    best_prec1 = min(prec1, best_prec1)\n",
    "#     save_checkpoint({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'state_dict': iceNet.state_dict(),\n",
    "#         'best_prec1': best_prec1,\n",
    "#         'optimizer' : optimizer.state_dict(),\n",
    "#     }, is_best)\n",
    "\n",
    "print(best_prec1)\n",
    "# export scalar data to JSON for external processing\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "best_prec1 =  0.11030842363834381\n",
      "While copying the parameter named layer_shallow.0.weight, whose dimensions in the model are torch.Size([128, 2, 4, 4]) and whose dimensions in the checkpoint are torch.Size([128, 2, 3, 3]), ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: sizes do not match at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/THCTensorCopy.cu:31",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-227c481b680f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Models/model_best.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_prec1 = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_prec1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0miceNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mown_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 print('While copying the parameter named {}, whose dimensions in the model are'\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: sizes do not match at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/THCTensorCopy.cu:31"
     ]
    }
   ],
   "source": [
    "print(\"=> loading checkpoint\")\n",
    "best_model = torch.load('./Models/model_best.pth.tar')\n",
    "print('best_prec1 = ', best_model['best_prec1'])\n",
    "iceNet.load_state_dict(best_model['state_dict'])\n",
    "optimizer.load_state_dict(best_model['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_dataset = icebergDataset(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "iceNet.eval()\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "results = []\n",
    "for features, features_angle in test_loader:\n",
    "    iceNet.eval()\n",
    "    features = Variable(features, volatile=True).cuda()\n",
    "    features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "    outputs = F.softmax(iceNet(features, features_angle))\n",
    "#     outputs = iceNet(features, features_angle)\n",
    "\n",
    "    results.append(outputs.data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45908028059236167"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train>0.5)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30282526115859448"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(results)>0.5)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['is_iceberg'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('./Submissions/sub_30Oct_val_1631.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal(m.weight.data)\n",
    "\n",
    "def scale_range (input_, min_, max_):\n",
    "    tr_min = np.min(input_)\n",
    "    input_ += -(np.min(input_))\n",
    "    tr_max = np.max(input_)\n",
    "    input_ /= np.max(input_) / (max_ - min_)\n",
    "    input_ += min_\n",
    "    return input_, tr_min, tr_max\n",
    "\n",
    "def scale_test_range (input_, train_min, train_max, min_, max_):\n",
    "    input_ += -(train_min)\n",
    "    input_ /= train_max / (max_ - min_)\n",
    "    input_ += min_\n",
    "    return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_KF = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_KF = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "X_KF = np.stack((band_1_KF, band_2_KF), axis=1)\n",
    "# temp = np.concatenate((X_KF, X_test[test['not_machine_generated']]), axis=0)\n",
    "# X_KF, train_min, train_max = scale_range(X_KF, -1, 1)\n",
    "# X_KF = scale_test_range(X_KF, train_min, train_max, -1, 1)\n",
    "X_KF = [X_KF, np.array(data['inc_angle']).reshape((len(data), 1))]\n",
    "\n",
    "# X_test[test['not_machine_generated']] = X_all[len(data):]\n",
    "X_test = [X_test, np.array(test['inc_angle']).reshape((len(test), 1))]\n",
    "\n",
    "# X_test = [scale_test_range(X_test, train_min, train_max, -1, 1), np.array(test['inc_angle']).reshape((len(test), 1))]\n",
    "y_KF = data['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        labels = Variable(labels, volatile=True).cuda()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n",
      "Val Score : 0.175512\n",
      "Fold [2/5]\n"
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "kfold_scores = []\n",
    "\n",
    "test_dataset = icebergDataset(X_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "results = []\n",
    "sss = KFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X_KF[0], X_KF[1], y_KF)):\n",
    "    X_train_KF, X_valid_KF = [X_KF[0][train_index], X_KF[1][train_index]], [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "    y_train_KF, y_valid_KF = y_KF[train_index], y_KF[test_index]\n",
    "    \n",
    "    seed = 0\n",
    "    # random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Define model\n",
    "    iceNet = net().apply(weight_init).cuda()\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    \n",
    "    # Data Loader\n",
    "    train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, transform=True)\n",
    "    val_dataset_KF = icebergDataset(X_valid_KF, y_valid_KF)\n",
    "\n",
    "    train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_KF = torch.utils.data.DataLoader(dataset=val_dataset_KF, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print('Fold [%d/%d]' % (i+1, kfold))\n",
    "    # Train\n",
    "    best_prec1 = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = []\n",
    "        for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "            iceNet.train()\n",
    "            features = Variable(features).cuda()\n",
    "            features_angle = Variable(features_angle).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = iceNet(features, features_angle)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        prec1 = accuracy(val_loader_KF)[0]\n",
    "        \n",
    "        # Save best model\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': iceNet.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, filename='./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "        \n",
    "    print('Val Score : %f' % (best_prec1))\n",
    "    kfold_scores.append(best_prec1)\n",
    "    # Load best model\n",
    "    best_model = torch.load('./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "    iceNet.load_state_dict(best_model['state_dict'])\n",
    "    optimizer.load_state_dict(best_model['optimizer'])\n",
    "    \n",
    "    # Predict\n",
    "    iceNet.eval()\n",
    "    \n",
    "    results_fold = []\n",
    "    for features, features_angle in test_loader:\n",
    "        iceNet.eval()\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        outputs = F.softmax(iceNet(features, features_angle))\n",
    "    #     outputs = iceNet(features, features_angle)\n",
    "\n",
    "        results_fold.append(outputs.data[0][1])\n",
    "    \n",
    "    results.append(results_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14218716472387313"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(kfold_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')\n",
    "sub['is_iceberg'] = np.array(results).mean(axis=0)\n",
    "sub.to_csv('./Submissions/Sub 16 - 5-fold _ Val-1422.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sub['is_iceberg']>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('./Submissions/Sub 3 - 5 fold _ Val - 0.1504.csv')['is_iceberg']\n",
    "# sub2 = pd.read_csv('./Submissions/Sub 4 - 10-fold _ Val-1269.csv')['is_iceberg']\n",
    "sub3 = pd.read_csv('./Submissions/Sub 5 - 5-fold _ Val-1538.csv')['is_iceberg']\n",
    "sub4 = pd.read_csv('./Submissions/Sub 6 - 5-fold _ Val-1480.csv')['is_iceberg']\n",
    "sub5 = pd.read_csv('./Submissions/Sub 8 - 5-fold _ Val-1479.csv')['is_iceberg']\n",
    "# sub6 = pd.read_csv('./Submissions/Sub 9 - 5-fold _ Val-1480.csv')['is_iceberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = np.mean((np.array(sub1), np.array(sub3),\n",
    "                   np.array(sub4), np.array(sub5)), axis=0)\n",
    "sub['is_iceberg'] = np.array(results)\n",
    "sub.to_csv('./Submissions/Sub 13 - Ensemble_3_5_6_8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
