{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.set_start_method('spawn') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shutil\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "seed = 0\n",
    "# random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "# from utils_3 import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('./Data/train.json')\n",
    "test = pd.read_json('./Data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "test['band_1'] = test['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "test['band_2'] = test['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce').fillna(0.0)\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "\n",
    "train, val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_range (input_, min_, max_):\n",
    "    input_ += -(np.min(input_))\n",
    "    input_ /= np.max(input_) / (max_ - min_)\n",
    "    input_ += min_\n",
    "    return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_tr = np.concatenate([im for im in train['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_tr = np.concatenate([im for im in train['band_2']]).reshape(-1, 75, 75)\n",
    "X_train = np.stack((band_1_tr, band_2_tr), axis=1)\n",
    "X_train = [X_train, np.array(train['inc_angle']).reshape((len(train), 1))]\n",
    "\n",
    "band_1_val = np.concatenate([im for im in val['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_val = np.concatenate([im for im in val['band_2']]).reshape(-1, 75, 75)\n",
    "X_val = np.stack((band_1_val, band_2_val), axis=1)\n",
    "X_val = [X_val, np.array(val['inc_angle']).reshape((len(val), 1))]\n",
    "\n",
    "band_1_test = np.concatenate([im for im in test['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_test = np.concatenate([im for im in test['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3_test = scale_range(band_1_test/band_2_test, -1, 1)\n",
    "rgb = np.stack((band_1_test, band_2_test), axis=1)\n",
    "X_test = [rgb, np.array(test['inc_angle']).reshape((len(test), 1))]\n",
    "\n",
    "y_train = train['is_iceberg'].values.astype(np.float32)\n",
    "y_val = val['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "width = 75\n",
    "height = 75\n",
    "channels = 2\n",
    "padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "class icebergDataset(data_utils.Dataset):\n",
    "    \"\"\"Iceberg-Ship dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y=None, transform=None, u_ed=0.2, u_zo=0.5, u_noisy=0.4, u_shift=0.3):\n",
    "        self.X_images = X[0]\n",
    "        self.X_angles = torch.from_numpy(X[1]).float()\n",
    "        if y!=None:\n",
    "            self.y = torch.from_numpy(y).long()\n",
    "        else:\n",
    "            self.y=None\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.u_ed = u_ed\n",
    "        self.u_zo = u_zo\n",
    "        self.u_noisy = u_noisy\n",
    "        self.u_shift = u_shift\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.X_images[idx]\n",
    "        if self.transform:\n",
    "#             if np.random.random() < 0.2:\n",
    "#                 im = cv2.blur(im, (2,2))\n",
    "            im = randomErodeDilate(im, u=self.u_ed)\n",
    "            im = randomZoomOut(im, u=self.u_zo)\n",
    "            im = randomNoisy(im, u=self.u_noisy)\n",
    "            im = randomShift(im, u=self.u_shift)\n",
    "        \n",
    "        try:\n",
    "            if self.y==None:\n",
    "                return [torch.from_numpy(im).float(), self.X_angles[idx]]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return [torch.from_numpy(im).float(), self.X_angles[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='./Models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, './Models/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = icebergDataset(X_train, y_train, transform=True)\n",
    "val_dataset = icebergDataset(X_val, y_val)\n",
    "test_dataset = icebergDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act = nn.LeakyReLU()\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.batch = nn.BatchNorm2d(channels)\n",
    "        self.batch1D = nn.BatchNorm1d(1)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, 9, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(9),\n",
    "            act,\n",
    "            nn.Conv2d(9, 18, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(18, 24, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(24),\n",
    "            act,\n",
    "            nn.Conv2d(24, 36, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(36),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(36, 72, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(72),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(72, 144, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(144),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.layer_shallow = nn.Sequential(\n",
    "            nn.Conv2d(channels, 128, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1+(3*3*144), 512),\n",
    "            act,\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 196),\n",
    "            act,\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc3 = nn.Linear(196, 2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x_im, x_angle):\n",
    "        x_im = self.batch(x_im)\n",
    "        x_angle = self.batch1D(x_angle)\n",
    "        out = self.layer1(x_im)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         out_shallow = self.layer_shallow(x_im)\n",
    "#         out_shallow  = F.max_pool2d(out_shallow, kernel_size=out_shallow.size()[2:])\n",
    "#         out_shallow = out_shallow.view(out_shallow.size(0), -1)\n",
    "        out = torch.cat([out, x_angle], dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 12\n",
    "# padding = 1\n",
    "# act = nn.LeakyReLU()\n",
    "# class net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(net, self).__init__()\n",
    "#         self.batch = nn.BatchNorm2d(channels)\n",
    "#         self.batch1D = nn.BatchNorm1d(1)\n",
    "#         self.init = nn.Sequential(\n",
    "#             nn.Conv2d(channels, k*2, kernel_size=3, padding=padding),\n",
    "#             nn.BatchNorm2d(k*2),\n",
    "#             act,\n",
    "#             nn.Dropout(0))\n",
    "#         self.b1l1 = nn.Sequential(\n",
    "#             nn.Conv2d(k*2, k, kernel_size=3, padding=padding),\n",
    "#             nn.BatchNorm2d(k),\n",
    "#             act,\n",
    "#             nn.Dropout(0))\n",
    "#         self.b1l2 = nn.Sequential(\n",
    "#             nn.Conv2d(k*3, k, kernel_size=3, padding=padding),\n",
    "#             nn.BatchNorm2d(k),\n",
    "#             act,\n",
    "#             nn.Dropout(0))\n",
    "#         self.transition1 = nn.Sequential(\n",
    "#             nn.Conv2d(k*4, k*4, kernel_size=1, padding=0),\n",
    "#             nn.BatchNorm2d(k*4),\n",
    "#             act,\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0))\n",
    "#         self.b2l1 = nn.Sequential(\n",
    "#             nn.Conv2d(k*4, k, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(k),\n",
    "#             act,\n",
    "#             nn.Dropout(0))\n",
    "#         self.b2l2 = nn.Sequential(\n",
    "#             nn.Conv2d(k*5, k, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(k),\n",
    "#             act,\n",
    "#             nn.Dropout(0))\n",
    "#         self.transition2 = nn.Sequential(\n",
    "#             nn.Conv2d(k*6, k*6, kernel_size=1, padding=0),\n",
    "#             nn.BatchNorm2d(k*6),\n",
    "#             act,\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0))\n",
    "#         self.b3l1 = nn.Sequential(\n",
    "#             nn.Conv2d(k*6, k, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(k),\n",
    "#             act,\n",
    "#             nn.Dropout(0))\n",
    "#         self.b3l2 = nn.Sequential(\n",
    "#             nn.Conv2d(k*7, k, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(k),\n",
    "#             act,\n",
    "#             nn.Dropout(0))\n",
    "#         self.transition3 = nn.Sequential(\n",
    "#             nn.Conv2d(k*8, k*8, kernel_size=1, padding=0),\n",
    "#             nn.BatchNorm2d(k*8),\n",
    "#             act,\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0))\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(k*8, k*6, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(k*6),\n",
    "#             act,\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0))\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(k*6, k*4, kernel_size=2, padding=0),\n",
    "#             nn.BatchNorm2d(k*4),\n",
    "#             act,\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0))\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(1+(4*4*k*6), 512),\n",
    "#             act,\n",
    "#             nn.Dropout(0.5))\n",
    "#         self.fc2 = nn.Sequential(\n",
    "#             nn.Linear(512, 196),\n",
    "#             act,\n",
    "#             nn.Dropout(0.3))\n",
    "#         self.fc3 = nn.Linear(196, 2)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "#     def forward(self, x_im, x_angle):\n",
    "#         x_im = self.batch(x_im)\n",
    "#         x_angle = self.batch1D(x_angle)\n",
    "#         out_prev = self.init(x_im)\n",
    "#         out = self.b1l1(out_prev)\n",
    "#         out_prev = torch.cat([out, out_prev], dim=1)\n",
    "#         out = self.b1l2(out_prev)\n",
    "#         out_prev = torch.cat([out, out_prev], dim=1)\n",
    "#         out_prev = self.transition1(out_prev)\n",
    "#         out = self.b2l1(out_prev)\n",
    "#         out_prev = torch.cat([F.pad(out, (0,1,0,1), \"constant\", 0), out_prev], dim=1)\n",
    "#         out = self.b2l2(out_prev)\n",
    "#         out_prev = torch.cat([F.pad(out, (0,1,0,1), \"constant\", 0), out_prev], dim=1)\n",
    "#         out_prev = self.transition2(out_prev)\n",
    "#         out = self.b3l1(out_prev)\n",
    "#         out_prev = torch.cat([F.pad(out, (0,1,0,1), \"constant\", 0), out_prev], dim=1)\n",
    "#         out = self.b3l2(out_prev)\n",
    "#         out_prev = torch.cat([F.pad(out, (0,1,0,1), \"constant\", 0), out_prev], dim=1)\n",
    "#         out_prev = self.transition3(out_prev)\n",
    "#         out = self.layer3(out_prev)\n",
    "# #         out = self.layer4(out)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "# #         out_shallow = self.layer_shallow(x_im)\n",
    "# #         out_shallow  = F.max_pool2d(out_shallow, kernel_size=out_shallow.size()[2:])\n",
    "# #         out_shallow = out_shallow.view(out_shallow.size(0), -1)\n",
    "#         out = torch.cat([out, x_angle], dim=1)\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.fc3(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (\n",
       "  (batch): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (batch1D): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (layer1): Sequential (\n",
       "    (0): Conv2d(2, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): Conv2d(9, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): LeakyReLU (0.01)\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): Conv2d(18, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): Conv2d(24, 36, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): LeakyReLU (0.01)\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer3): Sequential (\n",
       "    (0): Conv2d(36, 72, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer4): Sequential (\n",
       "    (0): Conv2d(72, 144, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0.2)\n",
       "  )\n",
       "  (layer_shallow): Sequential (\n",
       "    (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU (0.01)\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0.2)\n",
       "  )\n",
       "  (fc1): Sequential (\n",
       "    (0): Linear (1297 -> 512)\n",
       "    (1): LeakyReLU (0.01)\n",
       "    (2): Dropout (p = 0.5)\n",
       "  )\n",
       "  (fc2): Sequential (\n",
       "    (0): Linear (512 -> 196)\n",
       "    (1): LeakyReLU (0.01)\n",
       "    (2): Dropout (p = 0.5)\n",
       "  )\n",
       "  (fc3): Linear (196 -> 2)\n",
       "  (sig): Sigmoid ()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "iceNet = net()\n",
    "iceNet.apply(weight_init).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features_angle = features_angle.cuda()\n",
    "        features = Variable(features, volatile=True)\n",
    "        features_angle = Variable(features_angle, volatile=True)\n",
    "        labels = Variable(labels, volatile=True)\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] \n",
      "Training Loss: 0.6991\n",
      "Validation Loss: 0.6543, Accuracy: 64.80%\n",
      "Epoch [2/200] \n",
      "Training Loss: 0.6406\n",
      "Validation Loss: 0.6147, Accuracy: 70.72%\n",
      "Epoch [3/200] \n",
      "Training Loss: 0.5921\n",
      "Validation Loss: 0.5030, Accuracy: 70.09%\n",
      "Epoch [4/200] \n",
      "Training Loss: 0.5500\n",
      "Validation Loss: 0.5145, Accuracy: 75.08%\n",
      "Epoch [5/200] \n",
      "Training Loss: 0.4971\n",
      "Validation Loss: 0.3902, Accuracy: 80.69%\n",
      "Epoch [6/200] \n",
      "Training Loss: 0.4976\n",
      "Validation Loss: 0.3870, Accuracy: 81.31%\n",
      "Epoch [7/200] \n",
      "Training Loss: 0.4698\n",
      "Validation Loss: 0.3783, Accuracy: 83.49%\n",
      "Epoch [8/200] \n",
      "Training Loss: 0.4198\n",
      "Validation Loss: 0.3471, Accuracy: 84.11%\n",
      "Epoch [9/200] \n",
      "Training Loss: 0.4284\n",
      "Validation Loss: 0.3241, Accuracy: 85.67%\n",
      "Epoch [10/200] \n",
      "Training Loss: 0.4338\n",
      "Validation Loss: 0.2853, Accuracy: 86.92%\n",
      "Epoch [11/200] \n",
      "Training Loss: 0.4502\n",
      "Validation Loss: 0.2748, Accuracy: 87.85%\n",
      "Epoch [12/200] \n",
      "Training Loss: 0.4213\n",
      "Validation Loss: 0.2817, Accuracy: 87.85%\n",
      "Epoch [13/200] \n",
      "Training Loss: 0.3677\n",
      "Validation Loss: 0.2528, Accuracy: 87.85%\n",
      "Epoch [14/200] \n",
      "Training Loss: 0.3634\n",
      "Validation Loss: 0.2372, Accuracy: 88.16%\n",
      "Epoch [15/200] \n",
      "Training Loss: 0.3632\n",
      "Validation Loss: 0.2272, Accuracy: 90.03%\n",
      "Epoch [16/200] \n",
      "Training Loss: 0.3860\n",
      "Validation Loss: 0.3246, Accuracy: 84.11%\n",
      "Epoch [17/200] \n",
      "Training Loss: 0.4025\n",
      "Validation Loss: 0.2268, Accuracy: 89.10%\n",
      "Epoch [18/200] \n",
      "Training Loss: 0.4365\n",
      "Validation Loss: 0.2482, Accuracy: 88.16%\n",
      "Epoch [19/200] \n",
      "Training Loss: 0.3392\n",
      "Validation Loss: 0.2144, Accuracy: 89.41%\n",
      "Epoch [20/200] \n",
      "Training Loss: 0.3258\n",
      "Validation Loss: 0.2111, Accuracy: 88.79%\n",
      "Epoch [21/200] \n",
      "Training Loss: 0.3459\n",
      "Validation Loss: 0.2627, Accuracy: 87.23%\n",
      "Epoch [22/200] \n",
      "Training Loss: 0.3301\n",
      "Validation Loss: 0.2068, Accuracy: 89.72%\n",
      "Epoch [23/200] \n",
      "Training Loss: 0.3567\n",
      "Validation Loss: 0.2118, Accuracy: 89.72%\n",
      "Epoch [24/200] \n",
      "Training Loss: 0.3561\n",
      "Validation Loss: 0.2252, Accuracy: 90.65%\n",
      "Epoch [25/200] \n",
      "Training Loss: 0.4052\n",
      "Validation Loss: 0.2973, Accuracy: 87.23%\n",
      "Epoch [26/200] \n",
      "Training Loss: 0.3344\n",
      "Validation Loss: 0.2120, Accuracy: 88.47%\n",
      "Epoch [27/200] \n",
      "Training Loss: 0.2893\n",
      "Validation Loss: 0.2027, Accuracy: 89.10%\n",
      "Epoch [28/200] \n",
      "Training Loss: 0.2979\n",
      "Validation Loss: 0.2123, Accuracy: 88.47%\n",
      "Epoch [29/200] \n",
      "Training Loss: 0.3084\n",
      "Validation Loss: 0.2135, Accuracy: 90.65%\n",
      "Epoch [30/200] \n",
      "Training Loss: 0.3016\n",
      "Validation Loss: 0.2034, Accuracy: 92.52%\n",
      "Epoch [31/200] \n",
      "Training Loss: 0.3263\n",
      "Validation Loss: 0.2053, Accuracy: 91.28%\n",
      "Epoch [32/200] \n",
      "Training Loss: 0.3287\n",
      "Validation Loss: 0.2049, Accuracy: 90.97%\n",
      "Epoch [33/200] \n",
      "Training Loss: 0.3267\n",
      "Validation Loss: 0.2512, Accuracy: 87.85%\n",
      "Epoch [34/200] \n",
      "Training Loss: 0.3315\n",
      "Validation Loss: 0.2105, Accuracy: 91.59%\n",
      "Epoch [35/200] \n",
      "Training Loss: 0.2838\n",
      "Validation Loss: 0.1987, Accuracy: 91.90%\n",
      "Epoch [36/200] \n",
      "Training Loss: 0.3042\n",
      "Validation Loss: 0.2054, Accuracy: 90.34%\n",
      "Epoch [37/200] \n",
      "Training Loss: 0.2986\n",
      "Validation Loss: 0.2324, Accuracy: 89.41%\n",
      "Epoch [38/200] \n",
      "Training Loss: 0.2533\n",
      "Validation Loss: 0.2026, Accuracy: 91.59%\n",
      "Epoch [39/200] \n",
      "Training Loss: 0.2994\n",
      "Validation Loss: 0.2360, Accuracy: 89.41%\n",
      "Epoch [40/200] \n",
      "Training Loss: 0.2773\n",
      "Validation Loss: 0.2008, Accuracy: 89.10%\n",
      "Epoch [41/200] \n",
      "Training Loss: 0.2470\n",
      "Validation Loss: 0.1951, Accuracy: 92.21%\n",
      "Epoch [42/200] \n",
      "Training Loss: 0.2833\n",
      "Validation Loss: 0.2090, Accuracy: 91.90%\n",
      "Epoch [43/200] \n",
      "Training Loss: 0.2771\n",
      "Validation Loss: 0.2270, Accuracy: 89.72%\n",
      "Epoch [44/200] \n",
      "Training Loss: 0.2555\n",
      "Validation Loss: 0.1961, Accuracy: 91.59%\n",
      "Epoch [45/200] \n",
      "Training Loss: 0.2708\n",
      "Validation Loss: 0.2005, Accuracy: 91.28%\n",
      "Epoch [46/200] \n",
      "Training Loss: 0.3273\n",
      "Validation Loss: 0.1976, Accuracy: 90.97%\n",
      "Epoch [47/200] \n",
      "Training Loss: 0.3770\n",
      "Validation Loss: 0.2011, Accuracy: 90.03%\n",
      "Epoch [48/200] \n",
      "Training Loss: 0.3885\n",
      "Validation Loss: 0.1897, Accuracy: 90.03%\n",
      "Epoch [49/200] \n",
      "Training Loss: 0.3538\n",
      "Validation Loss: 0.1900, Accuracy: 91.59%\n",
      "Epoch [50/200] \n",
      "Training Loss: 0.2619\n",
      "Validation Loss: 0.1953, Accuracy: 90.65%\n",
      "Epoch [51/200] \n",
      "Training Loss: 0.2900\n",
      "Validation Loss: 0.1913, Accuracy: 91.59%\n",
      "Epoch [52/200] \n",
      "Training Loss: 0.2698\n",
      "Validation Loss: 0.1961, Accuracy: 90.03%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-23eeab7d09ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         writer.add_histogram('hist_fc2', iceNet.fc2[0].weight.data.cpu().numpy(), i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         writer.add_histogram('hist_fc3', iceNet.fc3.weight.data.cpu().numpy(), i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 482\u001b[0;31m                                self.ignore_index)\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maveraged\u001b[0m \u001b[0mover\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mignored\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \"\"\"\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, *params)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "best_prec1 = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = []\n",
    "    for i, (features, features_angle, labels) in enumerate(train_loader):\n",
    "        iceNet.train()\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features = Variable(features).float()\n",
    "        features_angle = Variable(features_angle).cuda()\n",
    "        labels = Variable(labels).long()\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        writer.add_graph(iceNet, outputs)\n",
    "#         writer.add_histogram('hist_fc1', iceNet.fc1[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc2', iceNet.fc2[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc3', iceNet.fc3.weight.data.cpu().numpy(), i)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    prec1 = accuracy(val_loader)[0]\n",
    "    print ('Epoch [%d/%d] \\nTraining Loss: %.4f' % (epoch+1, num_epochs, np.mean(epoch_train_loss).data[0]))\n",
    "    print('Validation Loss: %.4f, Accuracy: %.2f%%' % accuracy(val_loader))\n",
    "    \n",
    "    is_best = prec1 < best_prec1\n",
    "    best_prec1 = min(prec1, best_prec1)\n",
    "#     save_checkpoint({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'state_dict': iceNet.state_dict(),\n",
    "#         'best_prec1': best_prec1,\n",
    "#         'optimizer' : optimizer.state_dict(),\n",
    "#     }, is_best)\n",
    "\n",
    "print(best_prec1)\n",
    "# export scalar data to JSON for external processing\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] \n",
      "Training Loss: 0.6790\n",
      "Validation Loss: 0.6388, Accuracy: 65.11%\n",
      "Epoch [2/200] \n",
      "Training Loss: 0.6355\n",
      "Validation Loss: 0.6136, Accuracy: 69.47%\n",
      "Epoch [3/200] \n",
      "Training Loss: 0.5811\n",
      "Validation Loss: 0.4759, Accuracy: 72.59%\n",
      "Epoch [4/200] \n",
      "Training Loss: 0.5514\n",
      "Validation Loss: 0.4475, Accuracy: 76.95%\n",
      "Epoch [5/200] \n",
      "Training Loss: 0.5214\n",
      "Validation Loss: 0.4208, Accuracy: 78.82%\n",
      "Epoch [6/200] \n",
      "Training Loss: 0.4891\n",
      "Validation Loss: 0.3607, Accuracy: 81.31%\n",
      "Epoch [7/200] \n",
      "Training Loss: 0.4892\n",
      "Validation Loss: 0.3700, Accuracy: 85.05%\n",
      "Epoch [8/200] \n",
      "Training Loss: 0.4616\n",
      "Validation Loss: 0.3697, Accuracy: 83.49%\n",
      "Epoch [9/200] \n",
      "Training Loss: 0.4540\n",
      "Validation Loss: 0.3349, Accuracy: 85.98%\n",
      "Epoch [10/200] \n",
      "Training Loss: 0.4480\n",
      "Validation Loss: 0.3157, Accuracy: 85.98%\n",
      "Epoch [11/200] \n",
      "Training Loss: 0.4417\n",
      "Validation Loss: 0.2621, Accuracy: 88.47%\n",
      "Epoch [12/200] \n",
      "Training Loss: 0.4271\n",
      "Validation Loss: 0.2540, Accuracy: 89.72%\n",
      "Epoch [13/200] \n",
      "Training Loss: 0.3835\n",
      "Validation Loss: 0.2618, Accuracy: 89.72%\n",
      "Epoch [14/200] \n",
      "Training Loss: 0.3730\n",
      "Validation Loss: 0.2807, Accuracy: 88.79%\n",
      "Epoch [15/200] \n",
      "Training Loss: 0.3818\n",
      "Validation Loss: 0.2178, Accuracy: 90.34%\n",
      "Epoch [16/200] \n",
      "Training Loss: 0.3684\n",
      "Validation Loss: 0.3440, Accuracy: 83.49%\n",
      "Epoch [17/200] \n",
      "Training Loss: 0.3666\n",
      "Validation Loss: 0.2251, Accuracy: 89.41%\n",
      "Epoch [18/200] \n",
      "Training Loss: 0.4245\n",
      "Validation Loss: 0.2589, Accuracy: 86.29%\n",
      "Epoch [19/200] \n",
      "Training Loss: 0.3605\n",
      "Validation Loss: 0.2293, Accuracy: 89.72%\n",
      "Epoch [20/200] \n",
      "Training Loss: 0.3500\n",
      "Validation Loss: 0.2313, Accuracy: 88.79%\n",
      "Epoch [21/200] \n",
      "Training Loss: 0.3337\n",
      "Validation Loss: 0.2270, Accuracy: 90.34%\n",
      "Epoch [22/200] \n",
      "Training Loss: 0.3509\n",
      "Validation Loss: 0.2036, Accuracy: 90.97%\n",
      "Epoch [23/200] \n",
      "Training Loss: 0.3490\n",
      "Validation Loss: 0.2457, Accuracy: 87.54%\n",
      "Epoch [24/200] \n",
      "Training Loss: 0.3631\n",
      "Validation Loss: 0.2328, Accuracy: 91.59%\n",
      "Epoch [25/200] \n",
      "Training Loss: 0.3950\n",
      "Validation Loss: 0.2090, Accuracy: 91.28%\n",
      "Epoch [26/200] \n",
      "Training Loss: 0.3277\n",
      "Validation Loss: 0.2254, Accuracy: 88.79%\n",
      "Epoch [27/200] \n",
      "Training Loss: 0.3179\n",
      "Validation Loss: 0.2033, Accuracy: 89.41%\n",
      "Epoch [28/200] \n",
      "Training Loss: 0.2972\n",
      "Validation Loss: 0.2123, Accuracy: 90.34%\n",
      "Epoch [29/200] \n",
      "Training Loss: 0.2811\n",
      "Validation Loss: 0.2347, Accuracy: 88.16%\n",
      "Epoch [30/200] \n",
      "Training Loss: 0.2802\n",
      "Validation Loss: 0.2029, Accuracy: 91.28%\n",
      "Epoch [31/200] \n",
      "Training Loss: 0.3580\n",
      "Validation Loss: 0.1999, Accuracy: 91.59%\n",
      "Epoch [32/200] \n",
      "Training Loss: 0.3228\n",
      "Validation Loss: 0.1890, Accuracy: 90.97%\n",
      "Epoch [33/200] \n",
      "Training Loss: 0.3803\n",
      "Validation Loss: 0.2338, Accuracy: 87.54%\n",
      "Epoch [34/200] \n",
      "Training Loss: 0.3743\n",
      "Validation Loss: 0.2210, Accuracy: 89.41%\n",
      "Epoch [35/200] \n",
      "Training Loss: 0.2818\n",
      "Validation Loss: 0.1948, Accuracy: 90.03%\n",
      "Epoch [36/200] \n",
      "Training Loss: 0.3116\n",
      "Validation Loss: 0.1974, Accuracy: 90.03%\n",
      "Epoch [37/200] \n",
      "Training Loss: 0.3023\n",
      "Validation Loss: 0.1968, Accuracy: 90.97%\n",
      "Epoch [38/200] \n",
      "Training Loss: 0.2586\n",
      "Validation Loss: 0.1992, Accuracy: 90.65%\n",
      "Epoch [39/200] \n",
      "Training Loss: 0.3127\n",
      "Validation Loss: 0.2048, Accuracy: 90.65%\n",
      "Epoch [40/200] \n",
      "Training Loss: 0.2854\n",
      "Validation Loss: 0.2312, Accuracy: 87.85%\n",
      "Epoch [41/200] \n",
      "Training Loss: 0.2527\n",
      "Validation Loss: 0.1942, Accuracy: 91.28%\n",
      "Epoch [42/200] \n",
      "Training Loss: 0.3012\n",
      "Validation Loss: 0.1995, Accuracy: 90.34%\n",
      "Epoch [43/200] \n",
      "Training Loss: 0.2910\n",
      "Validation Loss: 0.2181, Accuracy: 91.59%\n",
      "Epoch [44/200] \n",
      "Training Loss: 0.2795\n",
      "Validation Loss: 0.1949, Accuracy: 91.59%\n",
      "Epoch [45/200] \n",
      "Training Loss: 0.2814\n",
      "Validation Loss: 0.2239, Accuracy: 89.72%\n",
      "Epoch [46/200] \n",
      "Training Loss: 0.3881\n",
      "Validation Loss: 0.2012, Accuracy: 91.28%\n",
      "Epoch [47/200] \n",
      "Training Loss: 0.3683\n",
      "Validation Loss: 0.2196, Accuracy: 90.97%\n",
      "Epoch [48/200] \n",
      "Training Loss: 0.3313\n",
      "Validation Loss: 0.2169, Accuracy: 89.72%\n",
      "Epoch [49/200] \n",
      "Training Loss: 0.3331\n",
      "Validation Loss: 0.2097, Accuracy: 90.34%\n",
      "Epoch [50/200] \n",
      "Training Loss: 0.2745\n",
      "Validation Loss: 0.1879, Accuracy: 90.34%\n",
      "Epoch [51/200] \n",
      "Training Loss: 0.2712\n",
      "Validation Loss: 0.1812, Accuracy: 91.59%\n",
      "Epoch [52/200] \n",
      "Training Loss: 0.2747\n",
      "Validation Loss: 0.1837, Accuracy: 91.28%\n",
      "Epoch [53/200] \n",
      "Training Loss: 0.2535\n",
      "Validation Loss: 0.1993, Accuracy: 90.03%\n",
      "Epoch [54/200] \n",
      "Training Loss: 0.2668\n",
      "Validation Loss: 0.1875, Accuracy: 92.21%\n",
      "Epoch [55/200] \n",
      "Training Loss: 0.2539\n",
      "Validation Loss: 0.1963, Accuracy: 91.59%\n",
      "Epoch [56/200] \n",
      "Training Loss: 0.2719\n",
      "Validation Loss: 0.1951, Accuracy: 90.97%\n",
      "Epoch [57/200] \n",
      "Training Loss: 0.2317\n",
      "Validation Loss: 0.2034, Accuracy: 90.34%\n",
      "Epoch [58/200] \n",
      "Training Loss: 0.2864\n",
      "Validation Loss: 0.1924, Accuracy: 91.90%\n",
      "Epoch [59/200] \n",
      "Training Loss: 0.2731\n",
      "Validation Loss: 0.2048, Accuracy: 90.97%\n",
      "Epoch [60/200] \n",
      "Training Loss: 0.2341\n",
      "Validation Loss: 0.2138, Accuracy: 91.59%\n",
      "Epoch [61/200] \n",
      "Training Loss: 0.2805\n",
      "Validation Loss: 0.1898, Accuracy: 89.41%\n",
      "Epoch [62/200] \n",
      "Training Loss: 0.2697\n",
      "Validation Loss: 0.1925, Accuracy: 90.97%\n",
      "Epoch [63/200] \n",
      "Training Loss: 0.2492\n",
      "Validation Loss: 0.1991, Accuracy: 90.65%\n",
      "Epoch [64/200] \n",
      "Training Loss: 0.2501\n",
      "Validation Loss: 0.2146, Accuracy: 89.72%\n",
      "Epoch [65/200] \n",
      "Training Loss: 0.2637\n",
      "Validation Loss: 0.1957, Accuracy: 90.65%\n",
      "Epoch [66/200] \n",
      "Training Loss: 0.2714\n",
      "Validation Loss: 0.2112, Accuracy: 90.03%\n",
      "Epoch [67/200] \n",
      "Training Loss: 0.2459\n",
      "Validation Loss: 0.2368, Accuracy: 90.34%\n",
      "Epoch [68/200] \n",
      "Training Loss: 0.2274\n",
      "Validation Loss: 0.2005, Accuracy: 90.65%\n",
      "Epoch [69/200] \n",
      "Training Loss: 0.2711\n",
      "Validation Loss: 0.1922, Accuracy: 91.90%\n",
      "Epoch [70/200] \n",
      "Training Loss: 0.2496\n",
      "Validation Loss: 0.2045, Accuracy: 90.65%\n",
      "Epoch [71/200] \n",
      "Training Loss: 0.2617\n",
      "Validation Loss: 0.2101, Accuracy: 90.65%\n",
      "Epoch [72/200] \n",
      "Training Loss: 0.2496\n",
      "Validation Loss: 0.1921, Accuracy: 89.41%\n",
      "Epoch [73/200] \n",
      "Training Loss: 0.2403\n",
      "Validation Loss: 0.1952, Accuracy: 90.34%\n",
      "Epoch [74/200] \n",
      "Training Loss: 0.2468\n",
      "Validation Loss: 0.2303, Accuracy: 89.41%\n",
      "Epoch [75/200] \n",
      "Training Loss: 0.2389\n",
      "Validation Loss: 0.2049, Accuracy: 90.65%\n",
      "Epoch [76/200] \n",
      "Training Loss: 0.2382\n",
      "Validation Loss: 0.1937, Accuracy: 91.28%\n",
      "Epoch [77/200] \n",
      "Training Loss: 0.2251\n",
      "Validation Loss: 0.1995, Accuracy: 91.59%\n",
      "Epoch [78/200] \n",
      "Training Loss: 0.2355\n",
      "Validation Loss: 0.2224, Accuracy: 90.65%\n",
      "Epoch [79/200] \n",
      "Training Loss: 0.2427\n",
      "Validation Loss: 0.2648, Accuracy: 89.41%\n",
      "Epoch [80/200] \n",
      "Training Loss: 0.2586\n",
      "Validation Loss: 0.2253, Accuracy: 90.34%\n",
      "Epoch [81/200] \n",
      "Training Loss: 0.2408\n",
      "Validation Loss: 0.2131, Accuracy: 90.34%\n",
      "Epoch [82/200] \n",
      "Training Loss: 0.2560\n",
      "Validation Loss: 0.2122, Accuracy: 89.10%\n",
      "Epoch [83/200] \n",
      "Training Loss: 0.2204\n",
      "Validation Loss: 0.2235, Accuracy: 89.72%\n",
      "Epoch [84/200] \n",
      "Training Loss: 0.2163\n",
      "Validation Loss: 0.2320, Accuracy: 88.79%\n",
      "Epoch [85/200] \n",
      "Training Loss: 0.2258\n",
      "Validation Loss: 0.2120, Accuracy: 90.65%\n",
      "Epoch [86/200] \n",
      "Training Loss: 0.2375\n",
      "Validation Loss: 0.2277, Accuracy: 90.03%\n",
      "Epoch [87/200] \n",
      "Training Loss: 0.2312\n",
      "Validation Loss: 0.2088, Accuracy: 90.65%\n",
      "Epoch [88/200] \n",
      "Training Loss: 0.2285\n",
      "Validation Loss: 0.1953, Accuracy: 90.97%\n",
      "Epoch [89/200] \n",
      "Training Loss: 0.2695\n",
      "Validation Loss: 0.2089, Accuracy: 89.72%\n",
      "Epoch [90/200] \n",
      "Training Loss: 0.3236\n",
      "Validation Loss: 0.2135, Accuracy: 90.97%\n",
      "Epoch [91/200] \n",
      "Training Loss: 0.2497\n",
      "Validation Loss: 0.2414, Accuracy: 90.97%\n",
      "Epoch [92/200] \n",
      "Training Loss: 0.2480\n",
      "Validation Loss: 0.2054, Accuracy: 89.10%\n",
      "Epoch [93/200] \n",
      "Training Loss: 0.2359\n",
      "Validation Loss: 0.2152, Accuracy: 90.97%\n",
      "Epoch [94/200] \n",
      "Training Loss: 0.2366\n",
      "Validation Loss: 0.2695, Accuracy: 89.10%\n",
      "Epoch [95/200] \n",
      "Training Loss: 0.2526\n",
      "Validation Loss: 0.2095, Accuracy: 89.41%\n",
      "Epoch [96/200] \n",
      "Training Loss: 0.2383\n",
      "Validation Loss: 0.2075, Accuracy: 90.97%\n",
      "Epoch [97/200] \n",
      "Training Loss: 0.2231\n",
      "Validation Loss: 0.2067, Accuracy: 89.72%\n",
      "Epoch [98/200] \n",
      "Training Loss: 0.2326\n",
      "Validation Loss: 0.1936, Accuracy: 90.34%\n",
      "Epoch [99/200] \n",
      "Training Loss: 0.2355\n",
      "Validation Loss: 0.1955, Accuracy: 90.97%\n",
      "Epoch [100/200] \n",
      "Training Loss: 0.2323\n",
      "Validation Loss: 0.2335, Accuracy: 88.16%\n",
      "Epoch [101/200] \n",
      "Training Loss: 0.2263\n",
      "Validation Loss: 0.2081, Accuracy: 89.10%\n",
      "Epoch [102/200] \n",
      "Training Loss: 0.2581\n",
      "Validation Loss: 0.1917, Accuracy: 89.72%\n",
      "Epoch [103/200] \n",
      "Training Loss: 0.2245\n",
      "Validation Loss: 0.1903, Accuracy: 91.59%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [104/200] \n",
      "Training Loss: 0.2253\n",
      "Validation Loss: 0.2214, Accuracy: 89.10%\n",
      "Epoch [105/200] \n",
      "Training Loss: 0.2270\n",
      "Validation Loss: 0.1969, Accuracy: 90.97%\n",
      "Epoch [106/200] \n",
      "Training Loss: 0.2243\n",
      "Validation Loss: 0.1922, Accuracy: 91.28%\n",
      "Epoch [107/200] \n",
      "Training Loss: 0.2205\n",
      "Validation Loss: 0.2045, Accuracy: 89.41%\n",
      "Epoch [108/200] \n",
      "Training Loss: 0.2093\n",
      "Validation Loss: 0.2181, Accuracy: 90.65%\n",
      "Epoch [109/200] \n",
      "Training Loss: 0.2162\n",
      "Validation Loss: 0.2180, Accuracy: 89.41%\n",
      "Epoch [110/200] \n",
      "Training Loss: 0.2386\n",
      "Validation Loss: 0.2187, Accuracy: 90.03%\n",
      "Epoch [111/200] \n",
      "Training Loss: 0.2200\n",
      "Validation Loss: 0.2048, Accuracy: 91.59%\n",
      "Epoch [112/200] \n",
      "Training Loss: 0.2314\n",
      "Validation Loss: 0.2174, Accuracy: 89.10%\n",
      "Epoch [113/200] \n",
      "Training Loss: 0.2096\n",
      "Validation Loss: 0.2209, Accuracy: 89.41%\n",
      "Epoch [114/200] \n",
      "Training Loss: 0.2639\n",
      "Validation Loss: 0.1913, Accuracy: 91.90%\n",
      "Epoch [115/200] \n",
      "Training Loss: 0.2698\n",
      "Validation Loss: 0.2058, Accuracy: 90.65%\n",
      "Epoch [116/200] \n",
      "Training Loss: 0.2257\n",
      "Validation Loss: 0.2038, Accuracy: 90.65%\n",
      "Epoch [117/200] \n",
      "Training Loss: 0.2265\n",
      "Validation Loss: 0.2151, Accuracy: 90.34%\n",
      "Epoch [118/200] \n",
      "Training Loss: 0.2240\n",
      "Validation Loss: 0.2378, Accuracy: 90.97%\n",
      "Epoch [119/200] \n",
      "Training Loss: 0.2500\n",
      "Validation Loss: 0.2050, Accuracy: 90.97%\n",
      "Epoch [120/200] \n",
      "Training Loss: 0.2053\n",
      "Validation Loss: 0.2174, Accuracy: 90.03%\n",
      "Epoch [121/200] \n",
      "Training Loss: 0.2466\n",
      "Validation Loss: 0.2309, Accuracy: 90.34%\n",
      "Epoch [122/200] \n",
      "Training Loss: 0.2516\n",
      "Validation Loss: 0.2073, Accuracy: 91.28%\n",
      "Epoch [123/200] \n",
      "Training Loss: 0.2512\n",
      "Validation Loss: 0.2000, Accuracy: 90.03%\n",
      "Epoch [124/200] \n",
      "Training Loss: 0.2286\n",
      "Validation Loss: 0.2030, Accuracy: 89.72%\n",
      "Epoch [125/200] \n",
      "Training Loss: 0.2089\n",
      "Validation Loss: 0.2316, Accuracy: 89.72%\n",
      "Epoch [126/200] \n",
      "Training Loss: 0.2110\n",
      "Validation Loss: 0.2126, Accuracy: 90.03%\n",
      "Epoch [127/200] \n",
      "Training Loss: 0.2455\n",
      "Validation Loss: 0.1965, Accuracy: 90.65%\n",
      "Epoch [128/200] \n",
      "Training Loss: 0.3150\n",
      "Validation Loss: 0.2333, Accuracy: 90.03%\n",
      "Epoch [129/200] \n",
      "Training Loss: 0.2737\n",
      "Validation Loss: 0.2334, Accuracy: 90.65%\n",
      "Epoch [130/200] \n",
      "Training Loss: 0.2320\n",
      "Validation Loss: 0.2269, Accuracy: 90.34%\n",
      "Epoch [131/200] \n",
      "Training Loss: 0.2608\n",
      "Validation Loss: 0.2608, Accuracy: 90.97%\n",
      "Epoch [132/200] \n",
      "Training Loss: 0.2357\n",
      "Validation Loss: 0.2452, Accuracy: 89.72%\n",
      "Epoch [133/200] \n",
      "Training Loss: 0.2408\n",
      "Validation Loss: 0.1993, Accuracy: 90.65%\n",
      "Epoch [134/200] \n",
      "Training Loss: 0.2014\n",
      "Validation Loss: 0.2337, Accuracy: 89.41%\n",
      "Epoch [135/200] \n",
      "Training Loss: 0.2775\n",
      "Validation Loss: 0.2000, Accuracy: 90.65%\n",
      "Epoch [136/200] \n",
      "Training Loss: 0.2362\n",
      "Validation Loss: 0.2099, Accuracy: 91.28%\n",
      "Epoch [137/200] \n",
      "Training Loss: 0.2450\n",
      "Validation Loss: 0.2062, Accuracy: 90.34%\n",
      "Epoch [138/200] \n",
      "Training Loss: 0.2202\n",
      "Validation Loss: 0.2130, Accuracy: 90.34%\n",
      "Epoch [139/200] \n",
      "Training Loss: 0.2203\n",
      "Validation Loss: 0.2000, Accuracy: 90.34%\n",
      "Epoch [140/200] \n",
      "Training Loss: 0.2014\n",
      "Validation Loss: 0.2007, Accuracy: 90.97%\n",
      "Epoch [141/200] \n",
      "Training Loss: 0.2862\n",
      "Validation Loss: 0.2095, Accuracy: 90.03%\n",
      "Epoch [142/200] \n",
      "Training Loss: 0.2360\n",
      "Validation Loss: 0.1933, Accuracy: 90.65%\n",
      "Epoch [143/200] \n",
      "Training Loss: 0.2151\n",
      "Validation Loss: 0.1929, Accuracy: 91.59%\n",
      "Epoch [144/200] \n",
      "Training Loss: 0.1959\n",
      "Validation Loss: 0.1895, Accuracy: 90.97%\n",
      "Epoch [145/200] \n",
      "Training Loss: 0.2265\n",
      "Validation Loss: 0.2039, Accuracy: 90.34%\n",
      "Epoch [146/200] \n",
      "Training Loss: 0.2161\n",
      "Validation Loss: 0.1947, Accuracy: 91.28%\n",
      "Epoch [147/200] \n",
      "Training Loss: 0.2015\n",
      "Validation Loss: 0.1882, Accuracy: 91.59%\n",
      "Epoch [148/200] \n",
      "Training Loss: 0.2131\n",
      "Validation Loss: 0.1882, Accuracy: 92.21%\n",
      "Epoch [149/200] \n",
      "Training Loss: 0.2279\n",
      "Validation Loss: 0.1864, Accuracy: 91.28%\n",
      "Epoch [150/200] \n",
      "Training Loss: 0.1992\n",
      "Validation Loss: 0.2222, Accuracy: 90.34%\n",
      "Epoch [151/200] \n",
      "Training Loss: 0.2023\n",
      "Validation Loss: 0.2106, Accuracy: 90.65%\n",
      "Epoch [152/200] \n",
      "Training Loss: 0.2213\n",
      "Validation Loss: 0.2082, Accuracy: 90.03%\n",
      "Epoch [153/200] \n",
      "Training Loss: 0.1950\n",
      "Validation Loss: 0.2116, Accuracy: 90.65%\n",
      "Epoch [154/200] \n",
      "Training Loss: 0.2353\n",
      "Validation Loss: 0.2364, Accuracy: 88.79%\n",
      "Epoch [155/200] \n",
      "Training Loss: 0.2657\n",
      "Validation Loss: 0.2059, Accuracy: 90.65%\n",
      "Epoch [156/200] \n",
      "Training Loss: 0.2349\n",
      "Validation Loss: 0.1902, Accuracy: 90.34%\n",
      "Epoch [157/200] \n",
      "Training Loss: 0.2071\n",
      "Validation Loss: 0.2117, Accuracy: 90.65%\n",
      "Epoch [158/200] \n",
      "Training Loss: 0.2181\n",
      "Validation Loss: 0.2204, Accuracy: 90.97%\n",
      "Epoch [159/200] \n",
      "Training Loss: 0.2420\n",
      "Validation Loss: 0.2231, Accuracy: 90.03%\n",
      "Epoch [160/200] \n",
      "Training Loss: 0.2639\n",
      "Validation Loss: 0.1978, Accuracy: 91.28%\n",
      "Epoch [161/200] \n",
      "Training Loss: 0.2389\n",
      "Validation Loss: 0.2021, Accuracy: 90.97%\n",
      "Epoch [162/200] \n",
      "Training Loss: 0.2081\n",
      "Validation Loss: 0.2103, Accuracy: 90.65%\n",
      "Epoch [163/200] \n",
      "Training Loss: 0.1931\n",
      "Validation Loss: 0.1874, Accuracy: 91.59%\n",
      "Epoch [164/200] \n",
      "Training Loss: 0.2193\n",
      "Validation Loss: 0.2066, Accuracy: 90.34%\n",
      "Epoch [165/200] \n",
      "Training Loss: 0.2075\n",
      "Validation Loss: 0.2099, Accuracy: 90.97%\n",
      "Epoch [166/200] \n",
      "Training Loss: 0.2155\n",
      "Validation Loss: 0.2572, Accuracy: 87.85%\n",
      "Epoch [167/200] \n",
      "Training Loss: 0.2135\n",
      "Validation Loss: 0.2006, Accuracy: 90.03%\n",
      "Epoch [168/200] \n",
      "Training Loss: 0.1987\n",
      "Validation Loss: 0.1969, Accuracy: 90.97%\n",
      "Epoch [169/200] \n",
      "Training Loss: 0.1952\n",
      "Validation Loss: 0.2269, Accuracy: 90.34%\n",
      "Epoch [170/200] \n",
      "Training Loss: 0.2223\n",
      "Validation Loss: 0.1897, Accuracy: 90.65%\n",
      "Epoch [171/200] \n",
      "Training Loss: 0.2029\n",
      "Validation Loss: 0.1943, Accuracy: 90.97%\n",
      "Epoch [172/200] \n",
      "Training Loss: 0.1948\n",
      "Validation Loss: 0.2055, Accuracy: 91.28%\n",
      "Epoch [173/200] \n",
      "Training Loss: 0.2072\n",
      "Validation Loss: 0.1940, Accuracy: 92.52%\n",
      "Epoch [174/200] \n",
      "Training Loss: 0.2004\n",
      "Validation Loss: 0.1967, Accuracy: 91.90%\n",
      "Epoch [175/200] \n",
      "Training Loss: 0.2021\n",
      "Validation Loss: 0.2061, Accuracy: 91.28%\n",
      "Epoch [176/200] \n",
      "Training Loss: 0.2116\n",
      "Validation Loss: 0.2162, Accuracy: 89.41%\n",
      "Epoch [177/200] \n",
      "Training Loss: 0.2084\n",
      "Validation Loss: 0.2267, Accuracy: 90.65%\n",
      "Epoch [178/200] \n",
      "Training Loss: 0.1967\n",
      "Validation Loss: 0.2380, Accuracy: 89.72%\n",
      "Epoch [179/200] \n",
      "Training Loss: 0.1842\n",
      "Validation Loss: 0.2303, Accuracy: 89.41%\n",
      "Epoch [180/200] \n",
      "Training Loss: 0.2105\n",
      "Validation Loss: 0.2161, Accuracy: 89.72%\n",
      "Epoch [181/200] \n",
      "Training Loss: 0.1995\n",
      "Validation Loss: 0.2075, Accuracy: 90.65%\n",
      "Epoch [182/200] \n",
      "Training Loss: 0.2001\n",
      "Validation Loss: 0.2175, Accuracy: 90.97%\n",
      "Epoch [183/200] \n",
      "Training Loss: 0.2095\n",
      "Validation Loss: 0.1928, Accuracy: 90.34%\n",
      "Epoch [184/200] \n",
      "Training Loss: 0.1827\n",
      "Validation Loss: 0.2208, Accuracy: 89.41%\n",
      "Epoch [185/200] \n",
      "Training Loss: 0.2179\n",
      "Validation Loss: 0.2067, Accuracy: 91.59%\n",
      "Epoch [186/200] \n",
      "Training Loss: 0.2198\n",
      "Validation Loss: 0.2197, Accuracy: 91.28%\n",
      "Epoch [187/200] \n",
      "Training Loss: 0.2058\n",
      "Validation Loss: 0.2653, Accuracy: 90.65%\n",
      "Epoch [188/200] \n",
      "Training Loss: 0.2292\n",
      "Validation Loss: 0.2031, Accuracy: 91.28%\n",
      "Epoch [189/200] \n",
      "Training Loss: 0.2195\n",
      "Validation Loss: 0.1976, Accuracy: 91.59%\n",
      "Epoch [190/200] \n",
      "Training Loss: 0.2218\n",
      "Validation Loss: 0.2195, Accuracy: 89.72%\n",
      "Epoch [191/200] \n",
      "Training Loss: 0.1980\n",
      "Validation Loss: 0.2136, Accuracy: 90.97%\n",
      "Epoch [192/200] \n",
      "Training Loss: 0.1810\n",
      "Validation Loss: 0.2237, Accuracy: 90.97%\n",
      "Epoch [193/200] \n",
      "Training Loss: 0.1692\n",
      "Validation Loss: 0.2072, Accuracy: 91.90%\n",
      "Epoch [194/200] \n",
      "Training Loss: 0.2198\n",
      "Validation Loss: 0.1995, Accuracy: 91.28%\n",
      "Epoch [195/200] \n",
      "Training Loss: 0.1999\n",
      "Validation Loss: 0.1997, Accuracy: 90.03%\n",
      "Epoch [196/200] \n",
      "Training Loss: 0.2081\n",
      "Validation Loss: 0.2116, Accuracy: 90.97%\n",
      "Epoch [197/200] \n",
      "Training Loss: 0.1953\n",
      "Validation Loss: 0.1998, Accuracy: 91.59%\n",
      "Epoch [198/200] \n",
      "Training Loss: 0.1983\n",
      "Validation Loss: 0.2111, Accuracy: 90.97%\n",
      "Epoch [199/200] \n",
      "Training Loss: 0.1868\n",
      "Validation Loss: 0.1993, Accuracy: 90.97%\n",
      "Epoch [200/200] \n",
      "Training Loss: 0.1757\n",
      "Validation Loss: 0.2191, Accuracy: 90.97%\n",
      "0.18120427429676056\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "best_prec1 = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = []\n",
    "    for i, (features, features_angle, labels) in enumerate(train_loader):\n",
    "        iceNet.train()\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features = Variable(features).float()\n",
    "        features_angle = Variable(features_angle).cuda()\n",
    "        labels = Variable(labels).long()\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        writer.add_graph(iceNet, outputs)\n",
    "#         writer.add_histogram('hist_fc1', iceNet.fc1[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc2', iceNet.fc2[0].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc3', iceNet.fc3.weight.data.cpu().numpy(), i)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    prec1 = accuracy(val_loader)[0]\n",
    "    print ('Epoch [%d/%d] \\nTraining Loss: %.4f' % (epoch+1, num_epochs, np.mean(epoch_train_loss).data[0]))\n",
    "    print('Validation Loss: %.4f, Accuracy: %.2f%%' % accuracy(val_loader))\n",
    "    \n",
    "    is_best = prec1 < best_prec1\n",
    "    best_prec1 = min(prec1, best_prec1)\n",
    "#     save_checkpoint({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'state_dict': iceNet.state_dict(),\n",
    "#         'best_prec1': best_prec1,\n",
    "#         'optimizer' : optimizer.state_dict(),\n",
    "#     }, is_best)\n",
    "\n",
    "print(best_prec1)\n",
    "# export scalar data to JSON for external processing\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "best_prec1 =  0.16314317286014557\n"
     ]
    }
   ],
   "source": [
    "print(\"=> loading checkpoint\")\n",
    "best_model = torch.load('./Models/model_best.pth.tar')\n",
    "print('best_prec1 = ', best_model['best_prec1'])\n",
    "iceNet.load_state_dict(best_model['state_dict'])\n",
    "optimizer.load_state_dict(best_model['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_dataset = icebergDataset(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "iceNet.eval()\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "results = []\n",
    "for features, features_angle in test_loader:\n",
    "    iceNet.eval()\n",
    "    features = Variable(features, volatile=True).cuda()\n",
    "    features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "    outputs = F.softmax(iceNet(features, features_angle))\n",
    "#     outputs = iceNet(features, features_angle)\n",
    "\n",
    "    results.append(outputs.data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45908028059236167"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train>0.5)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30282526115859448"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(results)>0.5)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['is_iceberg'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('./Submissions/sub_30Oct_val_1631.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_test_range (input_, train_min, train_max, min_, max_):\n",
    "    input_ += -(train_min)\n",
    "    input_ /= train_max / (max_ - min_)\n",
    "    input_ += min_\n",
    "    return input_\n",
    "\n",
    "def std_test_range (input_, mean, sd):\n",
    "    input_ -= mean\n",
    "    input_ /= sd\n",
    "    return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_KF = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_KF = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3_KF = scale_range(band_1_KF/band_2_KF, -1, 1)\n",
    "rgb = np.stack((band_1_KF, band_2_KF), axis=1)\n",
    "X_KF = [rgb, np.array(data['inc_angle']).reshape((len(data), 1))]\n",
    "\n",
    "y_KF = data['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        labels = Variable(labels, volatile=True).cuda()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "num_hyper_search_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "aug_probs = [[0.1,0.2,0.3], [0.3,0.4,0.5,0.6], [0.2,0.3,0.4,0.5,0.6], [0.1,0.2,0.3,0.4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-478b204b7ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hyper_search_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_angle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader_KF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0miceNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-fa2766131871>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomErodeDilate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_ed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomZoomOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_zo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomNoisy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_noisy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomShift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/Projects/Kaggle/Statoil_iceberg_classifier/utils.py\u001b[0m in \u001b[0;36mrandomNoisy\u001b[0;34m(im, u)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnoise_typ\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"speckle\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mgauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mgauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnoisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "kfold_scores = []\n",
    "\n",
    "test_dataset = icebergDataset(X_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "results = []\n",
    "sss = KFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X_KF[0], X_KF[1], y_KF)):\n",
    "    print('Fold [%d/%d]' % (i+1, kfold))\n",
    "    \n",
    "    X_train_KF, X_valid_KF = [X_KF[0][train_index], X_KF[1][train_index]], [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "    y_train_KF, y_valid_KF = y_KF[train_index], y_KF[test_index]\n",
    "    \n",
    "    val_dataset_KF = icebergDataset(X_valid_KF, y_valid_KF)\n",
    "    val_loader_KF = torch.utils.data.DataLoader(dataset=val_dataset_KF, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # ------------------------- Hyperparameter search for image augmentation -------------------------- #\n",
    "    best_prec_overall = 1\n",
    "    for params in list(itertools.product(aug_probs[0], aug_probs[1], aug_probs[2], aug_probs[3])):\n",
    "        # Define model\n",
    "        iceNet = net().apply(weight_init).cuda()\n",
    "\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "        \n",
    "        # Data Loader\n",
    "        train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, True, \n",
    "                                          params[0], params[1], params[2], params[3])\n",
    "        train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Train\n",
    "        best_prec1 = 1\n",
    "        for epoch in range(num_hyper_search_epochs):\n",
    "            epoch_train_loss = []\n",
    "            for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "                iceNet.train()\n",
    "                features = Variable(features).cuda()\n",
    "                features_angle = Variable(features_angle).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = iceNet(features, features_angle)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            prec1 = accuracy(val_loader_KF)[0]\n",
    "\n",
    "            # Save best model\n",
    "            is_best = prec1 < best_prec1\n",
    "            best_prec1 = min(prec1, best_prec1)\n",
    "            \n",
    "        # Save best model\n",
    "        is_params_best = best_prec1 < best_prec_overall\n",
    "        best_prec_overall = min(best_prec1, best_prec_overall)\n",
    "        if is_params_best:\n",
    "            best_params = params\n",
    "    \n",
    "    print('Hyperparameter search complete')\n",
    "    print('Selected hyperparameters : ', str(best_params))\n",
    "    # ------------------------ Complete training using selected hyperparameters ------------------------- #\n",
    "    \n",
    "    # Data Loader\n",
    "    train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, True, \n",
    "                                      best_params[0], best_params[1], best_params[2], best_params[3])\n",
    "    train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Define model\n",
    "    iceNet = net().apply(weight_init).cuda()\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    \n",
    "    # Train\n",
    "    best_prec1 = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = []\n",
    "        for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "            iceNet.train()\n",
    "            features = Variable(features).cuda()\n",
    "            features_angle = Variable(features_angle).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = iceNet(features, features_angle)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        prec1 = accuracy(val_loader_KF)[0]\n",
    "        \n",
    "        # Save best model\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': iceNet.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, filename='./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "        \n",
    "    print('Val Score : %f' % (best_prec1))\n",
    "    kfold_scores.append(best_prec1)\n",
    "    # Load best model\n",
    "    best_model = torch.load('./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "    iceNet.load_state_dict(best_model['state_dict'])\n",
    "    optimizer.load_state_dict(best_model['optimizer'])\n",
    "    \n",
    "    # Predict\n",
    "    iceNet.eval()\n",
    "    \n",
    "    results_fold = []\n",
    "    for features, features_angle in test_loader:\n",
    "        iceNet.eval()\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        outputs = F.softmax(iceNet(features, features_angle))\n",
    "    #     outputs = iceNet(features, features_angle)\n",
    "\n",
    "        results_fold.append(outputs.data[0][1])\n",
    "    \n",
    "    results.append(results_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14790893048048021"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(kfold_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8424"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sub['is_iceberg']<0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')\n",
    "sub['is_iceberg'] = np.array(results).mean(axis=0)\n",
    "sub.to_csv('./Submissions/Sub 12 - 5-fold _ Val-1395.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('./Submissions/Sub 3 - 5 fold _ Val - 0.1504.csv')['is_iceberg']\n",
    "# sub2 = pd.read_csv('./Submissions/Sub 4 - 10-fold _ Val-1269.csv')['is_iceberg']\n",
    "sub3 = pd.read_csv('./Submissions/Sub 5 - 5-fold _ Val-1538.csv')['is_iceberg']\n",
    "sub4 = pd.read_csv('./Submissions/Sub 6 - 5-fold _ Val-1480.csv')['is_iceberg']\n",
    "sub5 = pd.read_csv('./Submissions/Sub 8 - 5-fold _ Val-1479.csv')['is_iceberg']\n",
    "# sub6 = pd.read_csv('./Submissions/Sub 9 - 5-fold _ Val-1480.csv')['is_iceberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = np.mean((np.array(sub1), np.array(sub3),\n",
    "                   np.array(sub4), np.array(sub5)), axis=0)\n",
    "sub['is_iceberg'] = np.array(results)\n",
    "sub.to_csv('./Submissions/Sub 13 - Ensemble_3_5_6_8.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
