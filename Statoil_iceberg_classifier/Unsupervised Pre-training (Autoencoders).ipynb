{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.set_start_method('spawn') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "seed = 0\n",
    "# random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# from utils_3 import *\n",
    "# from utils import *\n",
    "# from utils_diff_ch import *\n",
    "from utils_diff_ch_scaled import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_range (input_, max_, min_):\n",
    "    input_[:,0,:,:] += -(-45.594448)\n",
    "    input_[:,0,:,:] /= 82.68497099999999 / (max_ - min_)\n",
    "    input_[:,0,:,:] += min_\n",
    "    input_[:,1,:,:] += -(-49.083500000000001)\n",
    "    input_[:,1,:,:] /= 69.686413000000002 / (max_ - min_)\n",
    "    input_[:,1,:,:] += min_\n",
    "    return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('./Data/train.json')\n",
    "test = pd.read_json('./Data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "test['band_1'] = test['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "test['band_2'] = test['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "test['not_machine_generated'] = test['inc_angle'].apply(lambda x: len(str(x))) <= 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Impute missing incidence angles\n",
    "data_impute = data.dropna(axis=0, how='any')\n",
    "data_impute_b1 = np.array([im.reshape(75*75) for im in data_impute.loc[:,'band_1']])\n",
    "data_impute_b2 = np.array([im.reshape(75*75) for im in data_impute.loc[:,'band_2']])\n",
    "data_impute_X = np.hstack(np.array((data_impute_b1, data_impute_b2, data_impute[['is_iceberg']])))\n",
    "data_impute_y = np.array(data_impute[['inc_angle']])\n",
    "\n",
    "data_impute_prec = data[~data.index.isin(data_impute.index.values)]\n",
    "data_impute_b1_prec = np.array([im.reshape(75*75) for im in data_impute_prec.loc[:,'band_1']])\n",
    "data_impute_b2_prec = np.array([im.reshape(75*75) for im in data_impute_prec.loc[:,'band_2']])\n",
    "data_impute_X_prec = np.hstack(np.array((data_impute_b1_prec, data_impute_b2_prec, data_impute_prec[['is_iceberg']])))\n",
    "\n",
    "modelImpute = LinearRegression()\n",
    "modelImpute = modelImpute.fit(data_impute_X, data_impute_y)\n",
    "data_impute_prec['inc_angle'] = modelImpute.predict(data_impute_X_prec)\n",
    "\n",
    "data.loc[data_impute_prec.index] = data_impute_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for autoencoder\n",
    "orig_data = data\n",
    "data = data.drop(['is_iceberg'], axis=1) \\\n",
    "            .append(test[test['not_machine_generated']].drop(['not_machine_generated'], axis=1)).reset_index() \\\n",
    "            .drop(['index'], axis=1)\n",
    "        \n",
    "train, val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "band_1_tr = np.concatenate([im for im in train['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_tr = np.concatenate([im for im in train['band_2']]).reshape(-1, 75, 75)\n",
    "X_train = scale_range(np.stack((band_1_tr, band_2_tr), axis=1), -1, 1)\n",
    "\n",
    "band_1_val = np.concatenate([im for im in val['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_val = np.concatenate([im for im in val['band_2']]).reshape(-1, 75, 75)\n",
    "X_val = scale_range(np.stack((band_1_val, band_2_val), axis=1), -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "width = 75\n",
    "height = 75\n",
    "channels = 2\n",
    "padding = 0\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "class icebergDataset(data_utils.Dataset):\n",
    "    \"\"\"Iceberg-Ship dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, transform=None):\n",
    "        self.X_images = X\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.X_images[idx]\n",
    "        if self.transform:\n",
    "            im = randomErodeDilate(im, u=0.5)\n",
    "            im = randomZoomOut(im, u=0.5)\n",
    "            im = randomNoisy(im, u=0.5)\n",
    "            im = randomShift(im, u=0.5)\n",
    "            im = randomRotation(im, u=0.5)\n",
    "            \n",
    "        return torch.from_numpy(im).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='./Models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, './Models/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = icebergDataset(X_train, transform=True)\n",
    "val_dataset = icebergDataset(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act = nn.PReLU()\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.batch = nn.BatchNorm2d(channels)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels, 18, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            act,\n",
    "            nn.Conv2d(18, 18, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv2d(18, 36, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(36),\n",
    "            act,\n",
    "            nn.Conv2d(36, 54, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(54),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "        \n",
    "            nn.Conv2d(54, 72, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(72),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "        \n",
    "            nn.Conv2d(72, 144, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(144),\n",
    "            act,\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(144, 18, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            act,\n",
    "            nn.ConvTranspose2d(18, 128, kernel_size=4, stride=2, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            act,\n",
    "            nn.Dropout(0.2),\n",
    "        \n",
    "            nn.ConvTranspose2d(128, 54, kernel_size=3, stride=2, padding=padding),\n",
    "            nn.BatchNorm2d(54),\n",
    "            act,\n",
    "            nn.ConvTranspose2d(54, 36, kernel_size=3, stride=2, padding=padding),\n",
    "            nn.BatchNorm2d(36),\n",
    "            act,\n",
    "            nn.Dropout(0.2),\n",
    "        \n",
    "            nn.ConvTranspose2d(36, 18, kernel_size=2, stride=2, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            act,\n",
    "            nn.Dropout(0.2),\n",
    "        \n",
    "            nn.ConvTranspose2d(18, 8, kernel_size=2, stride=2, padding=padding),\n",
    "            nn.BatchNorm2d(8),\n",
    "            act,\n",
    "            nn.Conv2d(8, 2, kernel_size=2, padding=padding))        \n",
    "        \n",
    "        \n",
    "    def forward(self, x_im):\n",
    "#         x_im = self.batch(x_im)\n",
    "        out = self.encoder(x_im)\n",
    "        out = self.decoder(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder (\n",
       "  (batch): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (encoder): Sequential (\n",
       "    (0): Conv2d(2, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): PReLU (1)\n",
       "    (3): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): PReLU (1)\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0.2)\n",
       "    (8): Conv2d(18, 36, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): PReLU (1)\n",
       "    (11): Conv2d(36, 54, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (12): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (13): PReLU (1)\n",
       "    (14): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (15): Dropout (p = 0.2)\n",
       "    (16): Conv2d(54, 72, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (17): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (18): PReLU (1)\n",
       "    (19): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (20): Dropout (p = 0.2)\n",
       "    (21): Conv2d(72, 144, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (22): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (23): PReLU (1)\n",
       "    (24): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (25): Dropout (p = 0.2)\n",
       "  )\n",
       "  (decoder): Sequential (\n",
       "    (0): Conv2d(144, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): PReLU (1)\n",
       "    (3): ConvTranspose2d(18, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): PReLU (1)\n",
       "    (6): Dropout (p = 0.2)\n",
       "    (7): ConvTranspose2d(128, 54, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (8): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (9): PReLU (1)\n",
       "    (10): ConvTranspose2d(54, 36, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (11): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (12): PReLU (1)\n",
       "    (13): Dropout (p = 0.2)\n",
       "    (14): ConvTranspose2d(36, 18, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (15): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (16): PReLU (1)\n",
       "    (17): Dropout (p = 0.2)\n",
       "    (18): ConvTranspose2d(18, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (20): PReLU (1)\n",
       "    (21): Conv2d(8, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iceNet = autoencoder()\n",
    "iceNet.apply(weight_init).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 39\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    for features in loader:\n",
    "        features = features.cuda()\n",
    "        labels = features.cuda()\n",
    "        features = Variable(features, volatile=True)\n",
    "        labels = Variable(labels, volatile=True).float()\n",
    "        outputs = iceNet(features)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        \n",
    "    return np.mean(loss).data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/39] \n",
      "Training Loss: 0.1439\n",
      "Validation Loss: 0.0488\n",
      "Epoch [2/39] \n",
      "Training Loss: 0.0455\n",
      "Validation Loss: 0.0156\n",
      "Epoch [3/39] \n",
      "Training Loss: 0.0219\n",
      "Validation Loss: 0.0111\n",
      "Epoch [4/39] \n",
      "Training Loss: 0.0159\n",
      "Validation Loss: 0.0083\n",
      "Epoch [5/39] \n",
      "Training Loss: 0.0129\n",
      "Validation Loss: 0.0073\n",
      "Epoch [6/39] \n",
      "Training Loss: 0.0113\n",
      "Validation Loss: 0.0072\n",
      "Epoch [7/39] \n",
      "Training Loss: 0.0105\n",
      "Validation Loss: 0.0072\n",
      "Epoch [8/39] \n",
      "Training Loss: 0.0101\n",
      "Validation Loss: 0.0070\n",
      "Epoch [9/39] \n",
      "Training Loss: 0.0097\n",
      "Validation Loss: 0.0070\n",
      "Epoch [10/39] \n",
      "Training Loss: 0.0095\n",
      "Validation Loss: 0.0068\n",
      "Epoch [11/39] \n",
      "Training Loss: 0.0092\n",
      "Validation Loss: 0.0068\n",
      "Epoch [12/39] \n",
      "Training Loss: 0.0088\n",
      "Validation Loss: 0.0069\n",
      "Epoch [13/39] \n",
      "Training Loss: 0.0089\n",
      "Validation Loss: 0.0067\n",
      "Epoch [14/39] \n",
      "Training Loss: 0.0086\n",
      "Validation Loss: 0.0068\n",
      "Epoch [15/39] \n",
      "Training Loss: 0.0085\n",
      "Validation Loss: 0.0065\n",
      "Epoch [16/39] \n",
      "Training Loss: 0.0085\n",
      "Validation Loss: 0.0066\n",
      "Epoch [17/39] \n",
      "Training Loss: 0.0084\n",
      "Validation Loss: 0.0063\n",
      "Epoch [18/39] \n",
      "Training Loss: 0.0084\n",
      "Validation Loss: 0.0064\n",
      "Epoch [19/39] \n",
      "Training Loss: 0.0081\n",
      "Validation Loss: 0.0063\n",
      "Epoch [20/39] \n",
      "Training Loss: 0.0082\n",
      "Validation Loss: 0.0065\n",
      "Epoch [21/39] \n",
      "Training Loss: 0.0082\n",
      "Validation Loss: 0.0066\n",
      "Epoch [22/39] \n",
      "Training Loss: 0.0081\n",
      "Validation Loss: 0.0063\n",
      "Epoch [23/39] \n",
      "Training Loss: 0.0081\n",
      "Validation Loss: 0.0061\n",
      "Epoch [24/39] \n",
      "Training Loss: 0.0080\n",
      "Validation Loss: 0.0064\n",
      "Epoch [25/39] \n",
      "Training Loss: 0.0080\n",
      "Validation Loss: 0.0060\n",
      "Epoch [26/39] \n",
      "Training Loss: 0.0078\n",
      "Validation Loss: 0.0058\n",
      "Epoch [27/39] \n",
      "Training Loss: 0.0079\n",
      "Validation Loss: 0.0056\n",
      "Epoch [28/39] \n",
      "Training Loss: 0.0079\n",
      "Validation Loss: 0.0058\n",
      "Epoch [29/39] \n",
      "Training Loss: 0.0079\n",
      "Validation Loss: 0.0060\n",
      "Epoch [30/39] \n",
      "Training Loss: 0.0079\n",
      "Validation Loss: 0.0056\n",
      "Epoch [31/39] \n",
      "Training Loss: 0.0080\n",
      "Validation Loss: 0.0053\n",
      "Epoch [32/39] \n",
      "Training Loss: 0.0077\n",
      "Validation Loss: 0.0055\n",
      "Epoch [33/39] \n",
      "Training Loss: 0.0077\n",
      "Validation Loss: 0.0056\n",
      "Epoch [34/39] \n",
      "Training Loss: 0.0077\n",
      "Validation Loss: 0.0059\n",
      "Epoch [35/39] \n",
      "Training Loss: 0.0077\n",
      "Validation Loss: 0.0056\n",
      "Epoch [36/39] \n",
      "Training Loss: 0.0078\n",
      "Validation Loss: 0.0058\n",
      "Epoch [37/39] \n",
      "Training Loss: 0.0077\n",
      "Validation Loss: 0.0056\n",
      "Epoch [38/39] \n",
      "Training Loss: 0.0077\n",
      "Validation Loss: 0.0060\n",
      "Epoch [39/39] \n",
      "Training Loss: 0.0076\n",
      "Validation Loss: 0.0054\n",
      "0.00534067302942276\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "best_prec1 = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = []\n",
    "    for i, (features) in enumerate(train_loader):\n",
    "        iceNet.train()\n",
    "        features = features.cuda()\n",
    "        labels = features.cuda()\n",
    "        features = Variable(features).float()\n",
    "        labels = Variable(labels).float()\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = iceNet(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    prec1 = accuracy(val_loader)\n",
    "    print ('Epoch [%d/%d] \\nTraining Loss: %.4f' % (epoch+1, num_epochs, np.mean(epoch_train_loss).data[0]))\n",
    "    print('Validation Loss: %.4f' % accuracy(val_loader))\n",
    "    \n",
    "    is_best = prec1 < best_prec1\n",
    "    best_prec1 = min(prec1, best_prec1)\n",
    "\n",
    "print(best_prec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoderNet = copy.deepcopy(iceNet)\n",
    "autoencoderResetNet = copy.deepcopy(iceNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tempNet = iceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Supervised Learning Net with freezed convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADW1JREFUeJzt3V/InvV9x/H3xxgFrYtNgzWNCSqEgZOxxpCKLSNCVzQY\n0gMZyUH9N3hQ6mihHoQK9qgH20FhTjELq1Sh0w38F0akqJTpDhSTEKPRuabOYULWMHVRUXRZvjt4\nLreHx+dffvf13Pcdfb/g5rmu6/e7r9+X3xM+uf4mqSok6VSdMeoCJJ2eDA9JTQwPSU0MD0lNDA9J\nTQwPSU3OHOTLSZYDfw9cDLwJ/GlVvTtDvzeB94H/AU5U1fpBxpU0eoMeeWwHnqmqtcAz3fpsrq6q\nPzI4pM+HQcNjC/BAt/wA8N0B9yfpNJFBnjBN8l9VdX63HODdT9en9fs34DiTpy1/U1U759jnBDDR\nrV7RXNwXwBVXOD3z2bt376hLGHtVlZbvzRseSZ4GLpyh6U7ggalhkeTdqvryDPtYVVVHklwAPAX8\neVU9O29xic/Oz8FXC+a3ZMmSUZcw1k6ePNkcHvNeMK2qb8/WluR3SVZW1dEkK4Fjs+zjSPfzWJLH\ngA3AvOEhaXwNes1jF3Bjt3wj8MT0DknOTXLep8vAd4BXBhxX0ogNes3jK8A/AGuAf2fyVu07Sb4G\n/G1VbUpyKfBY95Uzgb+rqp8ucP8el8/B05b5edoyt0FOWwYKj8VmeMxtnH9348LwmNsg4eETppKa\nGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoY\nHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6Smhgekpr0Eh5JrknyepJD\nSbbP0J4kd3ftB5Ks62NcSaMzcHgkWQLcC1wLXAZsS3LZtG7XAmu7zwRw36DjShqtPo48NgCHquqN\nqvoEeBjYMq3PFuDBmvQ8cH6SlT2MLWlE+giPVcBbU9YPd9tOtY+k08iZoy5guiQTTJ7aSBpjfYTH\nEWD1lPWLum2n2geAqtoJ7ARIUj3UJ2kR9HHa8iKwNsklSc4CtgK7pvXZBdzQ3XW5EjheVUd7GFvS\niAx85FFVJ5LcDvwKWALcX1UHk9zate8AdgObgEPAh8DNg44rabRSNb5nBp62zG2cf3fjYsmSJaMu\nYaydPHmSqkrLd33CVFITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8ND\nUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NS\nE8NDUhPDQ1KTXsIjyTVJXk9yKMn2Gdo3JjmeZH/3uauPcSWNzpmD7iDJEuBe4E+Aw8CLSXZV1avT\nuj5XVdcNOp6k8dDHkccG4FBVvVFVnwAPA1t62K+kMTbwkQewCnhryvph4Bsz9LsqyQHgCHBHVR2c\naWdJJoAJgHPOOYfNmzf3UOLn09VXXz3qEsbetm3bRl3CWHvyySebv9tHeCzEPmBNVX2QZBPwOLB2\npo5VtRPYCbB8+fIaUn2STlEfpy1HgNVT1i/qtv2fqnqvqj7olncDS5Os6GFsSSPSR3i8CKxNckmS\ns4CtwK6pHZJcmCTd8oZu3Ld7GFvSiAx82lJVJ5LcDvwKWALcX1UHk9zate8ArgduS3IC+AjYWlWe\nkkinsV6ueXSnIrunbdsxZfke4J4+xpI0HnzCVFITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NS\nE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1ITw0NSE8NDUhPDQ1IT\nw0NSE8NDUhPDQ1ITw0NSE8NDUpNewiPJ/UmOJXlllvYkuTvJoSQHkqzrY1xJo9PXkccvgGvmaL8W\nWNt9JoD7ehpX0oj0Eh5V9SzwzhxdtgAP1qTngfOTrOxjbEmjMaxrHquAt6asH+62fUaSiSR7kuz5\n+OOPh1KcpFM3dhdMq2pnVa2vqvVnn332qMuRNIthhccRYPWU9Yu6bZJOU8MKj13ADd1dlyuB41V1\ndEhjS1oEZ/axkyQPARuBFUkOAz8BlgJU1Q5gN7AJOAR8CNzcx7iSRqeX8KiqbfO0F/D9PsaSNB7G\n7oKppNOD4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGp\nieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqUkv4ZHk\n/iTHkrwyS/vGJMeT7O8+d/UxrqTR6eU/ugZ+AdwDPDhHn+eq6rqexpM0Yr0ceVTVs8A7fexL0umh\nryOPhbgqyQHgCHBHVR2cqVOSCWAC4IILLuCmm24aXoWnmc2bN4+6hLF3yy23jLqEsXbGGe3HD8O6\nYLoPWFNVfwj8NfD4bB2ramdVra+q9cuWLRtSeZJO1VDCo6req6oPuuXdwNIkK4YxtqTFMZTwSHJh\nknTLG7px3x7G2JIWRy/XPJI8BGwEViQ5DPwEWApQVTuA64HbkpwAPgK2VlX1Mbak0eglPKpq2zzt\n9zB5K1fS54RPmEpqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpi\neEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4\nSGoycHgkWZ3k10leTXIwyQ9m6JMkdyc5lORAknWDjitptPr4j65PAD+qqn1JzgP2Jnmqql6d0uda\nYG33+QZwX/dT0mlq4COPqjpaVfu65feB14BV07ptAR6sSc8D5ydZOejYkkan12seSS4Gvg68MK1p\nFfDWlPXDfDZgJJ1GeguPJF8CHgF+WFXvDbCfiSR7kuw5fvx4X+VJ6lkv4ZFkKZPB8cuqenSGLkeA\n1VPWL+q2fUZV7ayq9VW1ftmyZX2UJ2kR9HG3JcDPgdeq6mezdNsF3NDddbkSOF5VRwcdW9Lo9HG3\n5ZvA94CXk+zvtv0YWANQVTuA3cAm4BDwIXBzD+NKGqGBw6Oq/hnIPH0K+P6gY0kaHz5hKqmJ4SGp\nieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ\n4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIanJwOGRZHWSXyd5\nNcnBJD+Yoc/GJMeT7O8+dw06rqTROrOHfZwAflRV+5KcB+xN8lRVvTqt33NVdV0P40kaAwMfeVTV\n0ara1y2/D7wGrBp0v5LGW6qqv50lFwPPApdX1XtTtm8EHgUOA0eAO6rq4Cz7mAAmutXLgVd6K3Bw\nK4D/HHURU1jP/MatpnGr5/er6ryWL/YWHkm+BPwT8NOqenRa2+8BJ6vqgySbgL+qqrUL2Oeeqlrf\nS4E9sJ65jVs9MH41fZ7q6eVuS5KlwCPAL6cHB0BVvVdVH3TLu4GlSVb0Mbak0ejjbkuAnwOvVdXP\nZulzYdePJBu6cd8edGxJo9PH3ZZvAt8DXk6yv9v2Y2ANQFXtAK4HbktyAvgI2FoLO1/a2UN9fbKe\nuY1bPTB+NX1u6un1gqmkLw6fMJXUxPCQ1GRswiPJ8iRPJflN9/PLs/R7M8nL3WPuexahjmuSvJ7k\nUJLtM7Qnyd1d+4Ek6/quoaGmoT3+n+T+JMeSzPj8zYjmZ76ahvp6xAJf2RjaPC3aKyRVNRYf4C+B\n7d3yduAvZun3JrBikWpYAvwWuBQ4C3gJuGxan03Ak0CAK4EXFnleFlLTRuAfh/R7+mNgHfDKLO1D\nnZ8F1jS0+enGWwms65bPA/51lH+OFljPKc/R2Bx5AFuAB7rlB4DvjqCGDcChqnqjqj4BHu7qmmoL\n8GBNeh44P8nKEdc0NFX1LPDOHF2GPT8LqWmoamGvbAxtnhZYzykbp/D4alUd7Zb/A/jqLP0KeDrJ\n3u5R9j6tAt6asn6Yz07yQvoMuyaAq7rD3yeT/MEi1jOfYc/PQo1kfrpXNr4OvDCtaSTzNEc9cIpz\n1MdzHguW5Gngwhma7py6UlWVZLZ7yN+qqiNJLgCeSvIv3d88X2T7gDX1/4//Pw7M+/j/F8hI5qd7\nZeMR4Ic15V2vUZmnnlOeo6EeeVTVt6vq8hk+TwC/+/Swrft5bJZ9HOl+HgMeY/Kwvi9HgNVT1i/q\ntp1qnz7NO16N1+P/w56feY1ifuZ7ZYMhz9NivEIyTqctu4Abu+UbgSemd0hybib/zRCSnAt8h37f\nun0RWJvkkiRnAVu7uqbXeUN3tfxK4PiU063FMG9NY/b4/7DnZ17Dnp9urDlf2WCI87SQeprmaBhX\nnxd4RfgrwDPAb4CngeXd9q8Bu7vlS5m82/AScBC4cxHq2MTk1ejffrp/4Fbg1m45wL1d+8vA+iHM\nzXw13d7Nx0vA88BVi1jLQ8BR4L+ZPE//szGYn/lqGtr8dON9i8lrcweA/d1n06jmaYH1nPIc+Xi6\npCbjdNoi6TRieEhqYnhIamJ4SGpieEhqYnhIamJ4SGryv2H2CrOHryPXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09f82b8320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(iceNet.encoder[3].weight.data.cpu().numpy()[0][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto_data = data\n",
    "data = orig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "band_1_tr = np.concatenate([im for im in train['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_tr = np.concatenate([im for im in train['band_2']]).reshape(-1, 75, 75)\n",
    "X_train = scale_range(np.stack((band_1_tr, band_2_tr), axis=1), -1, 1)\n",
    "X_train = [X_train, np.array(train['inc_angle']).reshape((len(train), 1))]\n",
    "\n",
    "band_1_val = np.concatenate([im for im in val['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_val = np.concatenate([im for im in val['band_2']]).reshape(-1, 75, 75)\n",
    "X_val = scale_range(np.stack((band_1_val, band_2_val), axis=1), -1, 1)\n",
    "X_val = [X_val, np.array(val['inc_angle']).reshape((len(val), 1))]\n",
    "\n",
    "band_1_test = np.concatenate([im for im in test['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_test = np.concatenate([im for im in test['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3_test = scale_range(band_1_test/band_2_test, -1, 1)\n",
    "rgb = scale_range(np.stack((band_1_test, band_2_test), axis=1), -1, 1)\n",
    "X_test = [rgb, np.array(test['inc_angle']).reshape((len(test), 1))]\n",
    "\n",
    "y_train = train['is_iceberg'].values.astype(np.float32)\n",
    "y_val = val['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "class icebergDataset(data_utils.Dataset):\n",
    "    \"\"\"Iceberg-Ship dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y=None, transform=None, eval_=None):\n",
    "        self.X_images = X[0]\n",
    "        self.X_angles = torch.from_numpy(X[1]).float()\n",
    "        if y!=None:\n",
    "            self.y = torch.from_numpy(y.reshape((len(y),1))).float()\n",
    "        else:\n",
    "            self.y=None\n",
    "        self.transform = transform\n",
    "        self.eval_ = eval_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.X_images[idx]\n",
    "        if self.transform:\n",
    "#             if np.random.random() < 0.2:\n",
    "#                 im = cv2.blur(im, (2,2))\n",
    "            im = randomErodeDilate(im, u=0.3)\n",
    "            im = randomZoomOut(im, u=0.5)\n",
    "            im = randomNoisy(im, u=0.3)\n",
    "            im = randomShift(im, u=0.5)\n",
    "            im = randomRotation(im, u=0.3)\n",
    "        if self.eval_:\n",
    "            im = randomErodeDilate(im, u=0.2)\n",
    "            im = randomZoomOut(im, u=0.4)\n",
    "            im = randomNoisy(im, u=0.2)\n",
    "            im = randomShift(im, u=0.4)\n",
    "            im = randomRotation(im, u=0.2)\n",
    "        try:\n",
    "            if self.y==None:\n",
    "                return [torch.from_numpy(im).float(), self.X_angles[idx]]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return [torch.from_numpy(im).float(), self.X_angles[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = icebergDataset(X_train, y_train, transform=True)\n",
    "val_dataset = icebergDataset(X_val, y_val)\n",
    "test_dataset = icebergDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act = nn.PReLU()\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.batch = nn.BatchNorm2d(channels)\n",
    "        self.batch1D = nn.BatchNorm1d(1)\n",
    "        self.features = nn.Sequential(\n",
    "                    # stop at bottleneck\n",
    "                    *list(autoencoderNet.children())[:-1]\n",
    "                )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1+(3*3*144), 512),\n",
    "            act,\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 196),\n",
    "            act,\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc3 = nn.Linear(196, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x_im, x_angle):\n",
    "        x_angle = self.batch1D(x_angle)\n",
    "#         x_im = self.batch(x_im)\n",
    "        out = self.features(x_im)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = torch.cat([out, x_angle], dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (\n",
       "  (batch): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (batch1D): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (features): Sequential (\n",
       "    (0): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (1): Sequential (\n",
       "      (0): Conv2d(2, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (2): PReLU (1)\n",
       "      (3): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (4): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): PReLU (1)\n",
       "      (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "      (7): Dropout (p = 0.2)\n",
       "      (8): Conv2d(18, 36, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (9): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (10): PReLU (1)\n",
       "      (11): Conv2d(36, 54, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (12): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (13): PReLU (1)\n",
       "      (14): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "      (15): Dropout (p = 0.2)\n",
       "      (16): Conv2d(54, 72, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (17): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (18): PReLU (1)\n",
       "      (19): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "      (20): Dropout (p = 0.2)\n",
       "      (21): Conv2d(72, 144, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (22): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (23): PReLU (1)\n",
       "      (24): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "      (25): Dropout (p = 0.2)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Sequential (\n",
       "    (0): Linear (1297 -> 512)\n",
       "    (1): PReLU (1)\n",
       "    (2): Dropout (p = 0.5)\n",
       "  )\n",
       "  (fc2): Sequential (\n",
       "    (0): Linear (512 -> 196)\n",
       "    (1): PReLU (1)\n",
       "    (2): Dropout (p = 0.5)\n",
       "  )\n",
       "  (fc3): Linear (196 -> 1)\n",
       "  (sig): Sigmoid ()\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "autoencoderNet = copy.deepcopy(autoencoderResetNet)\n",
    "iceNet = net()\n",
    "iceNet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Freeze autoencoder feature layers\n",
    "# for param in iceNet.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# gradParams = list(iceNet.parameters())\n",
    "# for param in gradParams:\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features_angle = features_angle.cuda()\n",
    "        features = Variable(features, volatile=True)\n",
    "        features_angle = Variable(features_angle, volatile=True)\n",
    "        labels = Variable(labels, volatile=True).float()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        predicted = (F.sigmoid(outputs).data>0.5)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte().data).sum()\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] \n",
      "Training Loss: 0.6494\n",
      "Validation Loss: 0.7123, Accuracy: 52.34%\n",
      "Epoch [2/500] \n",
      "Training Loss: 0.5563\n",
      "Validation Loss: 0.5734, Accuracy: 65.42%\n",
      "Epoch [3/500] \n",
      "Training Loss: 0.5071\n",
      "Validation Loss: 0.7398, Accuracy: 72.90%\n",
      "Epoch [4/500] \n",
      "Training Loss: 0.5093\n",
      "Validation Loss: 0.4041, Accuracy: 78.19%\n",
      "Epoch [5/500] \n",
      "Training Loss: 0.4602\n",
      "Validation Loss: 0.3629, Accuracy: 78.82%\n",
      "Epoch [6/500] \n",
      "Training Loss: 0.4637\n",
      "Validation Loss: 0.4102, Accuracy: 75.08%\n",
      "Epoch [7/500] \n",
      "Training Loss: 0.4547\n",
      "Validation Loss: 0.3865, Accuracy: 78.82%\n",
      "Epoch [8/500] \n",
      "Training Loss: 0.4790\n",
      "Validation Loss: 0.4081, Accuracy: 76.64%\n",
      "Epoch [9/500] \n",
      "Training Loss: 0.5300\n",
      "Validation Loss: 0.6363, Accuracy: 79.13%\n",
      "Epoch [10/500] \n",
      "Training Loss: 0.4176\n",
      "Validation Loss: 0.4079, Accuracy: 76.64%\n",
      "Epoch [11/500] \n",
      "Training Loss: 0.4354\n",
      "Validation Loss: 0.3477, Accuracy: 81.00%\n",
      "Epoch [12/500] \n",
      "Training Loss: 0.4181\n",
      "Validation Loss: 0.3287, Accuracy: 85.36%\n",
      "Epoch [13/500] \n",
      "Training Loss: 0.3815\n",
      "Validation Loss: 0.3334, Accuracy: 82.87%\n",
      "Epoch [14/500] \n",
      "Training Loss: 0.3742\n",
      "Validation Loss: 0.3679, Accuracy: 80.69%\n",
      "Epoch [15/500] \n",
      "Training Loss: 0.3913\n",
      "Validation Loss: 0.9995, Accuracy: 72.59%\n",
      "Epoch [16/500] \n",
      "Training Loss: 0.3791\n",
      "Validation Loss: 0.3066, Accuracy: 82.55%\n",
      "Epoch [17/500] \n",
      "Training Loss: 0.3718\n",
      "Validation Loss: 0.3048, Accuracy: 83.18%\n",
      "Epoch [18/500] \n",
      "Training Loss: 0.4558\n",
      "Validation Loss: 0.3114, Accuracy: 83.80%\n",
      "Epoch [19/500] \n",
      "Training Loss: 0.3781\n",
      "Validation Loss: 0.3365, Accuracy: 82.87%\n",
      "Epoch [20/500] \n",
      "Training Loss: 0.3696\n",
      "Validation Loss: 0.3666, Accuracy: 82.24%\n",
      "Epoch [21/500] \n",
      "Training Loss: 0.3783\n",
      "Validation Loss: 0.3041, Accuracy: 81.93%\n",
      "Epoch [22/500] \n",
      "Training Loss: 0.3265\n",
      "Validation Loss: 0.2934, Accuracy: 85.05%\n",
      "Epoch [23/500] \n",
      "Training Loss: 0.3629\n",
      "Validation Loss: 0.3318, Accuracy: 82.55%\n",
      "Epoch [24/500] \n",
      "Training Loss: 0.3822\n",
      "Validation Loss: 0.5985, Accuracy: 80.37%\n",
      "Epoch [25/500] \n",
      "Training Loss: 0.3872\n",
      "Validation Loss: 0.3512, Accuracy: 81.00%\n",
      "Epoch [26/500] \n",
      "Training Loss: 0.4279\n",
      "Validation Loss: 0.2790, Accuracy: 83.80%\n",
      "Epoch [27/500] \n",
      "Training Loss: 0.3703\n",
      "Validation Loss: 0.3180, Accuracy: 84.74%\n",
      "Epoch [28/500] \n",
      "Training Loss: 0.4640\n",
      "Validation Loss: 0.6198, Accuracy: 74.45%\n",
      "Epoch [29/500] \n",
      "Training Loss: 0.4891\n",
      "Validation Loss: 0.2946, Accuracy: 82.87%\n",
      "Epoch [30/500] \n",
      "Training Loss: 0.4304\n",
      "Validation Loss: 0.3295, Accuracy: 82.87%\n",
      "Epoch [31/500] \n",
      "Training Loss: 0.3870\n",
      "Validation Loss: 0.3390, Accuracy: 81.62%\n",
      "Epoch [32/500] \n",
      "Training Loss: 0.3558\n",
      "Validation Loss: 0.2746, Accuracy: 85.36%\n",
      "Epoch [33/500] \n",
      "Training Loss: 0.3381\n",
      "Validation Loss: 0.2831, Accuracy: 86.29%\n",
      "Epoch [34/500] \n",
      "Training Loss: 0.3722\n",
      "Validation Loss: 0.2835, Accuracy: 84.74%\n",
      "Epoch [35/500] \n",
      "Training Loss: 0.3483\n",
      "Validation Loss: 0.3155, Accuracy: 83.18%\n",
      "Epoch [36/500] \n",
      "Training Loss: 0.3453\n",
      "Validation Loss: 0.4153, Accuracy: 82.55%\n",
      "Epoch [37/500] \n",
      "Training Loss: 0.3839\n",
      "Validation Loss: 0.2694, Accuracy: 84.74%\n",
      "Epoch [38/500] \n",
      "Training Loss: 0.3595\n",
      "Validation Loss: 0.3012, Accuracy: 82.55%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-6c0b1ff84f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "loss_plot = []\n",
    "best_prec1 = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = []\n",
    "    for i, (features, features_angle, labels) in enumerate(train_loader):\n",
    "        iceNet.train()\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features = Variable(features).float()\n",
    "        features_angle = Variable(features_angle).cuda()\n",
    "        labels = Variable(labels).float()\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        writer.add_graph(iceNet, outputs)\n",
    "#         writer.add_histogram('hist_fc1', iceNet.features_1[1].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc2', iceNet.features_1[3].weight.data.cpu().numpy(), i)\n",
    "#         writer.add_histogram('hist_fc3', iceNet.features_2.weight.data.cpu().numpy(), i)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    acc = accuracy(val_loader)\n",
    "    prec1 = acc[0]\n",
    "    tr_loss = np.mean(epoch_train_loss).data[0]\n",
    "    loss_plot.append([prec1, tr_loss])\n",
    "    print ('Epoch [%d/%d] \\nTraining Loss: %.4f' % (epoch+1, num_epochs, tr_loss))\n",
    "    print('Validation Loss: %.4f, Accuracy: %.2f%%' % acc)\n",
    "    \n",
    "    is_best = prec1 < best_prec1\n",
    "    best_prec1 = min(prec1, best_prec1)\n",
    "#     save_checkpoint({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'state_dict': iceNet.state_dict(),\n",
    "#         'best_prec1': best_prec1,\n",
    "#         'optimizer' : optimizer.state_dict(),\n",
    "#     }, is_best)\n",
    "\n",
    "print(best_prec1)\n",
    "# export scalar data to JSON for external processing\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5f8d40af9412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_prec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_plot' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "plt.plot(loss_plot)\n",
    "plt.legend(['Val', 'Train'])\n",
    "plt.show()\n",
    "print(best_prec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "best_prec1 =  0.19683799147605896\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'missing keys in state_dict: \"{\\'layer1.9.weight\\', \\'layer1.9.bias\\'}\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-227c481b680f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Models/model_best.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_prec1 = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_prec1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0miceNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mown_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'missing keys in state_dict: \"{}\"'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'missing keys in state_dict: \"{\\'layer1.9.weight\\', \\'layer1.9.bias\\'}\"'"
     ]
    }
   ],
   "source": [
    "print(\"=> loading checkpoint\")\n",
    "best_model = torch.load('./Models/model_best.pth.tar')\n",
    "print('best_prec1 = ', best_model['best_prec1'])\n",
    "iceNet.load_state_dict(best_model['state_dict'])\n",
    "optimizer.load_state_dict(best_model['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_dataset = icebergDataset(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "iceNet.eval()\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "results = []\n",
    "for features, features_angle in test_loader:\n",
    "    iceNet.eval()\n",
    "    features = Variable(features, volatile=True).cuda()\n",
    "    features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "    outputs = F.softmax(iceNet(features, features_angle))\n",
    "#     outputs = iceNet(features, features_angle)\n",
    "\n",
    "    results.append(outputs.data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['is_iceberg'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('./Submissions/sub_30Oct_val_1631.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import psuedo labels\n",
    "psuedo = pd.read_csv('./Submissions/Sub 31 - 5-fold _ Val-1753.csv')\n",
    "psuedo = psuedo[np.abs(psuedo['is_iceberg'])>0.95]\n",
    "psuedo['is_iceberg'] = np.array((psuedo['is_iceberg']>0.5), np.int)\n",
    "psuedo_test = test.join(psuedo.set_index('id'), how='right', on='id')\n",
    "\n",
    "data = data.append(psuedo_test[psuedo_test['not_machine_generated']].drop(['not_machine_generated']\n",
    "                                                                          , axis=1)).reset_index() \\\n",
    "        .drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_KF = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_KF = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3_KF = scale_range(band_1_KF/band_2_KF, -1, 1)\n",
    "X_KF = scale_range(np.stack((band_1_KF, band_2_KF), axis=1), -1, 1)\n",
    "X_KF = [X_KF, np.array(data['inc_angle']).reshape((len(data), 1))]\n",
    "\n",
    "# X_test[0] = scale_test_range(X_test[0], train_min, train_max, -1, 1)\n",
    "y_KF = data['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    predicted_val = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        labels = Variable(labels, volatile=True).cuda().float()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        predicted_val.append(outputs.data[:].cpu().numpy())\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        predicted = (F.sigmoid(outputs).data>0.5)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte().data).sum()\n",
    "    return np.mean(loss).data[0], (100 * correct / total), np.concatenate(predicted_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n",
      "Val Score : 0.149972\n",
      "Fold [2/5]\n",
      "Val Score : 0.158772\n",
      "Fold [3/5]\n",
      "Val Score : 0.209527\n",
      "Fold [4/5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-706d963da233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# kfold = 5\n",
    "# kfold_scores = []\n",
    "\n",
    "# test_dataset = icebergDataset(X_test)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# results = []\n",
    "# fold_predictions = []\n",
    "# sss = KFold(n_splits=kfold, random_state=0)\n",
    "# for i, (train_index, test_index) in enumerate(sss.split(X_KF[0], X_KF[1], y_KF)):\n",
    "#     X_train_KF = [X_KF[0][train_index], X_KF[1][train_index]]\n",
    "#     X_valid_KF = [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "#     y_train_KF, y_valid_KF = y_KF[train_index], y_KF[test_index]\n",
    "    \n",
    "#     # Define model\n",
    "#     autoencoderNet = copy.deepcopy(autoencoderResetNet)\n",
    "#     iceNet = net().cuda()\n",
    "    \n",
    "#     # Loss and Optimizer\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    \n",
    "#     # Data Loader\n",
    "#     train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, transform=True)\n",
    "#     val_dataset_KF = icebergDataset(X_valid_KF, y_valid_KF)\n",
    "\n",
    "#     train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader_KF = torch.utils.data.DataLoader(dataset=val_dataset_KF, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     print('Fold [%d/%d]' % (i+1, kfold))\n",
    "#     # Train\n",
    "#     best_prec1 = 1\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_train_loss = []\n",
    "#         for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "#             iceNet.train()\n",
    "#             features = Variable(features).cuda()\n",
    "#             features_angle = Variable(features_angle).cuda()\n",
    "#             labels = Variable(labels).cuda().float()\n",
    "#             # Forward + Backward + Optimize\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = iceNet(features, features_angle)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             epoch_train_loss.append(loss)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         acc = accuracy(val_loader_KF)\n",
    "#         prec1 = acc[0]\n",
    "\n",
    "#         # Save best model\n",
    "#         is_best = prec1 < best_prec1\n",
    "#         best_prec1 = min(prec1, best_prec1)\n",
    "#         if is_best:\n",
    "#             best_fold_predictions = acc[2]\n",
    "#         save_checkpoint({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'state_dict': iceNet.state_dict(),\n",
    "#             'best_prec1': best_prec1,\n",
    "#             'optimizer' : optimizer.state_dict(),\n",
    "#         }, is_best, filename='./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "        \n",
    "#     print('Val Score : %f' % (best_prec1))\n",
    "#     fold_predictions.append(best_fold_predictions)\n",
    "#     kfold_scores.append(best_prec1)\n",
    "#     # Load best model\n",
    "#     best_model = torch.load('./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "#     iceNet.load_state_dict(best_model['state_dict'])\n",
    "#     optimizer.load_state_dict(best_model['optimizer'])\n",
    "    \n",
    "#     # Predict\n",
    "#     iceNet.eval()\n",
    "    \n",
    "#     results_fold = []\n",
    "#     for features, features_angle in test_loader:\n",
    "#         iceNet.eval()\n",
    "#         features = Variable(features, volatile=True).cuda()\n",
    "#         features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "#         outputs = F.sigmoid(iceNet(features, features_angle))\n",
    "#     #     outputs = iceNet(features, features_angle)\n",
    "\n",
    "#         results_fold.append(outputs.data.cpu().numpy()[0][0])        \n",
    "    \n",
    "#     results.append(np.array(results_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n",
      "Val Score : 0.141197\n",
      "Fold [2/5]\n",
      "Val Score : 0.138402\n",
      "Fold [3/5]\n",
      "Val Score : 0.202200\n",
      "Fold [4/5]\n",
      "Val Score : 0.140164\n",
      "Fold [5/5]\n",
      "Val Score : 0.193309\n"
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "kfold_scores = []\n",
    "\n",
    "test_dataset = icebergDataset(X_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "results = []\n",
    "fold_predictions = []\n",
    "sss = KFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X_KF[0][:1604], X_KF[1][:1604], y_KF[:1604])):\n",
    "    sample_rate = 0.5\n",
    "    psuedo_samples = np.random.choice(np.arange(1604, len(data)), int((len(data)-1604)*sample_rate), replace=False)\n",
    "    X_train_KF = [np.vstack((X_KF[0][train_index],X_KF[0][psuedo_samples])), np.vstack((X_KF[1][train_index],X_KF[1][psuedo_samples]))]\n",
    "    X_valid_KF = [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "    y_train_KF, y_valid_KF = np.concatenate((y_KF[train_index],y_KF[psuedo_samples])), y_KF[test_index]\n",
    "    \n",
    "    # Define model\n",
    "    autoencoderNet = copy.deepcopy(autoencoderResetNet)\n",
    "    iceNet = net().cuda()\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    \n",
    "    # Data Loader\n",
    "    train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, transform=True)\n",
    "    val_dataset_KF = icebergDataset(X_valid_KF, y_valid_KF)\n",
    "\n",
    "    train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_KF = torch.utils.data.DataLoader(dataset=val_dataset_KF, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print('Fold [%d/%d]' % (i+1, kfold))\n",
    "    # Train\n",
    "    best_prec1 = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = []\n",
    "        for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "            iceNet.train()\n",
    "            features = Variable(features).cuda()\n",
    "            features_angle = Variable(features_angle).cuda()\n",
    "            labels = Variable(labels).cuda().float()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = iceNet(features, features_angle)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        acc = accuracy(val_loader_KF)\n",
    "        prec1 = acc[0]\n",
    "\n",
    "        # Save best model\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        if is_best:\n",
    "            best_fold_predictions = acc[2]\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': iceNet.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, filename='./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "        \n",
    "    print('Val Score : %f' % (best_prec1))\n",
    "    fold_predictions.append(best_fold_predictions)\n",
    "    kfold_scores.append(best_prec1)\n",
    "    # Load best model\n",
    "    best_model = torch.load('./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "    iceNet.load_state_dict(best_model['state_dict'])\n",
    "    optimizer.load_state_dict(best_model['optimizer'])\n",
    "    \n",
    "    # Predict\n",
    "    iceNet.eval()\n",
    "    \n",
    "    results_fold = []\n",
    "    for features, features_angle in test_loader:\n",
    "        iceNet.eval()\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        outputs = F.sigmoid(iceNet(features, features_angle))\n",
    "    #     outputs = iceNet(features, features_angle)\n",
    "\n",
    "        results_fold.append(outputs.data.cpu().numpy()[0][0])        \n",
    "    \n",
    "    results.append(np.array(results_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16305434703826904"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(kfold_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028485772863673157"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(kfold_scores).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6128"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sub['is_iceberg']<0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_num = 51\n",
    "os.makedirs(\"./Models/Sub \"+str(sub_num))\n",
    "np.savetxt(\"./Models/Sub \"+str(sub_num)+\"/results.csv\", np.array(results), delimiter=\",\")\n",
    "np.savetxt(\"./Models/Sub \"+str(sub_num)+\"/cv_fold_results.csv\", np.concatenate(fold_predictions), delimiter=\",\")\n",
    "sub = pd.read_csv('./Data/sample_submission.csv')\n",
    "sub['is_iceberg'] = np.array(results).mean(axis=0)\n",
    "sub.to_csv('./Submissions/Sub '+str(sub_num)+' - 5-fold _ Val-1525.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('./Submissions/Sub 24 - 5-fold _ Val-1770.csv')['is_iceberg']\n",
    "sub2 = pd.read_csv('./Submissions/Sub 25 - 5-fold _ Val-1757.csv')['is_iceberg']\n",
    "sub3 = pd.read_csv('./Submissions/Sub 26 - 5-fold _ Val-1802.csv')['is_iceberg']\n",
    "sub4 = pd.read_csv('./Submissions/Sub 27 - 5-fold _ Val-1839.csv')['is_iceberg']\n",
    "# sub5 = pd.read_csv('./Submissions/Sub 8 - 5-fold _ Val-1479.csv')['is_iceberg']\n",
    "# sub6 = pd.read_csv('./Submissions/Sub 16 - 5-fold _ Val-1422.csv')['is_iceberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = 1/np.mean((1/np.array(sub1), 1/np.array(sub3),\n",
    "                   1/np.array(sub4), 1/np.array(sub2)), axis=0)\n",
    "sub['is_iceberg'] = np.array(results)\n",
    "sub.to_csv('./Submissions/Sub 29 - Ensemble_24_25_26_27 - Harmonic Mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    inc_features = []\n",
    "    inc_feature_angles = []\n",
    "    inc_labels = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        labels = Variable(labels, volatile=True).cuda()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        inc_features.append(features.data.cpu().numpy()[np.where((predicted!=labels.data).cpu().numpy())])\n",
    "        inc_feature_angles.append(features_angle.data.cpu().numpy()[np.where((predicted!=labels.data).cpu().numpy())])\n",
    "        inc_labels.append(labels.data.cpu().numpy()[np.where((predicted!=labels.data).cpu().numpy())])\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total), [inc_features, inc_feature_angles, inc_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [3/5]\n",
      "0.2119673639535904\n"
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "kfold_scores = []\n",
    "\n",
    "results = []\n",
    "sss = KFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X_KF[0], X_KF[1], y_KF)):\n",
    "    if i == 2:\n",
    "        X_train_KF, X_valid_KF = [X_KF[0][train_index], X_KF[1][train_index]], [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "        y_train_KF, y_valid_KF = y_KF[train_index], y_KF[test_index]\n",
    "\n",
    "        # Define model\n",
    "        iceNet = net().apply(weight_init).cuda()\n",
    "\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "        # Data Loader\n",
    "        train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, transform=True)\n",
    "        val_dataset_KF = icebergDataset(X_valid_KF, y_valid_KF)\n",
    "\n",
    "        train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_KF = torch.utils.data.DataLoader(dataset=val_dataset_KF, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        print('Fold [%d/%d]' % (i+1, kfold))\n",
    "        # Train\n",
    "        best_prec1 = 1\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_train_loss = []\n",
    "            for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "                iceNet.train()\n",
    "                features = Variable(features).cuda()\n",
    "                features_angle = Variable(features_angle).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "                # Forward + Backward + Optimize\n",
    "                optimizer.zero_grad()\n",
    "                outputs = iceNet(features, features_angle)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_train_loss.append(loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            acc = accuracy(val_loader_KF)\n",
    "            prec1 = acc[0]\n",
    "\n",
    "            # Save best model\n",
    "            is_best = prec1 < best_prec1\n",
    "            best_prec1 = min(prec1, best_prec1)\n",
    "            if is_best:\n",
    "                incorrect = acc[2]\n",
    "print(best_prec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incorrect[0] = np.vstack(incorrect[0])\n",
    "incorrect[1] = np.vstack(incorrect[1])\n",
    "incorrect[2] = np.hstack(incorrect[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomZoomOut(im, zoom_ratio=(-0.5, 0.5), u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        height, width = im[0].shape\n",
    "        \n",
    "        zoom = height*np.random.uniform(zoom_ratio[0], zoom_ratio[1])\n",
    "        if int(zoom)<0:\n",
    "            zoom = abs(int(zoom))\n",
    "            im_0 = cv2.resize(im[0, zoom//2:-zoom//2, zoom//2:-zoom//2], (height, width))\n",
    "            im_1 = cv2.resize(im[1, zoom//2:-zoom//2, zoom//2:-zoom//2], (height, width))\n",
    "#            im_2 = cv2.resize(im[2, zoom//2:-zoom//2, zoom//2:-zoom//2], (height, width))\n",
    "#            im_3 = cv2.resize(im[3, zoom//2:-zoom//2, zoom//2:-zoom//2], (height, width))\n",
    "            return np.stack((im_0,im_1))\n",
    "        \n",
    "        if zoom<0:\n",
    "            zoom = 0\n",
    "        im_0 = cv2.resize(cv2.copyMakeBorder(im[0],int(zoom//2),int(zoom//2),int(zoom//2),int(zoom//2),\n",
    "                                   cv2.BORDER_REFLECT101), (height, width))\n",
    "        im_1 = cv2.resize(cv2.copyMakeBorder(im[1],int(zoom//2),int(zoom//2),int(zoom//2),int(zoom//2),\n",
    "                                   cv2.BORDER_WRAP), (height, width))\n",
    "#        im_2 = cv2.resize(cv2.copyMakeBorder(im[2],int(zoom//2),int(zoom//2),int(zoom//2),int(zoom//2),\n",
    "#                                   cv2.BORDER_REPLICATE), (height, width))\n",
    "#        im_3 = cv2.resize(cv2.copyMakeBorder(im[3],int(zoom//2),int(zoom//2),int(zoom//2),int(zoom//2),\n",
    "#                                   cv2.BORDER_REPLICATE), (height, width))\n",
    "        return np.stack((im_0,im_1))\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "/home/travis/miniconda/conda-bld/conda_1486587069159/work/opencv-3.1.0/modules/core/src/copy.cpp:928: error: (-5) Unknown/unsupported border type in function borderInterpolate\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-83225d65f6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label = '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m', Inc Angle = '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mzm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomZoomOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-3544e4fb7feb>\u001b[0m in \u001b[0;36mrandomZoomOut\u001b[0;34m(im, zoom_ratio, u)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mzoom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         im_0 = cv2.resize(cv2.copyMakeBorder(im[0],int(zoom//2),int(zoom//2),int(zoom//2),int(zoom//2),\n\u001b[0;32m---> 17\u001b[0;31m                                    cv2.BORDER_TRANSPARENT), (height, width))\n\u001b[0m\u001b[1;32m     18\u001b[0m         im_1 = cv2.resize(cv2.copyMakeBorder(im[1],int(zoom//2),int(zoom//2),int(zoom//2),int(zoom//2),\n\u001b[1;32m     19\u001b[0m                                    cv2.BORDER_WRAP), (height, width))\n",
      "\u001b[0;31merror\u001b[0m: /home/travis/miniconda/conda-bld/conda_1486587069159/work/opencv-3.1.0/modules/core/src/copy.cpp:928: error: (-5) Unknown/unsupported border type in function borderInterpolate\n"
     ]
    }
   ],
   "source": [
    "for idx, im in enumerate(incorrect[0]):\n",
    "    f, axarr = plt.subplots(2, 2, figsize=(10,8))\n",
    "    plt.title('Label = '+str(incorrect[2][idx])+', Inc Angle = '+str(incorrect[1][idx][0]))\n",
    "    zm = randomZoomOut(im, (0, 0.5), 1)\n",
    "    axarr[0, 0].imshow(im[0], cmap='gray')\n",
    "    axarr[0, 1].imshow(im[1], cmap='gray')\n",
    "    axarr[1, 0].imshow(zm[0], cmap='gray')\n",
    "    axarr[1, 1].imshow(zm[1], cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
