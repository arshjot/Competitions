{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/arsh/anaconda3/envs/deepLearn/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.set_start_method('spawn') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation\n",
    "from keras.layers import UpSampling2D, BatchNormalization, Flatten, Dense, GlobalMaxPooling2D\n",
    "from keras.layers.core import Dropout, Reshape\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('./Data/train.json')\n",
    "test = pd.read_json('./Data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "# mean_band1 = data['band_1'].mean().mean()\n",
    "# sd_band1 = np.array(data['band_1']).std()\n",
    "# data['band_1'] = (data['band_1'] - mean_band1)/sd_band1\n",
    "data['band_2'] = data['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "# mean_band2 = data['band_2'].mean().mean()\n",
    "# sd_band2 = np.array(data['band_2']).std()\n",
    "# data['band_2'] = (data['band_2'] - mean_band2)/sd_band2\n",
    "\n",
    "test['band_1'] = test['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "# test['band_1'] = (test['band_1'] - mean_band1)/sd_band1\n",
    "test['band_2'] = test['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "# test['band_2'] = (test['band_2'] - mean_band2)/sd_band2\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce').fillna(0.0)\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_train = np.array(data.index)\n",
    "ids_train_split, ids_valid_split = train_test_split(ids_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "width = 75\n",
    "height = 75\n",
    "channels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(ids_train_split), batch_size):\n",
    "            x_batch_0 = []\n",
    "            x_batch_1 = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(ids_train_split))\n",
    "            ids_train_batch = ids_train_split[start:end]\n",
    "            for idx in ids_train_batch:\n",
    "                \n",
    "                train = data.iloc[idx,]\n",
    "                band_1_tr = np.concatenate([im for im in train['band_1']]).reshape(-1, 75, 75)\n",
    "                band_2_tr = np.concatenate([im for im in train['band_2']]).reshape(-1, 75, 75)\n",
    "                X_train = np.concatenate((band_1_tr, band_2_tr), axis=0)\n",
    "                X_train = [X_train, np.array(train['inc_angle'])]\n",
    "\n",
    "                im = X_train[0]\n",
    "                im = randomErodeDilate(im, u=0.2)\n",
    "                im = randomZoomOut(im, u=0.5)\n",
    "                im = randomNoisy(im, u=0.4)\n",
    "                im = randomShift(im, u=0.3)\n",
    "                x_batch_0.append(im.reshape((75,75,2)))\n",
    "                x_batch_1.append(X_train[1].astype(np.float32))\n",
    "                y_batch.append(train['is_iceberg'].astype(np.float32))\n",
    "            \n",
    "            x_batch = [np.array(x_batch_0, np.float32), np.array(x_batch_1, np.float32)]\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def valid_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(ids_valid_split), batch_size):\n",
    "            x_batch_0 = []\n",
    "            x_batch_1 = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(ids_valid_split))\n",
    "            ids_valid_batch = ids_valid_split[start:end]\n",
    "            for idx in ids_valid_batch:\n",
    "                \n",
    "                val = data.iloc[idx,]\n",
    "                band_1_val = np.concatenate([im for im in val['band_1']]).reshape(-1, 75, 75)\n",
    "                band_2_val = np.concatenate([im for im in val['band_2']]).reshape(-1, 75, 75)\n",
    "                X_val = np.concatenate((band_1_val, band_2_val), axis=0)\n",
    "                X_val = [X_val, np.array(val['inc_angle'])]\n",
    "\n",
    "                im = X_val[0]\n",
    "                x_batch_0.append(im.reshape((75,75,2)))\n",
    "                x_batch_1.append(X_val[1].astype(np.float32))\n",
    "                y_batch.append(val['is_iceberg'].astype(np.float32))\n",
    "            \n",
    "            x_batch = [np.array(x_batch_0, np.float32), np.array(x_batch_1, np.float32)]\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_1 = Input(shape=(width, height, channels), name=\"X_1\")\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    \n",
    "    out = BatchNormalization()(input_1)\n",
    "    # Layers 1+2\n",
    "    out = Conv2D(9, (3, 3), padding='valid')(input_1)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Conv2D(18, (3, 3), padding='valid')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = MaxPooling2D((2, 2), strides=(2, 2))(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    \n",
    "    # Layers 2+3\n",
    "    out = Conv2D(24, (3, 3), padding='valid')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = Conv2D(36, (3, 3), padding='valid')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = MaxPooling2D((2, 2), strides=(2, 2))(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    \n",
    "    # Layer 4\n",
    "    out = Conv2D(72, (2, 2), padding='valid')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = MaxPooling2D((2, 2), strides=(2, 2))(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    \n",
    "    # Layer 5\n",
    "    out = Conv2D(144, (2, 2), padding='valid')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu')(out)\n",
    "    out = MaxPooling2D((2, 2), strides=(2, 2))(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    \n",
    "    out = Flatten()(out)\n",
    "    out_concat =  (Concatenate()([out, BatchNormalization()(input_2)]))\n",
    "    \n",
    "    # FCs 1+2+3\n",
    "    out = Dense(512, activation='relu')(out_concat)\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(196, activation='relu')(out)\n",
    "    out = Dropout(0.3)(out)\n",
    "    out = Dense(1, activation=\"sigmoid\")(out)\n",
    "\n",
    "    model = Model([input_1,input_2],  out)\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "41/41 [==============================] - 1s - loss: 0.9791 - acc: 0.5395 - val_loss: 0.6567 - val_acc: 0.5483\n",
      "Epoch 2/300\n",
      "41/41 [==============================] - 1s - loss: 0.7181 - acc: 0.5677 - val_loss: 0.6594 - val_acc: 0.4984\n",
      "Epoch 3/300\n",
      "41/41 [==============================] - 1s - loss: 0.6401 - acc: 0.6057 - val_loss: 0.6316 - val_acc: 0.7009\n",
      "Epoch 4/300\n",
      "41/41 [==============================] - 1s - loss: 0.6152 - acc: 0.6161 - val_loss: 0.6010 - val_acc: 0.7259\n",
      "Epoch 5/300\n",
      "41/41 [==============================] - 1s - loss: 0.5838 - acc: 0.6642 - val_loss: 0.5295 - val_acc: 0.7477\n",
      "Epoch 6/300\n",
      "41/41 [==============================] - 1s - loss: 0.5613 - acc: 0.6782 - val_loss: 0.5273 - val_acc: 0.7072\n",
      "Epoch 7/300\n",
      "41/41 [==============================] - 1s - loss: 0.5499 - acc: 0.6912 - val_loss: 0.5879 - val_acc: 0.6199\n",
      "Epoch 8/300\n",
      "41/41 [==============================] - 1s - loss: 0.5195 - acc: 0.7209 - val_loss: 0.4668 - val_acc: 0.7913\n",
      "Epoch 9/300\n",
      "41/41 [==============================] - 1s - loss: 0.4889 - acc: 0.7499 - val_loss: 0.4148 - val_acc: 0.8318\n",
      "Epoch 10/300\n",
      "41/41 [==============================] - 1s - loss: 0.4627 - acc: 0.7640 - val_loss: 0.4411 - val_acc: 0.8193\n",
      "Epoch 11/300\n",
      "41/41 [==============================] - 1s - loss: 0.4406 - acc: 0.7888 - val_loss: 0.3988 - val_acc: 0.8349\n",
      "Epoch 12/300\n",
      "41/41 [==============================] - 1s - loss: 0.4286 - acc: 0.8048 - val_loss: 0.3749 - val_acc: 0.8411\n",
      "Epoch 13/300\n",
      "41/41 [==============================] - 1s - loss: 0.3996 - acc: 0.8139 - val_loss: 0.3819 - val_acc: 0.8287\n",
      "Epoch 14/300\n",
      "41/41 [==============================] - 1s - loss: 0.3734 - acc: 0.8353 - val_loss: 0.3943 - val_acc: 0.8131\n",
      "Epoch 15/300\n",
      "41/41 [==============================] - 1s - loss: 0.3998 - acc: 0.8144 - val_loss: 0.3232 - val_acc: 0.8692\n",
      "Epoch 16/300\n",
      "41/41 [==============================] - 1s - loss: 0.4183 - acc: 0.8052 - val_loss: 0.3033 - val_acc: 0.8941\n",
      "Epoch 17/300\n",
      "41/41 [==============================] - 1s - loss: 0.4130 - acc: 0.8025 - val_loss: 0.3299 - val_acc: 0.8598\n",
      "Epoch 18/300\n",
      "41/41 [==============================] - 1s - loss: 0.3742 - acc: 0.8136 - val_loss: 0.3155 - val_acc: 0.8754\n",
      "Epoch 19/300\n",
      "41/41 [==============================] - 1s - loss: 0.3639 - acc: 0.8124 - val_loss: 0.3505 - val_acc: 0.8692\n",
      "Epoch 20/300\n",
      "41/41 [==============================] - 1s - loss: 0.3580 - acc: 0.8414 - val_loss: 0.3543 - val_acc: 0.8411\n",
      "Epoch 21/300\n",
      "41/41 [==============================] - 1s - loss: 0.3321 - acc: 0.8406 - val_loss: 0.4930 - val_acc: 0.7601\n",
      "Epoch 22/300\n",
      "41/41 [==============================] - 1s - loss: 0.3650 - acc: 0.8418 - val_loss: 0.2588 - val_acc: 0.9034\n",
      "Epoch 23/300\n",
      "41/41 [==============================] - 1s - loss: 0.3406 - acc: 0.8452 - val_loss: 0.2452 - val_acc: 0.9159\n",
      "Epoch 24/300\n",
      "41/41 [==============================] - 1s - loss: 0.3870 - acc: 0.8365 - val_loss: 0.2526 - val_acc: 0.8941\n",
      "Epoch 25/300\n",
      "41/41 [==============================] - 1s - loss: 0.3358 - acc: 0.8517 - val_loss: 0.2543 - val_acc: 0.8941\n",
      "Epoch 26/300\n",
      "41/41 [==============================] - 1s - loss: 0.3431 - acc: 0.8456 - val_loss: 0.2508 - val_acc: 0.8879\n",
      "Epoch 27/300\n",
      "41/41 [==============================] - 1s - loss: 0.3161 - acc: 0.8673 - val_loss: 0.3865 - val_acc: 0.8131\n",
      "Epoch 28/300\n",
      "41/41 [==============================] - 1s - loss: 0.2954 - acc: 0.8635 - val_loss: 0.3184 - val_acc: 0.8505\n",
      "Epoch 29/300\n",
      "41/41 [==============================] - 1s - loss: 0.2944 - acc: 0.8665 - val_loss: 0.3568 - val_acc: 0.8411\n",
      "Epoch 30/300\n",
      "41/41 [==============================] - 1s - loss: 0.3190 - acc: 0.8578 - val_loss: 0.3332 - val_acc: 0.8442\n",
      "Epoch 31/300\n",
      "41/41 [==============================] - 1s - loss: 0.3279 - acc: 0.8536 - val_loss: 0.4542 - val_acc: 0.7757\n",
      "Epoch 32/300\n",
      "41/41 [==============================] - 1s - loss: 0.3040 - acc: 0.8757 - val_loss: 0.3775 - val_acc: 0.8006\n",
      "Epoch 33/300\n",
      "41/41 [==============================] - 1s - loss: 0.3248 - acc: 0.8410 - val_loss: 0.2889 - val_acc: 0.8754\n",
      "Epoch 34/300\n",
      "41/41 [==============================] - 1s - loss: 0.2971 - acc: 0.8757 - val_loss: 0.2502 - val_acc: 0.9034\n",
      "Epoch 35/300\n",
      "41/41 [==============================] - 1s - loss: 0.2827 - acc: 0.8734 - val_loss: 0.4077 - val_acc: 0.8006\n",
      "Epoch 36/300\n",
      "41/41 [==============================] - 1s - loss: 0.3285 - acc: 0.8433 - val_loss: 0.2287 - val_acc: 0.9190\n",
      "Epoch 37/300\n",
      "41/41 [==============================] - 1s - loss: 0.3223 - acc: 0.8376 - val_loss: 0.2664 - val_acc: 0.8847\n",
      "Epoch 38/300\n",
      "41/41 [==============================] - 1s - loss: 0.3247 - acc: 0.8437 - val_loss: 0.3832 - val_acc: 0.8131\n",
      "Epoch 39/300\n",
      "41/41 [==============================] - 1s - loss: 0.3169 - acc: 0.8578 - val_loss: 0.2459 - val_acc: 0.9097\n",
      "Epoch 40/300\n",
      "41/41 [==============================] - 1s - loss: 0.3107 - acc: 0.8548 - val_loss: 0.2722 - val_acc: 0.8941\n",
      "Epoch 41/300\n",
      "41/41 [==============================] - 1s - loss: 0.3501 - acc: 0.8494 - val_loss: 0.3349 - val_acc: 0.8536\n",
      "Epoch 42/300\n",
      "41/41 [==============================] - 1s - loss: 0.2852 - acc: 0.8826 - val_loss: 0.2790 - val_acc: 0.8847\n",
      "Epoch 43/300\n",
      "41/41 [==============================] - 1s - loss: 0.2957 - acc: 0.8612 - val_loss: 0.4215 - val_acc: 0.7882\n",
      "Epoch 44/300\n",
      "41/41 [==============================] - 1s - loss: 0.2892 - acc: 0.8757 - val_loss: 0.2371 - val_acc: 0.9128\n",
      "Epoch 45/300\n",
      "41/41 [==============================] - 1s - loss: 0.2989 - acc: 0.8643 - val_loss: 0.4694 - val_acc: 0.7539\n",
      "Epoch 46/300\n",
      "41/41 [==============================] - 1s - loss: 0.3096 - acc: 0.8632 - val_loss: 0.2751 - val_acc: 0.8910\n",
      "Epoch 47/300\n",
      "41/41 [==============================] - 1s - loss: 0.2959 - acc: 0.8719 - val_loss: 0.2543 - val_acc: 0.8972\n",
      "Epoch 48/300\n",
      "41/41 [==============================] - 1s - loss: 0.2765 - acc: 0.8726 - val_loss: 0.2609 - val_acc: 0.9034\n",
      "Epoch 49/300\n",
      "41/41 [==============================] - 1s - loss: 0.3237 - acc: 0.8616 - val_loss: 0.2731 - val_acc: 0.8816\n",
      "Epoch 50/300\n",
      "41/41 [==============================] - 1s - loss: 0.3130 - acc: 0.8551 - val_loss: 0.2393 - val_acc: 0.9097\n",
      "Epoch 51/300\n",
      "41/41 [==============================] - 1s - loss: 0.3063 - acc: 0.8643 - val_loss: 0.2641 - val_acc: 0.8847\n",
      "Epoch 52/300\n",
      "41/41 [==============================] - 1s - loss: 0.2868 - acc: 0.8673 - val_loss: 0.2339 - val_acc: 0.9190\n",
      "Epoch 53/300\n",
      "41/41 [==============================] - 1s - loss: 0.3080 - acc: 0.8604 - val_loss: 0.2670 - val_acc: 0.9034\n",
      "Epoch 54/300\n",
      "41/41 [==============================] - 1s - loss: 0.2799 - acc: 0.8749 - val_loss: 0.3373 - val_acc: 0.8629\n",
      "Epoch 55/300\n",
      "41/41 [==============================] - 1s - loss: 0.2831 - acc: 0.8673 - val_loss: 0.2508 - val_acc: 0.9034\n",
      "Epoch 56/300\n",
      "41/41 [==============================] - 1s - loss: 0.2989 - acc: 0.8612 - val_loss: 0.2682 - val_acc: 0.8941\n",
      "Epoch 57/300\n",
      "41/41 [==============================] - 1s - loss: 0.2791 - acc: 0.8711 - val_loss: 0.2394 - val_acc: 0.9097\n",
      "Epoch 58/300\n",
      "41/41 [==============================] - 1s - loss: 0.2950 - acc: 0.8772 - val_loss: 0.2802 - val_acc: 0.9065\n",
      "Epoch 59/300\n",
      "41/41 [==============================] - 1s - loss: 0.2726 - acc: 0.8723 - val_loss: 0.2624 - val_acc: 0.9034\n",
      "Epoch 60/300\n",
      "41/41 [==============================] - 1s - loss: 0.2911 - acc: 0.8601 - val_loss: 0.2337 - val_acc: 0.9159\n",
      "Epoch 61/300\n",
      "41/41 [==============================] - 1s - loss: 0.2809 - acc: 0.8734 - val_loss: 0.2531 - val_acc: 0.8816\n",
      "Epoch 62/300\n",
      "41/41 [==============================] - 1s - loss: 0.2886 - acc: 0.8635 - val_loss: 0.3670 - val_acc: 0.8224\n",
      "Epoch 63/300\n",
      "41/41 [==============================] - 1s - loss: 0.2828 - acc: 0.8738 - val_loss: 0.5067 - val_acc: 0.7632\n",
      "Epoch 64/300\n",
      "41/41 [==============================] - 1s - loss: 0.2805 - acc: 0.8749 - val_loss: 0.3205 - val_acc: 0.8879\n",
      "Epoch 65/300\n",
      "41/41 [==============================] - 1s - loss: 0.2808 - acc: 0.8803 - val_loss: 0.3753 - val_acc: 0.8224\n",
      "Epoch 66/300\n",
      "41/41 [==============================] - 1s - loss: 0.2656 - acc: 0.8765 - val_loss: 0.3342 - val_acc: 0.8629\n",
      "Epoch 67/300\n",
      "41/41 [==============================] - 1s - loss: 0.2785 - acc: 0.8719 - val_loss: 0.2339 - val_acc: 0.9190\n",
      "Epoch 68/300\n",
      "41/41 [==============================] - 1s - loss: 0.2893 - acc: 0.8742 - val_loss: 0.2499 - val_acc: 0.9128\n",
      "Epoch 69/300\n",
      "41/41 [==============================] - 1s - loss: 0.2583 - acc: 0.8833 - val_loss: 0.3892 - val_acc: 0.7819\n",
      "Epoch 70/300\n",
      "41/41 [==============================] - 1s - loss: 0.2632 - acc: 0.8841 - val_loss: 0.3824 - val_acc: 0.8162\n",
      "Epoch 71/300\n",
      "41/41 [==============================] - 1s - loss: 0.2432 - acc: 0.8841 - val_loss: 0.3731 - val_acc: 0.8505\n",
      "Epoch 72/300\n",
      "41/41 [==============================] - 1s - loss: 0.2641 - acc: 0.8830 - val_loss: 0.2480 - val_acc: 0.9097\n",
      "Epoch 73/300\n",
      "41/41 [==============================] - 1s - loss: 0.2915 - acc: 0.8670 - val_loss: 0.2663 - val_acc: 0.8972\n",
      "Epoch 74/300\n",
      "41/41 [==============================] - 1s - loss: 0.2625 - acc: 0.8765 - val_loss: 0.2724 - val_acc: 0.9065\n",
      "Epoch 75/300\n",
      "41/41 [==============================] - 1s - loss: 0.2694 - acc: 0.8711 - val_loss: 0.3723 - val_acc: 0.8255\n",
      "Epoch 76/300\n",
      "41/41 [==============================] - 1s - loss: 0.2698 - acc: 0.8609 - val_loss: 0.3333 - val_acc: 0.8629\n",
      "Epoch 77/300\n",
      "41/41 [==============================] - 1s - loss: 0.2647 - acc: 0.8761 - val_loss: 0.2567 - val_acc: 0.9065\n",
      "Epoch 78/300\n",
      "41/41 [==============================] - 1s - loss: 0.3081 - acc: 0.8521 - val_loss: 0.3045 - val_acc: 0.9128\n",
      "Epoch 79/300\n",
      "41/41 [==============================] - 1s - loss: 0.2525 - acc: 0.8833 - val_loss: 0.2781 - val_acc: 0.9034\n",
      "Epoch 80/300\n",
      "41/41 [==============================] - 1s - loss: 0.2896 - acc: 0.8632 - val_loss: 0.2846 - val_acc: 0.8816\n",
      "Epoch 81/300\n",
      "41/41 [==============================] - 1s - loss: 0.2898 - acc: 0.8635 - val_loss: 0.2418 - val_acc: 0.9003\n",
      "Epoch 82/300\n",
      "41/41 [==============================] - 1s - loss: 0.2614 - acc: 0.8871 - val_loss: 0.2480 - val_acc: 0.9159\n",
      "Epoch 83/300\n",
      "41/41 [==============================] - 1s - loss: 0.2772 - acc: 0.8841 - val_loss: 0.2680 - val_acc: 0.8972\n",
      "Epoch 84/300\n",
      "41/41 [==============================] - 1s - loss: 0.2452 - acc: 0.8856 - val_loss: 0.2712 - val_acc: 0.9034\n",
      "Epoch 85/300\n",
      "41/41 [==============================] - 1s - loss: 0.2460 - acc: 0.8917 - val_loss: 0.2818 - val_acc: 0.8972\n",
      "Epoch 86/300\n",
      "41/41 [==============================] - 1s - loss: 0.2616 - acc: 0.8879 - val_loss: 0.2579 - val_acc: 0.9065\n",
      "Epoch 87/300\n",
      "41/41 [==============================] - 1s - loss: 0.2570 - acc: 0.8848 - val_loss: 0.2647 - val_acc: 0.9128\n",
      "Epoch 88/300\n",
      "41/41 [==============================] - 1s - loss: 0.2505 - acc: 0.8856 - val_loss: 0.2789 - val_acc: 0.8910\n",
      "Epoch 89/300\n",
      "41/41 [==============================] - 1s - loss: 0.2693 - acc: 0.8868 - val_loss: 0.2847 - val_acc: 0.8941\n",
      "Epoch 90/300\n",
      "41/41 [==============================] - 1s - loss: 0.2893 - acc: 0.8696 - val_loss: 0.2249 - val_acc: 0.8879\n",
      "Epoch 91/300\n",
      "41/41 [==============================] - 1s - loss: 0.2626 - acc: 0.8772 - val_loss: 0.4041 - val_acc: 0.8287\n",
      "Epoch 92/300\n",
      "22/41 [===============>..............] - ETA: 0s - loss: 0.2592 - acc: 0.8793"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=100,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=60,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='Keras_models/best_weights.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True),\n",
    "             TensorBoard(log_dir='keras_logs')]\n",
    "\n",
    "model.fit_generator(generator=train_generator(),\n",
    "                    steps_per_epoch=np.ceil(float(len(ids_train_split)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45908028059236167"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train>0.5)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30282526115859448"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(results)>0.5)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['is_iceberg'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('./Submissions/sub_30Oct_val_1631.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_KF = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_KF = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "rgb = np.stack((band_1_KF, band_2_KF), axis=1)\n",
    "X_KF = [rgb, np.array(data['inc_angle']).reshape((len(data), 1))]\n",
    "\n",
    "y_KF = data['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        labels = Variable(labels, volatile=True).cuda()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n",
      "Val Score : 0.144305\n",
      "Fold [2/5]\n",
      "Val Score : 0.150032\n",
      "Fold [3/5]\n",
      "Val Score : 0.203240\n",
      "Fold [4/5]\n",
      "Val Score : 0.137823\n",
      "Fold [5/5]\n",
      "Val Score : 0.113948\n"
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "kfold_scores = []\n",
    "\n",
    "test_dataset = icebergDataset(X_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "results = []\n",
    "sss = KFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X_KF[0], X_KF[1], y_KF)):\n",
    "    X_train_KF, X_valid_KF = [X_KF[0][train_index], X_KF[1][train_index]], [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "    y_train_KF, y_valid_KF = y_KF[train_index], y_KF[test_index]\n",
    "    \n",
    "    # Define model\n",
    "    iceNet = net().cuda()\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Data Loader\n",
    "    train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, transform=True)\n",
    "    val_dataset_KF = icebergDataset(X_valid_KF, y_valid_KF)\n",
    "\n",
    "    train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_KF = torch.utils.data.DataLoader(dataset=val_dataset_KF, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print('Fold [%d/%d]' % (i+1, kfold))\n",
    "    # Train\n",
    "    best_prec1 = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = []\n",
    "        for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "            iceNet.train()\n",
    "            features = Variable(features).cuda()\n",
    "            features_angle = Variable(features_angle).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = iceNet(features, features_angle)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        prec1 = accuracy(val_loader_KF)[0]\n",
    "        \n",
    "        # Save best model\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': iceNet.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, filename='./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "        \n",
    "    print('Val Score : %f' % (best_prec1))\n",
    "    kfold_scores.append(best_prec1)\n",
    "    # Load best model\n",
    "    best_model = torch.load('./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "    iceNet.load_state_dict(best_model['state_dict'])\n",
    "    optimizer.load_state_dict(best_model['optimizer'])\n",
    "    \n",
    "    # Predict\n",
    "    iceNet.eval()\n",
    "    \n",
    "    results_fold = []\n",
    "    for features, features_angle in test_loader:\n",
    "        iceNet.eval()\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        outputs = F.softmax(iceNet(features, features_angle))\n",
    "    #     outputs = iceNet(features, features_angle)\n",
    "\n",
    "        results_fold.append(outputs.data[0][1])\n",
    "    \n",
    "    results.append(results_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14986965507268907"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(kfold_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')\n",
    "sub['is_iceberg'] = np.array(results).mean(axis=0)\n",
    "sub.to_csv('./Submissions/Sub 4 - 10-fold _ Val-1269.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
