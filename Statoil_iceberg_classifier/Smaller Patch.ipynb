{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# mp.set_start_method('spawn') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data_utils\n",
    "from collections import OrderedDict\n",
    "\n",
    "seed = 0\n",
    "# random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "from utils_diff_ch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('./Data/train.json')\n",
    "test = pd.read_json('./Data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['band_1'] = data['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "data['band_2'] = data['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "test['band_1'] = test['band_1'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "test['band_2'] = test['band_2'].apply(lambda x : np.array(x).reshape(75, 75))\n",
    "\n",
    "data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n",
    "test['inc_angle'] = pd.to_numeric(test['inc_angle'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Impute missing incidence angles\n",
    "data_impute = data.dropna(axis=0, how='any')\n",
    "data_impute_b1 = np.array([im.reshape(75*75) for im in data_impute.loc[:,'band_1']])\n",
    "data_impute_b2 = np.array([im.reshape(75*75) for im in data_impute.loc[:,'band_2']])\n",
    "data_impute_X = np.hstack(np.array((data_impute_b1, data_impute_b2, data_impute[['is_iceberg']])))\n",
    "data_impute_y = np.array(data_impute[['inc_angle']])\n",
    "\n",
    "data_impute_prec = data[~data.index.isin(data_impute.index.values)]\n",
    "data_impute_b1_prec = np.array([im.reshape(75*75) for im in data_impute_prec.loc[:,'band_1']])\n",
    "data_impute_b2_prec = np.array([im.reshape(75*75) for im in data_impute_prec.loc[:,'band_2']])\n",
    "data_impute_X_prec = np.hstack(np.array((data_impute_b1_prec, data_impute_b2_prec, data_impute_prec[['is_iceberg']])))\n",
    "\n",
    "modelImpute = LinearRegression()\n",
    "modelImpute = modelImpute.fit(data_impute_X, data_impute_y)\n",
    "data_impute_prec['inc_angle'] = modelImpute.predict(data_impute_X_prec)\n",
    "\n",
    "data.loc[data_impute_prec.index] = data_impute_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "band_1_tr = np.concatenate([im for im in train['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_tr = np.concatenate([im for im in train['band_2']]).reshape(-1, 75, 75)\n",
    "X_train = np.stack((band_1_tr, band_2_tr), axis=1)\n",
    "X_train = [X_train, np.array(train['inc_angle']).reshape((len(train), 1))]\n",
    "\n",
    "band_1_val = np.concatenate([im for im in val['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_val = np.concatenate([im for im in val['band_2']]).reshape(-1, 75, 75)\n",
    "X_val = np.stack((band_1_val, band_2_val), axis=1)\n",
    "X_val = [X_val, np.array(val['inc_angle']).reshape((len(val), 1))]\n",
    "\n",
    "band_1_test = np.concatenate([im for im in test['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_test = np.concatenate([im for im in test['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3_test = scale_range(band_1_test/band_2_test, -1, 1)\n",
    "rgb = np.stack((band_1_test, band_2_test), axis=1)\n",
    "X_test = [rgb, np.array(test['inc_angle']).reshape((len(test), 1))]\n",
    "\n",
    "y_train = train['is_iceberg'].values.astype(np.float32)\n",
    "y_val = val['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "width = 55\n",
    "height = 55\n",
    "channels = 2\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "class icebergDataset(data_utils.Dataset):\n",
    "    \"\"\"Iceberg-Ship dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y=None, transform=None, eval_=None):\n",
    "        self.X_images = X[0]\n",
    "        self.X_angles = torch.from_numpy(X[1]).float()\n",
    "        if y!=None:\n",
    "            self.y = torch.from_numpy(y.reshape((len(y),1))).float()\n",
    "        else:\n",
    "            self.y=None\n",
    "        self.transform = transform\n",
    "        self.eval = eval_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.X_images[idx]\n",
    "        if self.transform:\n",
    "#             if np.random.random() < 0.2:\n",
    "#                 im = cv2.blur(im, (2,2))\n",
    "            im = randomErodeDilate(im, u=0.2)\n",
    "            im = randomZoomOut(im, u=0.4)\n",
    "            im = randomNoisy(im, u=0.2)\n",
    "            im = randomRotation(im, u=0.2)\n",
    "#             else:\n",
    "#                 im = randomDenoising(im, u=0.8)\n",
    "#             im = randomShift(im, u=0.3)\n",
    "        \n",
    "        # Get patch\n",
    "        if self.eval:\n",
    "            ims = []\n",
    "            for patch_num in range(20):                \n",
    "                im_sw = np.random.randint(im.shape[1]-1-55)\n",
    "                im_sh = np.random.randint(im.shape[2]-1-55)\n",
    "                ims.append(torch.from_numpy(im[:, im_sw:im_sw+55, im_sh:im_sh+55]).float())\n",
    "            features = torch.stack(ims)\n",
    "            features_angle = self.X_angles[idx]\n",
    "        else:\n",
    "            im_sw = np.random.randint(im.shape[1]-1-55)\n",
    "            im_sh = np.random.randint(im.shape[2]-1-55)\n",
    "            im = im[:, im_sw:im_sw+55, im_sh:im_sh+55]\n",
    "            features = torch.from_numpy(im).float()\n",
    "            features_angle = self.X_angles[idx]\n",
    "        \n",
    "        try:\n",
    "            if self.y==None:\n",
    "                return [features, features_angle]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return [features, features_angle, self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='./Models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, './Models/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = icebergDataset(X_train, y_train, transform=True)\n",
    "val_dataset = icebergDataset(X_val, y_val, eval_=True)\n",
    "test_dataset = icebergDataset(X_test, eval_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.batch = nn.BatchNorm2d(channels)\n",
    "        self.batch1D = nn.BatchNorm1d(1)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(channels, 18, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(18, 18, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(18),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(18, 36, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(36),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(36, 54, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(54),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(54, 72, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(72),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(72, 144, kernel_size=2, padding=padding),\n",
    "            nn.BatchNorm2d(144),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0))\n",
    "        self.layer_shallow = nn.Sequential(\n",
    "            nn.Conv2d(channels, 128, kernel_size=3, padding=padding),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear((4*4*144), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 196),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        self.fc3 = nn.Linear(196, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x_im, x_angle):\n",
    "        x_im = self.batch(x_im)\n",
    "        x_angle = self.batch1D(x_angle)\n",
    "        out = self.layer1(x_im)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         out_shallow = self.layer_shallow(x_im)\n",
    "#         out_shallow  = F.max_pool2d(out_shallow, kernel_size=out_shallow.size()[2:])\n",
    "#         out_shallow = out_shallow.view(out_shallow.size(0), -1)\n",
    "#         out = torch.cat([out, x_angle], dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (\n",
       "  (batch): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (batch1D): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (layer1): Sequential (\n",
       "    (0): Conv2d(2, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU ()\n",
       "    (3): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU ()\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0)\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): Conv2d(18, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU ()\n",
       "    (3): Conv2d(36, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU ()\n",
       "    (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (7): Dropout (p = 0)\n",
       "  )\n",
       "  (layer3): Sequential (\n",
       "    (0): Conv2d(54, 72, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU ()\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0)\n",
       "  )\n",
       "  (layer4): Sequential (\n",
       "    (0): Conv2d(72, 144, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU ()\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0)\n",
       "  )\n",
       "  (layer_shallow): Sequential (\n",
       "    (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU ()\n",
       "    (3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Dropout (p = 0.2)\n",
       "  )\n",
       "  (fc1): Sequential (\n",
       "    (0): Linear (2304 -> 512)\n",
       "    (1): ReLU ()\n",
       "    (2): Dropout (p = 0.5)\n",
       "  )\n",
       "  (fc2): Sequential (\n",
       "    (0): Linear (512 -> 196)\n",
       "    (1): ReLU ()\n",
       "    (2): Dropout (p = 0.3)\n",
       "  )\n",
       "  (fc3): Linear (196 -> 1)\n",
       "  (sig): Sigmoid ()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "iceNet = net()\n",
    "iceNet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features_angle = features_angle.cuda()\n",
    "        features = Variable(features, volatile=True)\n",
    "        features_angle = Variable(features_angle, volatile=True)\n",
    "        labels = Variable(labels, volatile=True)\n",
    "        outputs = []\n",
    "        for patch_num in range(features.size()[1]):\n",
    "            patch_features = features[:,patch_num,:,:].contiguous()\n",
    "            outputs.append(iceNet(patch_features, features_angle))\n",
    "        outputs = torch.stack(outputs)\n",
    "        outputs = torch.mean(outputs, 0)\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        \n",
    "    return np.mean(loss).data[0], (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] \n",
      "Training Loss: 0.6695\n",
      "Validation Loss: 0.6559, Accuracy: 61.99%\n",
      "Epoch [2/200] \n",
      "Training Loss: 0.5100\n",
      "Validation Loss: 0.3841, Accuracy: 80.69%\n",
      "Epoch [3/200] \n",
      "Training Loss: 0.4457\n",
      "Validation Loss: 0.3174, Accuracy: 83.49%\n",
      "Epoch [4/200] \n",
      "Training Loss: 0.4150\n",
      "Validation Loss: 0.3560, Accuracy: 84.74%\n",
      "Epoch [5/200] \n",
      "Training Loss: 0.4379\n",
      "Validation Loss: 0.3224, Accuracy: 82.55%\n",
      "Epoch [6/200] \n",
      "Training Loss: 0.5393\n",
      "Validation Loss: 0.2906, Accuracy: 86.60%\n",
      "Epoch [7/200] \n",
      "Training Loss: 0.3816\n",
      "Validation Loss: 0.4342, Accuracy: 78.50%\n",
      "Epoch [8/200] \n",
      "Training Loss: 0.3637\n",
      "Validation Loss: 0.2824, Accuracy: 88.47%\n",
      "Epoch [9/200] \n",
      "Training Loss: 0.3505\n",
      "Validation Loss: 0.2622, Accuracy: 87.54%\n",
      "Epoch [10/200] \n",
      "Training Loss: 0.3408\n",
      "Validation Loss: 0.2738, Accuracy: 86.60%\n",
      "Epoch [11/200] \n",
      "Training Loss: 0.3530\n",
      "Validation Loss: 0.2479, Accuracy: 90.03%\n",
      "Epoch [12/200] \n",
      "Training Loss: 0.3448\n",
      "Validation Loss: 0.3474, Accuracy: 82.24%\n",
      "Epoch [13/200] \n",
      "Training Loss: 0.3125\n",
      "Validation Loss: 0.2530, Accuracy: 87.23%\n",
      "Epoch [14/200] \n",
      "Training Loss: 0.3161\n",
      "Validation Loss: 0.2097, Accuracy: 91.28%\n",
      "Epoch [15/200] \n",
      "Training Loss: 0.3039\n",
      "Validation Loss: 0.2430, Accuracy: 89.41%\n",
      "Epoch [16/200] \n",
      "Training Loss: 0.3052\n",
      "Validation Loss: 0.2329, Accuracy: 89.10%\n",
      "Epoch [17/200] \n",
      "Training Loss: 0.3249\n",
      "Validation Loss: 0.2806, Accuracy: 87.54%\n",
      "Epoch [18/200] \n",
      "Training Loss: 0.2978\n",
      "Validation Loss: 0.2329, Accuracy: 89.10%\n",
      "Epoch [19/200] \n",
      "Training Loss: 0.3148\n",
      "Validation Loss: 0.2085, Accuracy: 92.21%\n",
      "Epoch [20/200] \n",
      "Training Loss: 0.2841\n",
      "Validation Loss: 0.1998, Accuracy: 91.59%\n",
      "Epoch [21/200] \n",
      "Training Loss: 0.2642\n",
      "Validation Loss: 0.2130, Accuracy: 91.28%\n",
      "Epoch [22/200] \n",
      "Training Loss: 0.2933\n",
      "Validation Loss: 0.2119, Accuracy: 90.65%\n",
      "Epoch [23/200] \n",
      "Training Loss: 0.2558\n",
      "Validation Loss: 0.2426, Accuracy: 90.34%\n",
      "Epoch [24/200] \n",
      "Training Loss: 0.2673\n",
      "Validation Loss: 0.3495, Accuracy: 81.00%\n",
      "Epoch [25/200] \n",
      "Training Loss: 0.2833\n",
      "Validation Loss: 0.2551, Accuracy: 87.85%\n",
      "Epoch [26/200] \n",
      "Training Loss: 0.3267\n",
      "Validation Loss: 0.2165, Accuracy: 90.34%\n",
      "Epoch [27/200] \n",
      "Training Loss: 0.3191\n",
      "Validation Loss: 0.2661, Accuracy: 87.85%\n",
      "Epoch [28/200] \n",
      "Training Loss: 0.2959\n",
      "Validation Loss: 0.2205, Accuracy: 90.65%\n",
      "Epoch [29/200] \n",
      "Training Loss: 0.3497\n",
      "Validation Loss: 0.2383, Accuracy: 89.10%\n",
      "Epoch [30/200] \n",
      "Training Loss: 0.2716\n",
      "Validation Loss: 0.2197, Accuracy: 90.65%\n",
      "Epoch [31/200] \n",
      "Training Loss: 0.2812\n",
      "Validation Loss: 0.2311, Accuracy: 89.72%\n",
      "Epoch [32/200] \n",
      "Training Loss: 0.2407\n",
      "Validation Loss: 0.2306, Accuracy: 87.85%\n",
      "Epoch [33/200] \n",
      "Training Loss: 0.2918\n",
      "Validation Loss: 0.2210, Accuracy: 91.28%\n",
      "Epoch [34/200] \n",
      "Training Loss: 0.2510\n",
      "Validation Loss: 0.1931, Accuracy: 92.83%\n",
      "Epoch [35/200] \n",
      "Training Loss: 0.2724\n",
      "Validation Loss: 0.2646, Accuracy: 87.23%\n",
      "Epoch [36/200] \n",
      "Training Loss: 0.2509\n",
      "Validation Loss: 0.2682, Accuracy: 87.54%\n",
      "Epoch [37/200] \n",
      "Training Loss: 0.2593\n",
      "Validation Loss: 0.2248, Accuracy: 91.28%\n",
      "Epoch [38/200] \n",
      "Training Loss: 0.2506\n",
      "Validation Loss: 0.2060, Accuracy: 91.28%\n",
      "Epoch [39/200] \n",
      "Training Loss: 0.2477\n",
      "Validation Loss: 0.2245, Accuracy: 91.28%\n",
      "Epoch [40/200] \n",
      "Training Loss: 0.2794\n",
      "Validation Loss: 0.2332, Accuracy: 91.28%\n",
      "Epoch [41/200] \n",
      "Training Loss: 0.2427\n",
      "Validation Loss: 0.2029, Accuracy: 91.59%\n",
      "Epoch [42/200] \n",
      "Training Loss: 0.2656\n",
      "Validation Loss: 0.2711, Accuracy: 86.29%\n",
      "Epoch [43/200] \n",
      "Training Loss: 0.2217\n",
      "Validation Loss: 0.2135, Accuracy: 91.28%\n",
      "Epoch [44/200] \n",
      "Training Loss: 0.2326\n",
      "Validation Loss: 0.2182, Accuracy: 90.65%\n",
      "Epoch [45/200] \n",
      "Training Loss: 0.2820\n",
      "Validation Loss: 0.3601, Accuracy: 88.16%\n",
      "Epoch [46/200] \n",
      "Training Loss: 0.2973\n",
      "Validation Loss: 0.1976, Accuracy: 91.90%\n",
      "Epoch [47/200] \n",
      "Training Loss: 0.2449\n",
      "Validation Loss: 0.1891, Accuracy: 92.21%\n",
      "Epoch [48/200] \n",
      "Training Loss: 0.2335\n",
      "Validation Loss: 0.3265, Accuracy: 85.98%\n",
      "Epoch [49/200] \n",
      "Training Loss: 0.2738\n",
      "Validation Loss: 0.1930, Accuracy: 92.21%\n",
      "Epoch [50/200] \n",
      "Training Loss: 0.2566\n",
      "Validation Loss: 0.2198, Accuracy: 91.28%\n",
      "Epoch [51/200] \n",
      "Training Loss: 0.2454\n",
      "Validation Loss: 0.1949, Accuracy: 92.21%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b26eb301bfbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 new_grads.append(\n\u001b[0;32m---> 37\u001b[0;31m                     Variable(data.new().resize_as_(data).fill_(1), volatile=not create_graph))\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "best_prec1 = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = []\n",
    "    for i, (features, features_angle, labels) in enumerate(train_loader):\n",
    "        iceNet.train()\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        features = Variable(features).float()\n",
    "        features_angle = Variable(features_angle).cuda()\n",
    "        labels = Variable(labels).long()\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        writer.add_graph(iceNet, outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    prec1 = accuracy(val_loader)[0]\n",
    "    print ('Epoch [%d/%d] \\nTraining Loss: %.4f' % (epoch+1, num_epochs, np.mean(epoch_train_loss).data[0]))\n",
    "    print('Validation Loss: %.4f, Accuracy: %.2f%%' % accuracy(val_loader))\n",
    "    \n",
    "    is_best = prec1 < best_prec1\n",
    "    best_prec1 = min(prec1, best_prec1)\n",
    "#     save_checkpoint({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'state_dict': iceNet.state_dict(),\n",
    "#         'best_prec1': best_prec1,\n",
    "#         'optimizer' : optimizer.state_dict(),\n",
    "#     }, is_best)\n",
    "\n",
    "print(best_prec1)\n",
    "# export scalar data to JSON for external processing\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"=> loading checkpoint\")\n",
    "best_model = torch.load('./Models/model_best.pth.tar')\n",
    "print('best_prec1 = ', best_model['best_prec1'])\n",
    "iceNet.load_state_dict(best_model['state_dict'])\n",
    "optimizer.load_state_dict(best_model['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_dataset = icebergDataset(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "iceNet.eval()\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "results = []\n",
    "for features, features_angle in test_loader:\n",
    "    iceNet.eval()\n",
    "    features = Variable(features, volatile=True).cuda()\n",
    "    features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "    outputs = F.softmax(iceNet(features, features_angle))\n",
    "#     outputs = iceNet(features, features_angle)\n",
    "\n",
    "    results.append(outputs.data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45908028059236167"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train>0.5)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30282526115859448"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(results)>0.5)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02592930756509304,\n",
       " 0.3247637450695038,\n",
       " 1.0454600713242189e-14,\n",
       " 0.9999004602432251,\n",
       " 0.40994328260421753,\n",
       " 0.6011435389518738,\n",
       " 0.024099018424749374,\n",
       " 0.9999616146087646,\n",
       " 2.8529889561923483e-10,\n",
       " 2.0306337167319555e-10,\n",
       " 1.0756996557609658e-15,\n",
       " 0.30658578872680664,\n",
       " 1.2682083252002485e-05,\n",
       " 0.29034730792045593,\n",
       " 4.115712783914205e-07,\n",
       " 0.0020702641922980547,\n",
       " 0.0006665511173196137,\n",
       " 0.001083467504940927,\n",
       " 0.003712332108989358,\n",
       " 0.9074830412864685,\n",
       " 0.02027943916618824,\n",
       " 0.18286937475204468,\n",
       " 0.23720575869083405,\n",
       " 0.12182788550853729,\n",
       " 3.2119743469767245e-14,\n",
       " 7.707850357974166e-09,\n",
       " 0.007208712864667177,\n",
       " 0.4834314286708832,\n",
       " 0.1449306458234787,\n",
       " 0.9999291896820068,\n",
       " 1.087272768563255e-13,\n",
       " 0.6034151315689087,\n",
       " 0.4970799684524536,\n",
       " 0.016889972612261772,\n",
       " 0.7639496326446533,\n",
       " 0.9922479391098022,\n",
       " 0.0013621319085359573,\n",
       " 0.2865104377269745,\n",
       " 0.28295210003852844,\n",
       " 0.8613411784172058,\n",
       " 3.21758954014231e-15,\n",
       " 0.5548590421676636,\n",
       " 0.8960628509521484,\n",
       " 2.0677757675002795e-06,\n",
       " 0.6169924736022949,\n",
       " 0.9477837085723877,\n",
       " 0.9972792267799377,\n",
       " 8.354654241549858e-21,\n",
       " 0.9999982118606567,\n",
       " 0.6218925714492798]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub['is_iceberg'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('./Submissions/sub_30Oct_val_1631.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band_1_KF = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n",
    "band_2_KF = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n",
    "# band_3_KF = scale_range(band_1_KF/band_2_KF, -1, 1)\n",
    "X_KF = np.stack((band_1_KF, band_2_KF), axis=1)\n",
    "X_KF = [X_KF, np.array(data['inc_angle']).reshape((len(data), 1))]\n",
    "\n",
    "# X_test[0] = scale_test_range(X_test[0], train_min, train_max, -1, 1)\n",
    "y_KF = data['is_iceberg'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    iceNet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = []\n",
    "    predicted_val = []\n",
    "    for features, features_angle, labels in loader:\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        labels = Variable(labels, volatile=True).cuda().float()\n",
    "        outputs = iceNet(features, features_angle)\n",
    "        predicted_val.append(outputs.data[:].cpu().numpy())\n",
    "        _loss = criterion(outputs, labels)\n",
    "        loss.append(_loss)\n",
    "        predicted = (F.sigmoid(outputs).data>0.5)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.byte().data).sum()\n",
    "    return np.mean(loss).data[0], (100 * correct / total), np.concatenate(predicted_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arsh/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n",
      "Val Score : 0.192183\n",
      "Fold [2/5]\n",
      "Val Score : 0.168901\n",
      "Fold [3/5]\n",
      "Val Score : 0.226362\n",
      "Fold [4/5]\n",
      "Val Score : 0.171711\n",
      "Fold [5/5]\n",
      "Val Score : 0.202793\n"
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "kfold_scores = []\n",
    "\n",
    "test_dataset = icebergDataset(X_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "results = []\n",
    "fold_predictions = []\n",
    "sss = KFold(n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X_KF[0], X_KF[1], y_KF)):\n",
    "    X_train_KF, X_valid_KF = [X_KF[0][train_index], X_KF[1][train_index]], [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "    y_train_KF, y_valid_KF = y_KF[train_index], y_KF[test_index]\n",
    "    \n",
    "    # Define model\n",
    "    iceNet = net().apply(weight_init).cuda()\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(iceNet.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    \n",
    "    # Data Loader\n",
    "    train_dataset_KF = icebergDataset(X_train_KF, y_train_KF, transform=True)\n",
    "    val_dataset_KF = icebergDataset(X_valid_KF, y_valid_KF)\n",
    "\n",
    "    train_loader_KF = torch.utils.data.DataLoader(dataset=train_dataset_KF, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_KF = torch.utils.data.DataLoader(dataset=val_dataset_KF, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print('Fold [%d/%d]' % (i+1, kfold))\n",
    "    # Train\n",
    "    best_prec1 = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss = []\n",
    "        for idx, (features, features_angle, labels) in enumerate(train_loader_KF):\n",
    "            iceNet.train()\n",
    "            features = Variable(features).cuda()\n",
    "            features_angle = Variable(features_angle).cuda()\n",
    "            labels = Variable(labels).cuda().float()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = iceNet(features, features_angle)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_train_loss.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        acc = accuracy(val_loader_KF)\n",
    "        prec1 = acc[0]\n",
    "\n",
    "        # Save best model\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        if is_best:\n",
    "            best_fold_predictions = acc[2]\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': iceNet.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, filename='./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "        \n",
    "    print('Val Score : %f' % (best_prec1))\n",
    "    fold_predictions.append(best_fold_predictions)\n",
    "    kfold_scores.append(best_prec1)\n",
    "    # Load best model\n",
    "    best_model = torch.load('./Models/v1Nov_v1/model_fold_'+str(i+1)+'.pth.tar')\n",
    "    iceNet.load_state_dict(best_model['state_dict'])\n",
    "    optimizer.load_state_dict(best_model['optimizer'])\n",
    "    \n",
    "    # Predict\n",
    "    iceNet.eval()\n",
    "    \n",
    "    results_fold = []\n",
    "    for features, features_angle in test_loader:\n",
    "        iceNet.eval()\n",
    "        features = Variable(features, volatile=True).cuda()\n",
    "        features_angle = Variable(features_angle, volatile=True).cuda()\n",
    "        outputs = F.sigmoid(iceNet(features, features_angle))\n",
    "    #     outputs = iceNet(features, features_angle)\n",
    "\n",
    "        results_fold.append(outputs.data.cpu().numpy()[0][0])        \n",
    "    \n",
    "    results.append(np.array(results_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19239003062248231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(kfold_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_num = 26\n",
    "os.makedirs(\"./Models/Sub \"+str(sub_num))\n",
    "np.savetxt(\"./Models/Sub \"+str(sub_num)+\"/results.csv\", np.array(results), delimiter=\",\")\n",
    "np.savetxt(\"./Models/Sub \"+str(sub_num)+\"/cv_fold_results.csv\", np.concatenate(fold_predictions), delimiter=\",\")\n",
    "sub = pd.read_csv('./Data/sample_submission.csv')\n",
    "sub['is_iceberg'] = np.array(results).mean(axis=0)\n",
    "sub.to_csv('./Submissions/Sub '+str(sub_num)+' - 5-fold _ Val-1802.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5385"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sub['is_iceberg']<0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('./Submissions/Sub 3 - 5 fold _ Val - 0.1504.csv')['is_iceberg']\n",
    "sub2 = pd.read_csv('./Submissions/Sub 4 - 10-fold _ Val-1269.csv')['is_iceberg']\n",
    "sub3 = pd.read_csv('./Submissions/Sub 5 - 5-fold _ Val-1538.csv')['is_iceberg']\n",
    "sub4 = pd.read_csv('./Submissions/Sub 6 - 5-fold _ Val-1480.csv')['is_iceberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = np.mean((np.array(sub1), np.array(sub2), np.array(sub3), np.array(sub4)), axis=0)\n",
    "sub['is_iceberg'] = np.array(results)\n",
    "sub.to_csv('./Submissions/Sub 7 - Ensemble_3_4_5_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring folds 3 and 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats(train,label=1):\n",
    "    train['max'+str(label)] = [np.max(np.array(x)) for x in train[0][:,label,:,:]]\n",
    "    train['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in train[0][:,label,:,:]]\n",
    "    train['min'+str(label)] = [np.min(np.array(x)) for x in train[0][:,label,:,:]]\n",
    "    train['minpos'+str(label)] = [np.argmin(np.array(x)) for x in train[0][:,label,:,:]]\n",
    "    train['med'+str(label)] = [np.median(np.array(x)) for x in train[0][:,label,:,:]]\n",
    "    train['std'+str(label)] = [np.std(np.array(x)) for x in train[0][:,label,:,:]]\n",
    "    train['mean'+str(label)] = [np.mean(np.array(x)) for x in train[0][:,label,:,:]]\n",
    "    train['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in train[0][:,label,:,:]]\n",
    "    train['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in train[0][:,label,:,:]]\n",
    "    train['mid50_'+str(label)] = train['p75_'+str(label)]-train['p25_'+str(label)]\n",
    "\n",
    "    return train\n",
    "\n",
    "def plot_var(name,nbins=50):\n",
    "    minval = train[name].min()\n",
    "    maxval = train[name].max()\n",
    "    plt.hist(train.loc[train.is_iceberg==1,name],range=[minval,maxval],\n",
    "             bins=nbins,color='b',alpha=0.5,label='Boat')\n",
    "    plt.hist(train.loc[train.is_iceberg==0,name],range=[minval,maxval],\n",
    "             bins=nbins,color='r',alpha=0.5,label='Iceberg')\n",
    "    plt.legend()\n",
    "    plt.xlim([minval,maxval])\n",
    "    plt.xlabel(name)\n",
    "    plt.ylabel('Number')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-98adfa1996a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_KF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_KF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'min1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'max1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'std1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'med1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mean1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mid50_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-b7e314440b2e>\u001b[0m in \u001b[0;36mget_stats\u001b[0;34m(train, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maxpos'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minpos'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "kfold_scores = []\n",
    "\n",
    "test_dataset = icebergDataset(X_test)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "results = []\n",
    "sss = KFold(n_splits=kfold, random_state=0)\n",
    "temp = []\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X_KF[0], X_KF[1], y_KF)):\n",
    "    X_train_KF, X_valid_KF = [X_KF[0][train_index], X_KF[1][train_index]], [X_KF[0][test_index], X_KF[1][test_index]]\n",
    "    y_train_KF, y_valid_KF = y_KF[train_index], y_KF[test_index]\n",
    "    \n",
    "    for x in X_valid_KF[0][:,label,:,:]\n",
    "    if i==3 or i==5:\n",
    "        train = get_stats(X_valid_KF,1)\n",
    "        train = get_stats(X_valid_KF,2)\n",
    "        for col in ['min1','max1','std1','med1','mean1','mid50_1']:\n",
    "            plot_var(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
